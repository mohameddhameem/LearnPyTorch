{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Basics with PyTorch - NLTK - Word2Vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWoHqJ/MgQjV8qByrsyTFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohameddhameem/LearnPyTorch/blob/master/NLP_Basics_with_PyTorch_NLTK_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70OkV4V5paqN",
        "colab_type": "text"
      },
      "source": [
        "# Basics of NLP with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVd7SxQtplrw",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "\n",
        "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
        "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
        "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
        "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
        "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
        "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
        "<br><br>\n",
        "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
        "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
        "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
        "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
        "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
        "    - <a href=\"#section-3-1-4-fill-cbow\">The CBOW model</a>\n",
        "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
        "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
        "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
        "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
        "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
        "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
        "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
        "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
        "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
        "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
        "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
        "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
        "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
        "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZGAC0BHp0eZ",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0\"></a>\n",
        "# 3.0. Data Preparation\n",
        "\n",
        "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
        "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
        "\n",
        "There are already several other libraries that help with loading text datasets, e.g. \n",
        "\n",
        " - FastAI https://docs.fast.ai/text.data.html\n",
        " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
        " - Torch Text https://github.com/pytorch/text#data\n",
        " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
        " - SpaCy https://github.com/explosion/thinc\n",
        " \n",
        "\n",
        "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cltZ9XdoqGP1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-1\"></a>\n",
        "## 3.0.1  Vocabulary\n",
        "\n",
        "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmM3NtDFwFEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c47280e3-edee-4f2e-d596-04c50aeaa683"
      },
      "source": [
        "!pip install sklearn torch tqdm nltk lazyme ansi requests gensim tsundoku\n",
        "!python -m nltk.downloader movie_reviews punkt\n",
        "from IPython.display import display, Markdown, Latex\n",
        "from tsundoku.word2vec_hints import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting lazyme\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/8c/3bd9a3100a0a702d10fccf60b8ad1007674ea0e2b834dc15314d74f4af8b/lazyme-0.0.23.tar.gz\n",
            "Collecting ansi\n",
            "  Downloading https://files.pythonhosted.org/packages/60/d3/ffdb7b88446f124e2a550e5444c9ee2fb531f8d79103a318c061b63c87aa/ansi-0.1.3.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Collecting tsundoku\n",
            "  Downloading https://files.pythonhosted.org/packages/31/f6/0e29124c6fa0aeaaf369bce012b07d2c06adc4fa7568593331fed32b34fb/tsundoku-0.0.6.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.5)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.6/dist-packages (from tsundoku) (5.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.11.10)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (45.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (1.0.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.14.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->tsundoku) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->tsundoku) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->tsundoku) (0.1.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n",
            "Building wheels for collected packages: lazyme, ansi, tsundoku\n",
            "  Building wheel for lazyme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lazyme: filename=lazyme-0.0.23-cp36-none-any.whl size=7930 sha256=34b48235d6db9782967c9c1fc3c7a42e06ce0a032572dd87fb708bda74dbdfb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/0a/0c/0d199c54c031fd72b1914b7348ffcb7676e6e5caac6a84846d\n",
            "  Building wheel for ansi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ansi: filename=ansi-0.1.3-cp36-none-any.whl size=6826 sha256=2e947aa45434adf7f7bae91df8a766bd7b5c7d28f1a99ef765dd0977787f9a27\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/52/fd/670f79c4bde50505647446c7ed82dc876696d185560fab75bd\n",
            "  Building wheel for tsundoku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tsundoku: filename=tsundoku-0.0.6-cp36-none-any.whl size=4873 sha256=525db1530cc8fa6bf4ea98e7405b125057b1054d0198fb862ff08cf7aca358ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/6d/48/03bd6921d25fd80c5df44ce7d0ba0de706bbac8cdac9425d09\n",
            "Successfully built lazyme ansi tsundoku\n",
            "Installing collected packages: lazyme, ansi, tsundoku\n",
            "Successfully installed ansi-0.1.3 lazyme-0.0.23 tsundoku-0.0.6\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOfn_lHQpfJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from itertools import chain\n",
        "\n",
        "from tqdm import tqdm\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNksrIzDqLWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea662acf-5190-4b81-a57b-63eba716e92b"
      },
      "source": [
        "try: # Use the default NLTK tokenizer.\n",
        "    from nltk import word_tokenize, sent_tokenize \n",
        "    # Testing whether it works. \n",
        "    # Sometimes it doesn't work on some machines because of setup issues.\n",
        "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
        "    print('It Works')\n",
        "except: # Use a naive sentence tokenizer and toktok.\n",
        "    import re\n",
        "    from nltk.tokenize import ToktokTokenizer\n",
        "    # See https://stackoverflow.com/a/25736515/610569\n",
        "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
        "    # Use the toktok tokenizer that requires no dependencies.\n",
        "    toktok = ToktokTokenizer()\n",
        "    word_tokenize = word_tokenize = toktok.tokenize\n",
        "    print('It Still Works')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It Works\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeJwCwxbqpQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "uniq_tokens = set(chain(*tokenized_text))\n",
        "\n",
        "vocab = {}   # Assign indices to every word.\n",
        "idx2tok = {} # Also keep an dict of index to words.\n",
        "for i, token in enumerate(uniq_tokens):\n",
        "    vocab[token] = i\n",
        "    idx2tok[i] = token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uKEXqgq21y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "835af137-256a-4e75-d5be-ec303313d684"
      },
      "source": [
        "vocab"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 46,\n",
              " ')': 59,\n",
              " ',': 47,\n",
              " '.': 8,\n",
              " 'a': 7,\n",
              " 'able': 56,\n",
              " 'almost': 63,\n",
              " 'always': 84,\n",
              " 'and': 49,\n",
              " 'arbitrary': 70,\n",
              " 'are': 19,\n",
              " 'associations': 2,\n",
              " 'at': 75,\n",
              " 'be': 54,\n",
              " 'been': 18,\n",
              " 'between': 71,\n",
              " 'choose': 41,\n",
              " 'corpora': 35,\n",
              " 'corpus': 74,\n",
              " 'data': 86,\n",
              " 'demonstrably': 28,\n",
              " 'do': 79,\n",
              " 'does': 37,\n",
              " 'enough': 68,\n",
              " 'essentially': 53,\n",
              " 'establish': 23,\n",
              " 'evidence': 34,\n",
              " 'experimental': 48,\n",
              " 'fact': 81,\n",
              " 'frequencies': 16,\n",
              " 'frequently': 64,\n",
              " 'has': 0,\n",
              " 'have': 32,\n",
              " 'hence': 80,\n",
              " 'how': 57,\n",
              " 'hypothesis': 30,\n",
              " 'in': 50,\n",
              " 'inference': 51,\n",
              " 'is': 52,\n",
              " 'it': 43,\n",
              " 'language': 27,\n",
              " 'led': 85,\n",
              " 'linguistic': 3,\n",
              " 'literature': 31,\n",
              " 'look': 58,\n",
              " 'misleading': 66,\n",
              " 'moreover': 22,\n",
              " 'never': 17,\n",
              " 'non-random': 20,\n",
              " 'not': 26,\n",
              " 'null': 78,\n",
              " 'of': 29,\n",
              " 'often': 45,\n",
              " 'or': 5,\n",
              " 'phenomena': 42,\n",
              " 'posits': 73,\n",
              " 'present': 77,\n",
              " 'randomly': 10,\n",
              " 'randomness': 24,\n",
              " 'relation': 11,\n",
              " 'results': 55,\n",
              " 'review': 62,\n",
              " 'shall': 76,\n",
              " 'show': 25,\n",
              " 'so': 38,\n",
              " 'statistical': 67,\n",
              " 'studies': 15,\n",
              " 'support': 33,\n",
              " 'systematically': 69,\n",
              " 'testing': 82,\n",
              " 'that': 1,\n",
              " 'the': 9,\n",
              " 'there': 60,\n",
              " 'to': 44,\n",
              " 'true': 14,\n",
              " 'two': 6,\n",
              " 'unhelpful': 12,\n",
              " 'used': 4,\n",
              " 'users': 13,\n",
              " 'uses': 61,\n",
              " 'we': 83,\n",
              " 'when': 39,\n",
              " 'where': 72,\n",
              " 'which': 21,\n",
              " 'will': 65,\n",
              " 'word': 40,\n",
              " 'words': 36}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbBzwCehrATy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d42413c8-37c1-4c7d-cccb-9dc15613c03a"
      },
      "source": [
        "# Retrieve the index of the word 'corpora'\n",
        "vocab['corpora']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDAfYLSLrEKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3b61f743-65cb-48ca-dcc0-f61cdb60f954"
      },
      "source": [
        "# The indexed representation of the first sentence.\n",
        "\n",
        "sent0 = tokenized_text[0]\n",
        "print('sent0', sent0)\n",
        "[vocab[token] for token in sent0] "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent0 ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27, 13, 17, 41, 36, 10, 47, 49, 27, 52, 53, 20, 8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARvZ0wFarXi6",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-1-a\"></a>\n",
        "\n",
        "### Pet Peeve (Gensim)\n",
        "\n",
        "`gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
        "\n",
        "Using `gensim`, I would have written the above as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BMX_O3HrJH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "vocab = Dictionary(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_T-LSAXrl7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "313117e9-87d7-4b01-ecdf-db28a699a2e3"
      },
      "source": [
        "# Note the key-value order is different of gensim from the native Python's\n",
        "dict(vocab.items())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ',',\n",
              " 1: '.',\n",
              " 2: 'and',\n",
              " 3: 'choose',\n",
              " 4: 'essentially',\n",
              " 5: 'is',\n",
              " 6: 'language',\n",
              " 7: 'never',\n",
              " 8: 'non-random',\n",
              " 9: 'randomly',\n",
              " 10: 'users',\n",
              " 11: 'words',\n",
              " 12: 'a',\n",
              " 13: 'hypothesis',\n",
              " 14: 'null',\n",
              " 15: 'posits',\n",
              " 16: 'randomness',\n",
              " 17: 'statistical',\n",
              " 18: 'testing',\n",
              " 19: 'uses',\n",
              " 20: 'which',\n",
              " 21: 'at',\n",
              " 22: 'be',\n",
              " 23: 'corpora',\n",
              " 24: 'hence',\n",
              " 25: 'in',\n",
              " 26: 'linguistic',\n",
              " 27: 'look',\n",
              " 28: 'phenomena',\n",
              " 29: 'the',\n",
              " 30: 'true',\n",
              " 31: 'we',\n",
              " 32: 'when',\n",
              " 33: 'will',\n",
              " 34: '(',\n",
              " 35: ')',\n",
              " 36: 'able',\n",
              " 37: 'almost',\n",
              " 38: 'always',\n",
              " 39: 'data',\n",
              " 40: 'enough',\n",
              " 41: 'establish',\n",
              " 42: 'it',\n",
              " 43: 'moreover',\n",
              " 44: 'not',\n",
              " 45: 'shall',\n",
              " 46: 'that',\n",
              " 47: 'there',\n",
              " 48: 'to',\n",
              " 49: 'where',\n",
              " 50: 'arbitrary',\n",
              " 51: 'between',\n",
              " 52: 'corpus',\n",
              " 53: 'demonstrably',\n",
              " 54: 'do',\n",
              " 55: 'does',\n",
              " 56: 'fact',\n",
              " 57: 'frequently',\n",
              " 58: 'have',\n",
              " 59: 'inference',\n",
              " 60: 'relation',\n",
              " 61: 'so',\n",
              " 62: 'studies',\n",
              " 63: 'support',\n",
              " 64: 'two',\n",
              " 65: 'are',\n",
              " 66: 'associations',\n",
              " 67: 'evidence',\n",
              " 68: 'experimental',\n",
              " 69: 'frequencies',\n",
              " 70: 'how',\n",
              " 71: 'of',\n",
              " 72: 'present',\n",
              " 73: 'systematically',\n",
              " 74: 'word',\n",
              " 75: 'been',\n",
              " 76: 'has',\n",
              " 77: 'led',\n",
              " 78: 'literature',\n",
              " 79: 'misleading',\n",
              " 80: 'often',\n",
              " 81: 'or',\n",
              " 82: 'results',\n",
              " 83: 'review',\n",
              " 84: 'show',\n",
              " 85: 'unhelpful',\n",
              " 86: 'used'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2KbCph1rqJs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77e9bcd2-9491-4ce1-9007-e994141aa640"
      },
      "source": [
        "vocab.token2id['corpora']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IWY7H80ruKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c474deed-cfa1-4cce-f8dd-d7307deed972"
      },
      "source": [
        "vocab.doc2idx(sent0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LR3hTmr5FG",
        "colab_type": "text"
      },
      "source": [
        "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzuaIDNr_D2",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2\"></a>\n",
        "\n",
        "# 3.0.2 Dataset\n",
        "\n",
        "Lets try creating a `torch.utils.data.Dataset` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aJ1f0z9rzRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Text(Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self.vocab = Dictionary(tokenized_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        # Hint: You want to return a vectorized sentence here.\n",
        "        return {'x': self.vectorize(self.sents[index])}\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkicubMs1R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BELOE IS FROM MY LECTURER. FEEL FREE TO TRY\n",
        "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
        "##hint_dataset_vectorize()\n",
        "##code_text_dataset_vectorize()\n",
        "\n",
        "# Option 2: \"I give up just, run the code for me\" \n",
        "# Uncomment the next two lines, if you really gave up... \n",
        "#full_code_text_dataset_vectorize()\n",
        "#from tsundoku.word2vec import Text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V98xzXBZvLt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8e893863-c2c1-4585-bea7-d46fffdf42a7"
      },
      "source": [
        "tokenized_text[5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " 'present',\n",
              " 'experimental',\n",
              " 'evidence',\n",
              " 'of',\n",
              " 'how',\n",
              " 'arbitrary',\n",
              " 'associations',\n",
              " 'between',\n",
              " 'word',\n",
              " 'frequencies',\n",
              " 'and',\n",
              " 'corpora',\n",
              " 'are',\n",
              " 'systematically',\n",
              " 'non-random',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nTtQnJivdpe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d31ee085-f868-427c-cd19-106395f931bb"
      },
      "source": [
        "text_dataset = Text(tokenized_text)\n",
        "text_dataset[5] # First sentence. Representation of above text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': [31, 72, 68, 67, 71, 70, 50, 66, 51, 74, 69, 2, 23, 65, 73, 8, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mj8VJ_mwpLM",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2-return-dict\"></a>\n",
        "\n",
        "### Return `dict` in `__getitem__()`\n",
        "\n",
        "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
        "\n",
        "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYAQWlCweKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LabeledText(Dataset):\n",
        "    def __init__(self, tokenized_texts, labels):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self.labels = labels # Sentence level labels.\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        return {'x': self.vectorize(self.sents[index]), 'y': self.labels[index]}\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuHeSkMiw1PB",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2-labeleddata\"></a>\n",
        "\n",
        "### Lets try the `LabeledDataset` on a movie review corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKyJmgC1wvwM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f057f032-89f8-4431-c062-1c0cc2e96627"
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "documents = []\n",
        "labels = []\n",
        "\n",
        "for fileid in tqdm(movie_reviews.fileids()):\n",
        "    label = fileid.split('/')[0]\n",
        "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
        "    documents.append(doc)\n",
        "    labels.append(label)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:09<00:00, 195.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwEZ5VmNw5xo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cff7f202-e8fa-4830-e61f-7fecb29445a2"
      },
      "source": [
        "print(documents[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'s\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '``', 'sorta', '``', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'did', \"n't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'s\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '``', 'normal', '``', 'but', 'then', 'downshifts', 'into', 'this', '``', 'fantasy', '``', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'s\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'do', \"n't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'s\", 'biggest', 'problem', '.', 'it', \"'s\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'did', \"n't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '``', 'into', 'it', '``', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'do', \"n't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'ve\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '``', 'the', 'suits', '``', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'s\", 'unraveling', '.', 'overall', ',', 'the', 'film', 'does', \"n't\", 'stick', 'because', 'it', 'does', \"n't\", 'entertain', ',', 'it', \"'s\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'s\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'s\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwziDJdFw-of",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9a1a87a4-8efe-4036-f014-c79597fc9592"
      },
      "source": [
        "labeled_dataset = LabeledText(documents, labels)\n",
        "print(labeled_dataset[0])  # First review in the data."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'x': [243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5], 'y': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnNPd0OxEyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "593d6c1e-4114-4f1a-92d3-d92c5acad6ec"
      },
      "source": [
        "print(labeled_dataset[0]['x'])  # First review in vectorized index format."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqLJVcrCxLdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7936c8af-503b-4aa3-906d-858eeac4e3d3"
      },
      "source": [
        "print(labeled_dataset[0]['y'])  # Label of the first review in the data. "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985OjlPTxUlk",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1\"></a>\n",
        "\n",
        "# 3.1 Word2Vec Training\n",
        "\n",
        "Word2Vec has two training variants:\n",
        "\n",
        " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
        " - **Skip-grams**: Predict context words given center word.\n",
        "  \n",
        "Visually, they look like this:\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm9cdGM2xhND",
        "colab_type": "text"
      },
      "source": [
        "Fig. 1. The skip-gram model. Both the input vector xx and the output yy are one-hot encoded word representations. <br>The hidden layer is the word embedding of size NN.\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_b0MdnzyXRm",
        "colab_type": "text"
      },
      "source": [
        "Fig. 2. The CBOW model. Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer. Other symbols have the same meanings as in Fig 1.\n",
        "\n",
        "(Pretty network images above are from [https://lilianweng.github.io](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-continuous-bag-of-words-cbow))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdcgrrdky6P6",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-1\"></a>\n",
        "\n",
        "## 3.1.1. CBOW\n",
        "\n",
        "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE-og_THxOWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6be8e287-1fbf-4626-d6ec-bc067b1f8c42"
      },
      "source": [
        "from lazyme import per_window, per_chunk\n",
        "\n",
        "xx =[1,2,3,4]\n",
        "list(per_window(xx, n=2))\n",
        "list(per_chunk(xx, n=3))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 3), (4, None, None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suLX0NBvzFn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def per_window(sequence, n=1):\n",
        "    \"\"\"\n",
        "    From http://stackoverflow.com/q/42220614/610569\n",
        "        >>> list(per_window([1,2,3,4], n=2))\n",
        "        [(1, 2), (2, 3), (3, 4)]\n",
        "        >>> list(per_window([1,2,3,4], n=3))\n",
        "        [(1, 2, 3), (2, 3, 4)]\n",
        "    \"\"\"\n",
        "    start, stop = 0, n\n",
        "    seq = list(sequence)\n",
        "    while stop <= len(seq):\n",
        "        yield seq[start:stop]\n",
        "        start += 1\n",
        "        stop += 1\n",
        "\n",
        "def cbow_iterator(tokens, window_size):\n",
        "    n = window_size * 2 + 1\n",
        "    for window in per_window(tokens, n):\n",
        "        target = window.pop(window_size)\n",
        "        yield window, target   # X = window ; Y = target. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyf9slOh06S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
        "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNZ4NW2r1IFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9eb62b3b-e898-459a-c53c-4e4366b24e15"
      },
      "source": [
        "list(cbow_iterator(sent0, 2)) \n",
        "#the first part is X and target is Y\n",
        "#X => 'language', 'users', 'choose', 'words'\n",
        "#y => 'never'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['language', 'users', 'choose', 'words'], 'never'),\n",
              " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
              " (['never', 'choose', 'randomly', ','], 'words'),\n",
              " (['choose', 'words', ',', 'and'], 'randomly'),\n",
              " (['words', 'randomly', 'and', 'language'], ','),\n",
              " (['randomly', ',', 'language', 'is'], 'and'),\n",
              " ([',', 'and', 'is', 'essentially'], 'language'),\n",
              " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
              " (['language', 'is', 'non-random', '.'], 'essentially')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4upcyLol1KF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5b653643-75f1-4baa-d836-1c940846cd97"
      },
      "source": [
        "list(cbow_iterator(sent0, 3)) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
              " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
              " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
              " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
              " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
              " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
              " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiIkqfO1ndl",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-2\"></a>\n",
        "\n",
        "## 3.1.2. Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGmQ1741i-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def skipgram_iterator(tokens, window_size):\n",
        "    n = window_size * 2 + 1 \n",
        "    for i, window in enumerate(per_window(tokens, n)):\n",
        "        target = window.pop(window_size)\n",
        "        # Generate positive samples.\n",
        "        for context_word in window:\n",
        "            yield target, context_word, 1\n",
        "        # Generate negative samples.\n",
        "        for _ in range(n-1):\n",
        "            leftovers = tokens[:i] + tokens[i+n:]\n",
        "            yield target, random.choice(leftovers), 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tw3CKO81ruM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2656214a-9a8d-4f26-8ce0-388b83108556"
      },
      "source": [
        "list(skipgram_iterator(sent0, 2))\n",
        "#1 is positive sample and 0 negative"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('never', 'language', 1),\n",
              " ('never', 'users', 1),\n",
              " ('never', 'choose', 1),\n",
              " ('never', 'words', 1),\n",
              " ('never', 'and', 0),\n",
              " ('never', ',', 0),\n",
              " ('never', ',', 0),\n",
              " ('never', 'and', 0),\n",
              " ('choose', 'users', 1),\n",
              " ('choose', 'never', 1),\n",
              " ('choose', 'words', 1),\n",
              " ('choose', 'randomly', 1),\n",
              " ('choose', 'language', 0),\n",
              " ('choose', 'and', 0),\n",
              " ('choose', '.', 0),\n",
              " ('choose', 'language', 0),\n",
              " ('words', 'never', 1),\n",
              " ('words', 'choose', 1),\n",
              " ('words', 'randomly', 1),\n",
              " ('words', ',', 1),\n",
              " ('words', 'non-random', 0),\n",
              " ('words', 'language', 0),\n",
              " ('words', 'users', 0),\n",
              " ('words', 'users', 0),\n",
              " ('randomly', 'choose', 1),\n",
              " ('randomly', 'words', 1),\n",
              " ('randomly', ',', 1),\n",
              " ('randomly', 'and', 1),\n",
              " ('randomly', 'language', 0),\n",
              " ('randomly', 'non-random', 0),\n",
              " ('randomly', 'is', 0),\n",
              " ('randomly', 'language', 0),\n",
              " (',', 'words', 1),\n",
              " (',', 'randomly', 1),\n",
              " (',', 'and', 1),\n",
              " (',', 'language', 1),\n",
              " (',', 'non-random', 0),\n",
              " (',', 'users', 0),\n",
              " (',', 'non-random', 0),\n",
              " (',', 'language', 0),\n",
              " ('and', 'randomly', 1),\n",
              " ('and', ',', 1),\n",
              " ('and', 'language', 1),\n",
              " ('and', 'is', 1),\n",
              " ('and', 'non-random', 0),\n",
              " ('and', '.', 0),\n",
              " ('and', 'essentially', 0),\n",
              " ('and', 'words', 0),\n",
              " ('language', ',', 1),\n",
              " ('language', 'and', 1),\n",
              " ('language', 'is', 1),\n",
              " ('language', 'essentially', 1),\n",
              " ('language', 'users', 0),\n",
              " ('language', 'non-random', 0),\n",
              " ('language', 'randomly', 0),\n",
              " ('language', 'randomly', 0),\n",
              " ('is', 'and', 1),\n",
              " ('is', 'language', 1),\n",
              " ('is', 'essentially', 1),\n",
              " ('is', 'non-random', 1),\n",
              " ('is', 'choose', 0),\n",
              " ('is', 'randomly', 0),\n",
              " ('is', 'users', 0),\n",
              " ('is', '.', 0),\n",
              " ('essentially', 'language', 1),\n",
              " ('essentially', 'is', 1),\n",
              " ('essentially', 'non-random', 1),\n",
              " ('essentially', '.', 1),\n",
              " ('essentially', 'choose', 0),\n",
              " ('essentially', 'choose', 0),\n",
              " ('essentially', 'choose', 0),\n",
              " ('essentially', 'randomly', 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCDPaz12EyN",
        "colab_type": "text"
      },
      "source": [
        "## Cut-away: What is `partial`?\n",
        "\n",
        "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYt9mXzG1tk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e2795e1-c7ea-4291-d7ef-e0e5ab4dbb98"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "# Generates bigrams\n",
        "list(ngrams('this is a sentence'.split(), n=2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcbr82d2Ipu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
        "bigrams = partial(ngrams, n=2)\n",
        "trigrams = partial(ngrams, n=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iKOZYNH2NYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bc2757b-683f-4732-e5ac-24cb16e120ae"
      },
      "source": [
        "list(trigrams('this is a sentence'.split()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3dOFOLO2PeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4805ed7-180d-438a-d431-480031032702"
      },
      "source": [
        "list(bigrams('this is a sentence'.split()))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLyudWp2Uy1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-3\"></a>\n",
        "\n",
        "## 3.1.3 Word2Vec Dataset\n",
        "\n",
        "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
        "\n",
        "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es7gpYNM2Qx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecText(Dataset):\n",
        "    def __init__(self, tokenized_texts, window_size, variant):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self._len = len(self.sents)\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "        self.window_size = window_size\n",
        "        self.variant = variant\n",
        "        if variant.lower() == 'cbow':\n",
        "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
        "        elif variant.lower() == 'skipgram':\n",
        "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "\n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        vectorized_sent = self.vectorize(self.sents[index])\n",
        "        return list(self._iterator(vectorized_sent))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized.\n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
        "        return self.vocab.doc2idx(tokens)\n",
        "\n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]\n",
        "\n",
        "    def cbow_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1\n",
        "        for window in per_window(tokens, n):\n",
        "            target = window.pop(window_size)\n",
        "            yield {'x':window, 'y':target}   # X = window ; Y = target. \n",
        "\n",
        "    def skipgram_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1 \n",
        "        for i, window in enumerate(per_window(tokens, n)):\n",
        "            target = window.pop(window_size)\n",
        "            # Generate positive samples.\n",
        "            for context_word in window:\n",
        "                yield {'x':(target, context_word), 'y':1}\n",
        "            # Generate negative samples.\n",
        "            for _ in range(n-1):\n",
        "                leftovers = tokens[:i] + tokens[i+n:]\n",
        "                yield {'x': (target, random.choice(leftovers)), 'y':0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHreIlYP3e2z",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-3-hint\"></a>\n",
        "## Hints for the cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DWMTG8l3fUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
        "##hint_word2vec_dataset()\n",
        "\n",
        "# Option 2: \"I give up just, run the code for me\" \n",
        "# Uncomment the next two lines, if you really gave up... \n",
        "##full_code_word2vec_dataset()\n",
        "##from tsundoku.word2vec import Word2VecText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPxvwu4w3qYP",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-hint\"></a>\n",
        "\n",
        "## 3.1.4. Train a CBOW model\n",
        "\n",
        "### Lets Get Some Data\n",
        "\n",
        "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxICbkpt3iaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os,requests, io #codecs\n",
        "\n",
        "\n",
        "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
        "if os.path.isfile('language-never-random.txt'):\n",
        "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
        "        text = fin.read()\n",
        "else:\n",
        "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "    text = requests.get(url).content.decode('utf8')\n",
        "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "        fout.write(text)\n",
        "\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
        "window_size = 2\n",
        "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxe-EDyT36OX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8783004b-bd61-4d05-aa83-6aead2e79572"
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish that it is not true. In\n",
            "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
            "tion between two phenomena is demonstrably non-random, does not sup-\n",
            "port the inference that it is not arbitrary. We present experimental evidence\n",
            "of how arbitrary associations between word frequencies and corpora are\n",
            "systematically non-random. We review literature in which hypothesis test-\n",
            "ing has been used, and show how it has often led to unhelpful or mislead-\n",
            "ing results.\n",
            "Keywords: 쎲쎲쎲\n",
            "\n",
            "1. Int\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqHLaRwm4IOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ed4c154b-dde6-4189-9d3c-158f5a5d0dc0"
      },
      "source": [
        "# Sanity check, lets take a look at the data.\n",
        "print(tokenized_text[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxUNZmu-4O11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "910da824-6ab8-4ec4-828d-3911c67f460c"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzY04ie74TKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "81b2fadc-61f1-4d75-cc82-52f21d59f8b6"
      },
      "source": [
        "from lazyme import color_str\n",
        "\n",
        "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
        "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
        "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
        "    target = vocab.get(int(y), '<unk>')\n",
        "\n",
        "    if not prediction:\n",
        "        predicted_word = '______'\n",
        "    else:\n",
        "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
        "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
        "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
        "    \n",
        "\n",
        "sent_idx = 10\n",
        "window_size = 2\n",
        "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
        "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
        "for w2v_io in w2v_dataset[sent_idx]:\n",
        "    context, target = w2v_io['x'], w2v_io['y']\n",
        "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
        "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
            "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
            "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
            "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
            "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
            "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
            "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
            "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
            "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
            "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
            "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
            "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
            "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
            "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
            "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
            "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
            "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
            "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
            "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
            "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
            "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
            "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
            "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
            "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
            "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
            "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
            "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
            "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
            "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
            "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
            "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
            "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09Ap2Nq4oZ1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-cbow-model\"></a>\n",
        "\n",
        "## The CBOW Model\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"600\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VLFQA3T44RR",
        "colab_type": "text"
      },
      "source": [
        "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN142ipB4g9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974oU-pE5JYa",
        "colab_type": "text"
      },
      "source": [
        "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
        "\n",
        "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aohU8BTc5F0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0d166936-cbcd-413e-8092-b37364dbf0cd"
      },
      "source": [
        "w2v_dataset[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'x': [10, 8, 0, 7], 'y': 11},\n",
              " {'x': [8, 11, 7, 0], 'y': 0},\n",
              " {'x': [11, 0, 0, 7], 'y': 7},\n",
              " {'x': [0, 7, 7, 0], 'y': 0},\n",
              " {'x': [7, 0, 0, 13], 'y': 7},\n",
              " {'x': [0, 7, 13, 3], 'y': 0},\n",
              " {'x': [7, 0, 3, 9], 'y': 13},\n",
              " {'x': [0, 13, 9, 2], 'y': 3},\n",
              " {'x': [13, 3, 2, 10], 'y': 9},\n",
              " {'x': [3, 9, 10, 15], 'y': 2},\n",
              " {'x': [9, 2, 15, 11], 'y': 10},\n",
              " {'x': [2, 10, 11, 5], 'y': 15},\n",
              " {'x': [10, 15, 5, 16], 'y': 11},\n",
              " {'x': [15, 11, 16, 14], 'y': 5},\n",
              " {'x': [11, 5, 14, 0], 'y': 16},\n",
              " {'x': [5, 16, 0, 4], 'y': 14},\n",
              " {'x': [16, 14, 4, 10], 'y': 0},\n",
              " {'x': [14, 0, 10, 8], 'y': 4},\n",
              " {'x': [0, 4, 8, 6], 'y': 10},\n",
              " {'x': [4, 10, 6, 12], 'y': 8},\n",
              " {'x': [10, 8, 12, 1], 'y': 6}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPbUZnBd5Lpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dfa0a0f3-bea7-47ec-8949-9286016e200b"
      },
      "source": [
        "# Lets take a look at the first output.\n",
        "x, y = w2v_dataset[0][0]['x'],  w2v_dataset[0][0]['y']\n",
        "\n",
        "x = tensor(x)\n",
        "y = autograd.Variable(tensor(y, dtype=torch.long))\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10,  8,  0,  7])\n",
            "tensor(11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPyMs7z5RJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7070d1d3-7a27-4fd5-fb70-3b18e0905d21"
      },
      "source": [
        "embd_size = 5\n",
        "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
        "emb.state_dict()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight', tensor([[-0.1880, -1.0322, -0.6407, -0.5579, -0.2491],\n",
              "                      [-1.7828, -0.4636,  0.1035, -0.9201,  1.7040],\n",
              "                      [-0.2808, -0.5405,  0.6176,  0.7817, -1.0528],\n",
              "                      ...,\n",
              "                      [ 1.3659, -0.8441, -0.0289, -1.2922, -0.4534],\n",
              "                      [ 2.6299,  0.8170, -0.8663,  0.3115,  0.6880],\n",
              "                      [ 1.9180, -0.3142, -0.0353,  1.1396,  0.9176]]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4PxmYTw5cg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4e3b02ad-a2be-42b5-e518-498f280e101e"
      },
      "source": [
        "print(emb.state_dict()['weight'].shape)\n",
        "emb.state_dict()['weight']"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1388, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1880, -1.0322, -0.6407, -0.5579, -0.2491],\n",
              "        [-1.7828, -0.4636,  0.1035, -0.9201,  1.7040],\n",
              "        [-0.2808, -0.5405,  0.6176,  0.7817, -1.0528],\n",
              "        ...,\n",
              "        [ 1.3659, -0.8441, -0.0289, -1.2922, -0.4534],\n",
              "        [ 2.6299,  0.8170, -0.8663,  0.3115,  0.6880],\n",
              "        [ 1.9180, -0.3142, -0.0353,  1.1396,  0.9176]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WEktdC5fWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b78a827c-2716-4e01-fd49-6ec290e17801"
      },
      "source": [
        "print(emb(x).shape)\n",
        "print(emb(x))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5])\n",
            "tensor([[ 0.6233, -0.4180, -0.5385, -0.1029, -1.9694],\n",
            "        [ 1.7846,  1.3032,  0.0956, -0.2175, -1.1177],\n",
            "        [-0.1880, -1.0322, -0.6407, -0.5579, -0.2491],\n",
            "        [-0.2510, -0.6036,  0.1473,  0.3737,  0.1276]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDUv-3Vq5jRl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "324f121e-85cf-437d-9f5b-184c768f771e"
      },
      "source": [
        "print(emb(x).view(1, -1).shape)\n",
        "emb(x).view(1, -1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6233, -0.4180, -0.5385, -0.1029, -1.9694,  1.7846,  1.3032,  0.0956,\n",
              "         -0.2175, -1.1177, -0.1880, -1.0322, -0.6407, -0.5579, -0.2491, -0.2510,\n",
              "         -0.6036,  0.1473,  0.3737,  0.1276]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrjn4g0-5v8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "16d3e856-20b5-4916-bbe7-766d2d81c815"
      },
      "source": [
        "hidden_size = 100\n",
        "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
        "print(lin1.state_dict())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('weight', tensor([[ 0.1395, -0.1348,  0.1772,  ..., -0.1822, -0.0425, -0.1952],\n",
            "        [-0.1049,  0.1705,  0.0626,  ...,  0.0684,  0.0562, -0.1269],\n",
            "        [-0.2056,  0.2188,  0.0930,  ..., -0.1376,  0.0188, -0.0706],\n",
            "        ...,\n",
            "        [-0.0437,  0.2165,  0.0801,  ...,  0.2199, -0.0498,  0.0908],\n",
            "        [ 0.1374,  0.1878,  0.1229,  ..., -0.0079,  0.0438,  0.1954],\n",
            "        [-0.0854,  0.0267, -0.1673,  ..., -0.1811,  0.1540,  0.1923]])), ('bias', tensor([-6.6642e-04, -1.6775e-01, -1.5766e-01,  6.5517e-02, -1.7500e-01,\n",
            "        -1.8796e-01,  1.5863e-01,  6.1583e-02, -1.2127e-01,  1.8813e-02,\n",
            "        -1.4024e-01, -1.3510e-01, -1.8666e-02,  1.9872e-01,  8.4633e-02,\n",
            "         1.9423e-01,  1.3615e-01, -3.0980e-02,  2.0134e-03, -1.3587e-01,\n",
            "         1.3541e-01,  5.6269e-02, -2.1328e-01,  1.9340e-01, -1.1316e-01,\n",
            "         4.4479e-02, -9.8065e-05, -1.3612e-01,  2.1333e-01,  3.7946e-02,\n",
            "        -1.3791e-01,  2.9989e-02, -2.0840e-01, -9.1589e-02, -1.6304e-01,\n",
            "         1.4159e-01,  9.9028e-03,  1.2067e-01, -2.5100e-02, -2.1116e-01,\n",
            "        -6.9324e-02,  1.8936e-01, -1.6497e-01, -2.2659e-02, -1.6084e-01,\n",
            "        -3.3134e-02,  2.1768e-01,  7.3564e-02, -6.2819e-02,  1.6915e-01,\n",
            "         1.1383e-01, -1.4115e-01,  2.1519e-01,  2.0722e-01, -2.1911e-01,\n",
            "         6.1928e-02,  1.9464e-02, -1.4287e-01, -6.1221e-02,  1.0474e-01,\n",
            "        -7.4354e-02, -1.1205e-01,  7.1464e-02, -5.9949e-02, -8.6578e-02,\n",
            "        -9.9550e-02,  1.4135e-01,  8.6276e-02, -2.0563e-01, -1.6448e-01,\n",
            "         1.9050e-01, -1.9372e-01, -1.0283e-01,  7.5898e-03, -1.5278e-01,\n",
            "        -1.7469e-01,  1.6500e-01,  4.5787e-02, -2.0299e-01,  1.0788e-01,\n",
            "         4.5854e-02,  7.4283e-02, -7.4481e-02, -9.7709e-02,  1.6092e-01,\n",
            "         1.2546e-01,  4.9957e-02, -1.0641e-01, -1.3932e-01,  2.0121e-01,\n",
            "         1.2365e-01,  1.3164e-01, -8.8181e-02, -8.7557e-03,  5.1957e-02,\n",
            "        -8.7707e-02,  1.8675e-01, -1.7871e-01,  6.1426e-02,  7.5692e-02]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q53JRdWz6H3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c79dc6aa-6fec-49e3-c0d1-02df4cfda5ce"
      },
      "source": [
        "print(lin1.state_dict()['weight'].shape)\n",
        "print(lin1.state_dict()['weight'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 20])\n",
            "tensor([[ 0.1395, -0.1348,  0.1772,  ..., -0.1822, -0.0425, -0.1952],\n",
            "        [-0.1049,  0.1705,  0.0626,  ...,  0.0684,  0.0562, -0.1269],\n",
            "        [-0.2056,  0.2188,  0.0930,  ..., -0.1376,  0.0188, -0.0706],\n",
            "        ...,\n",
            "        [-0.0437,  0.2165,  0.0801,  ...,  0.2199, -0.0498,  0.0908],\n",
            "        [ 0.1374,  0.1878,  0.1229,  ..., -0.0079,  0.0438,  0.1954],\n",
            "        [-0.0854,  0.0267, -0.1673,  ..., -0.1811,  0.1540,  0.1923]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD17_G0Q6N21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fe9760e0-d700-41d4-9d5b-905986e80093"
      },
      "source": [
        "print(lin1(emb(x).view(1, -1)).shape)\n",
        "lin1(emb(x).view(1, -1))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3991, -0.4228, -0.1659, -0.2379, -0.1640, -0.0594,  0.0123, -0.8033,\n",
              "         -0.5607,  0.5134,  0.5758, -0.7968,  0.4448,  1.1847,  0.0280,  0.5083,\n",
              "          1.0348, -0.4118, -0.6796,  0.1356, -0.6196,  0.7921, -0.0330,  0.2802,\n",
              "         -0.2207,  0.1725, -0.5441,  0.5341,  0.3671,  0.2238,  0.2853,  0.1797,\n",
              "          0.3686, -0.9890,  0.5579, -0.5201, -0.3643, -0.4651, -0.3341, -0.5795,\n",
              "         -0.8552,  0.5131, -1.0989,  0.1362, -0.8178, -0.6387, -0.4009,  0.2042,\n",
              "         -0.9274, -0.1024,  0.3727, -1.0003,  0.1631, -0.3870,  0.3035, -0.1144,\n",
              "         -0.3884,  0.0498,  0.2525, -0.3793, -0.2092, -0.3617, -0.8901,  0.2082,\n",
              "          0.1685,  0.6717,  0.4523,  0.2058, -1.0579,  0.2358,  0.3253,  0.1246,\n",
              "         -0.1935,  0.1409,  0.4460, -0.5093,  0.0179,  0.1105,  0.1445,  0.1096,\n",
              "          0.4176, -0.2483, -0.3286,  1.0436, -0.4838,  0.8258, -0.0132, -0.4853,\n",
              "         -0.3448, -0.0188,  0.1080,  0.4869,  0.0506,  0.4917,  0.5158, -1.1613,\n",
              "          0.0694, -0.2172,  0.0547,  0.0235]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DhwHGqf6TrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a5ef9a20-e7f9-442c-cc5f-e97f006b496b"
      },
      "source": [
        "relu = nn.ReLU()\n",
        "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
        "relu(lin1(emb(x).view(1, -1)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3991, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0123, 0.0000, 0.0000,\n",
              "         0.5134, 0.5758, 0.0000, 0.4448, 1.1847, 0.0280, 0.5083, 1.0348, 0.0000,\n",
              "         0.0000, 0.1356, 0.0000, 0.7921, 0.0000, 0.2802, 0.0000, 0.1725, 0.0000,\n",
              "         0.5341, 0.3671, 0.2238, 0.2853, 0.1797, 0.3686, 0.0000, 0.5579, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5131, 0.0000, 0.1362, 0.0000,\n",
              "         0.0000, 0.0000, 0.2042, 0.0000, 0.0000, 0.3727, 0.0000, 0.1631, 0.0000,\n",
              "         0.3035, 0.0000, 0.0000, 0.0498, 0.2525, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.2082, 0.1685, 0.6717, 0.4523, 0.2058, 0.0000, 0.2358, 0.3253, 0.1246,\n",
              "         0.0000, 0.1409, 0.4460, 0.0000, 0.0179, 0.1105, 0.1445, 0.1096, 0.4176,\n",
              "         0.0000, 0.0000, 1.0436, 0.0000, 0.8258, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.1080, 0.4869, 0.0506, 0.4917, 0.5158, 0.0000, 0.0694, 0.0000, 0.0547,\n",
              "         0.0235]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcMI7Gcf6YV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "db450d4d-1126-49ae-e351-17d97b876678"
      },
      "source": [
        "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
        "print(lin2.state_dict()['weight'].shape)\n",
        "lin2.state_dict()['weight']"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1388, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0067, -0.0181, -0.0822,  ..., -0.0143, -0.0153, -0.0144],\n",
              "        [-0.0342, -0.0956,  0.0532,  ...,  0.0291,  0.0894,  0.0858],\n",
              "        [ 0.0055,  0.0219,  0.0820,  ...,  0.0189,  0.0017, -0.0883],\n",
              "        ...,\n",
              "        [ 0.0146, -0.0216, -0.0860,  ..., -0.0846, -0.0355, -0.0425],\n",
              "        [ 0.0508,  0.0464, -0.0454,  ...,  0.0408,  0.0294, -0.0520],\n",
              "        [-0.0971,  0.0971, -0.0170,  ...,  0.0062,  0.0516,  0.0686]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFasnpS36ciE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "768d16a5-e458-438a-9c90-23ad86dfca52"
      },
      "source": [
        "h_x = relu(lin1(emb(x).view(1, -1)))\n",
        "print(lin2(h_x).shape)\n",
        "lin2(h_x)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1388])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0508, -0.0918, -0.2075,  ..., -0.0713, -0.0362, -0.0326]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_a33B3p6lrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6434c92d-52cf-46fe-c5d9-f465e05d9e45"
      },
      "source": [
        "softmax = nn.LogSoftmax(dim=1)\n",
        "softmax(lin2(h_x)).detach().numpy().tolist()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-7.206191062927246,\n",
              "  -7.348847389221191,\n",
              "  -7.464487075805664,\n",
              "  -7.032924175262451,\n",
              "  -7.121955871582031,\n",
              "  -7.1980085372924805,\n",
              "  -7.200874328613281,\n",
              "  -7.287985801696777,\n",
              "  -7.295189380645752,\n",
              "  -7.1871490478515625,\n",
              "  -7.230546474456787,\n",
              "  -7.130277633666992,\n",
              "  -7.145940780639648,\n",
              "  -7.841395854949951,\n",
              "  -7.243438720703125,\n",
              "  -7.36161994934082,\n",
              "  -7.070356369018555,\n",
              "  -6.961628437042236,\n",
              "  -7.26784086227417,\n",
              "  -7.016376495361328,\n",
              "  -7.458911895751953,\n",
              "  -7.463275909423828,\n",
              "  -7.238275527954102,\n",
              "  -7.019607067108154,\n",
              "  -7.421109676361084,\n",
              "  -7.342872142791748,\n",
              "  -6.931158542633057,\n",
              "  -7.221442222595215,\n",
              "  -7.288270473480225,\n",
              "  -7.319530487060547,\n",
              "  -7.632073402404785,\n",
              "  -7.397898197174072,\n",
              "  -7.3117523193359375,\n",
              "  -7.436592102050781,\n",
              "  -7.057208061218262,\n",
              "  -7.308782577514648,\n",
              "  -7.36535120010376,\n",
              "  -7.118256092071533,\n",
              "  -6.935368537902832,\n",
              "  -7.276739120483398,\n",
              "  -7.161487579345703,\n",
              "  -7.52125883102417,\n",
              "  -7.493717670440674,\n",
              "  -7.453890323638916,\n",
              "  -7.183237552642822,\n",
              "  -7.484580039978027,\n",
              "  -7.180050849914551,\n",
              "  -7.2121968269348145,\n",
              "  -7.202940940856934,\n",
              "  -7.119243144989014,\n",
              "  -7.671718597412109,\n",
              "  -7.159111499786377,\n",
              "  -7.000242710113525,\n",
              "  -7.272288799285889,\n",
              "  -6.8232927322387695,\n",
              "  -7.2648539543151855,\n",
              "  -7.529823303222656,\n",
              "  -7.297431945800781,\n",
              "  -7.168764114379883,\n",
              "  -7.365647792816162,\n",
              "  -6.732451438903809,\n",
              "  -7.36647891998291,\n",
              "  -7.265663146972656,\n",
              "  -7.073788166046143,\n",
              "  -7.417260646820068,\n",
              "  -6.980523109436035,\n",
              "  -7.067327499389648,\n",
              "  -7.294347286224365,\n",
              "  -7.374109268188477,\n",
              "  -7.23755407333374,\n",
              "  -7.481732368469238,\n",
              "  -7.065830707550049,\n",
              "  -7.1183648109436035,\n",
              "  -7.3082594871521,\n",
              "  -7.460757255554199,\n",
              "  -6.989571571350098,\n",
              "  -7.096036434173584,\n",
              "  -7.548545837402344,\n",
              "  -7.4934983253479,\n",
              "  -6.8064446449279785,\n",
              "  -7.053509712219238,\n",
              "  -7.286635875701904,\n",
              "  -7.295167922973633,\n",
              "  -6.983932018280029,\n",
              "  -7.246851444244385,\n",
              "  -6.946181297302246,\n",
              "  -6.704293251037598,\n",
              "  -6.998753070831299,\n",
              "  -7.3884100914001465,\n",
              "  -6.895461082458496,\n",
              "  -7.564343452453613,\n",
              "  -7.04532527923584,\n",
              "  -7.096255302429199,\n",
              "  -7.179685115814209,\n",
              "  -7.189002513885498,\n",
              "  -6.836197853088379,\n",
              "  -7.288873672485352,\n",
              "  -7.369362831115723,\n",
              "  -7.467311859130859,\n",
              "  -7.128990650177002,\n",
              "  -7.095774173736572,\n",
              "  -7.096137523651123,\n",
              "  -7.0950751304626465,\n",
              "  -7.357684135437012,\n",
              "  -6.938969612121582,\n",
              "  -7.34239387512207,\n",
              "  -7.844524383544922,\n",
              "  -7.190566539764404,\n",
              "  -7.589465618133545,\n",
              "  -7.152524948120117,\n",
              "  -7.362024307250977,\n",
              "  -7.274479866027832,\n",
              "  -7.120656967163086,\n",
              "  -7.179721832275391,\n",
              "  -7.3397603034973145,\n",
              "  -7.197841644287109,\n",
              "  -7.236244201660156,\n",
              "  -7.160351753234863,\n",
              "  -7.556652545928955,\n",
              "  -7.01308012008667,\n",
              "  -7.279162406921387,\n",
              "  -7.316762447357178,\n",
              "  -7.142291069030762,\n",
              "  -7.2849812507629395,\n",
              "  -6.992334365844727,\n",
              "  -7.265434741973877,\n",
              "  -7.21802282333374,\n",
              "  -7.1098103523254395,\n",
              "  -7.509832382202148,\n",
              "  -7.447537899017334,\n",
              "  -7.234889984130859,\n",
              "  -7.284101963043213,\n",
              "  -7.1436004638671875,\n",
              "  -7.022961139678955,\n",
              "  -7.082249164581299,\n",
              "  -7.044203758239746,\n",
              "  -7.134304523468018,\n",
              "  -7.433230876922607,\n",
              "  -7.077291488647461,\n",
              "  -6.9122419357299805,\n",
              "  -7.1804938316345215,\n",
              "  -7.470695495605469,\n",
              "  -7.468245029449463,\n",
              "  -7.621013164520264,\n",
              "  -7.195844650268555,\n",
              "  -7.012894630432129,\n",
              "  -7.020833969116211,\n",
              "  -7.547470569610596,\n",
              "  -7.125866413116455,\n",
              "  -7.274680137634277,\n",
              "  -7.4709553718566895,\n",
              "  -7.104600429534912,\n",
              "  -7.540894985198975,\n",
              "  -7.190213203430176,\n",
              "  -6.970774173736572,\n",
              "  -7.262087821960449,\n",
              "  -7.016740798950195,\n",
              "  -7.166107177734375,\n",
              "  -7.203564167022705,\n",
              "  -7.779204368591309,\n",
              "  -7.191080570220947,\n",
              "  -7.781463623046875,\n",
              "  -7.474318981170654,\n",
              "  -7.421813488006592,\n",
              "  -7.5764031410217285,\n",
              "  -7.27677583694458,\n",
              "  -7.106028079986572,\n",
              "  -7.29417085647583,\n",
              "  -7.066132068634033,\n",
              "  -7.1400346755981445,\n",
              "  -7.430233001708984,\n",
              "  -7.136497497558594,\n",
              "  -7.009721755981445,\n",
              "  -7.200923919677734,\n",
              "  -6.962040424346924,\n",
              "  -7.481955528259277,\n",
              "  -6.77484130859375,\n",
              "  -7.195628643035889,\n",
              "  -7.13194465637207,\n",
              "  -7.050217628479004,\n",
              "  -7.168835163116455,\n",
              "  -7.471381187438965,\n",
              "  -7.373802185058594,\n",
              "  -6.987943649291992,\n",
              "  -7.30846643447876,\n",
              "  -7.243645668029785,\n",
              "  -7.052271366119385,\n",
              "  -7.361171722412109,\n",
              "  -7.063816547393799,\n",
              "  -7.373736381530762,\n",
              "  -7.099964618682861,\n",
              "  -7.215658664703369,\n",
              "  -7.266955375671387,\n",
              "  -7.485812187194824,\n",
              "  -7.2930169105529785,\n",
              "  -7.437164306640625,\n",
              "  -7.115522861480713,\n",
              "  -7.327226638793945,\n",
              "  -7.476080417633057,\n",
              "  -7.285824775695801,\n",
              "  -7.50970458984375,\n",
              "  -7.238647937774658,\n",
              "  -7.181107997894287,\n",
              "  -7.63340425491333,\n",
              "  -7.140899181365967,\n",
              "  -7.436427593231201,\n",
              "  -6.915651798248291,\n",
              "  -7.1570281982421875,\n",
              "  -7.280435085296631,\n",
              "  -7.0891547203063965,\n",
              "  -7.158804893493652,\n",
              "  -7.190170764923096,\n",
              "  -7.179023742675781,\n",
              "  -7.00313663482666,\n",
              "  -7.285295009613037,\n",
              "  -7.325260162353516,\n",
              "  -6.994484901428223,\n",
              "  -7.377373695373535,\n",
              "  -7.4471025466918945,\n",
              "  -7.318938732147217,\n",
              "  -7.0045976638793945,\n",
              "  -6.978442192077637,\n",
              "  -7.393021106719971,\n",
              "  -7.0133185386657715,\n",
              "  -6.9807658195495605,\n",
              "  -7.430241584777832,\n",
              "  -7.094728469848633,\n",
              "  -7.105677604675293,\n",
              "  -7.205089569091797,\n",
              "  -7.524652481079102,\n",
              "  -7.0973005294799805,\n",
              "  -6.974724292755127,\n",
              "  -7.365541458129883,\n",
              "  -7.248871803283691,\n",
              "  -7.608558654785156,\n",
              "  -7.440265655517578,\n",
              "  -7.4304399490356445,\n",
              "  -6.871267795562744,\n",
              "  -7.411550521850586,\n",
              "  -7.367156505584717,\n",
              "  -7.063403606414795,\n",
              "  -7.099436283111572,\n",
              "  -7.363470554351807,\n",
              "  -7.50927734375,\n",
              "  -7.2176618576049805,\n",
              "  -7.222165107727051,\n",
              "  -7.252640724182129,\n",
              "  -7.158705711364746,\n",
              "  -6.967294692993164,\n",
              "  -7.641106128692627,\n",
              "  -7.469723701477051,\n",
              "  -7.487886428833008,\n",
              "  -7.139671325683594,\n",
              "  -7.0963239669799805,\n",
              "  -7.389526844024658,\n",
              "  -7.245285987854004,\n",
              "  -6.935495853424072,\n",
              "  -7.11459493637085,\n",
              "  -7.259333610534668,\n",
              "  -7.197555065155029,\n",
              "  -7.160487174987793,\n",
              "  -7.225067138671875,\n",
              "  -7.105464458465576,\n",
              "  -7.327035427093506,\n",
              "  -7.043182373046875,\n",
              "  -7.398984432220459,\n",
              "  -7.245623588562012,\n",
              "  -7.260496139526367,\n",
              "  -7.296778678894043,\n",
              "  -6.883026123046875,\n",
              "  -7.033570289611816,\n",
              "  -7.269052505493164,\n",
              "  -7.295149326324463,\n",
              "  -7.376094341278076,\n",
              "  -7.502945899963379,\n",
              "  -7.582644939422607,\n",
              "  -7.060188293457031,\n",
              "  -7.204625606536865,\n",
              "  -7.224959373474121,\n",
              "  -7.4958038330078125,\n",
              "  -7.052979469299316,\n",
              "  -7.38541316986084,\n",
              "  -7.2274346351623535,\n",
              "  -7.4288740158081055,\n",
              "  -7.361794948577881,\n",
              "  -7.397957801818848,\n",
              "  -7.301326274871826,\n",
              "  -7.324789047241211,\n",
              "  -7.165752410888672,\n",
              "  -7.798590183258057,\n",
              "  -7.326921463012695,\n",
              "  -7.382437705993652,\n",
              "  -7.07729434967041,\n",
              "  -7.089337348937988,\n",
              "  -7.2618889808654785,\n",
              "  -7.129967212677002,\n",
              "  -7.3447747230529785,\n",
              "  -7.167844295501709,\n",
              "  -6.6810197830200195,\n",
              "  -7.217626571655273,\n",
              "  -7.279526710510254,\n",
              "  -7.213587760925293,\n",
              "  -6.966460227966309,\n",
              "  -7.210701942443848,\n",
              "  -7.223240852355957,\n",
              "  -6.9327006340026855,\n",
              "  -7.372717380523682,\n",
              "  -7.245365619659424,\n",
              "  -7.10618257522583,\n",
              "  -6.997257709503174,\n",
              "  -7.154492378234863,\n",
              "  -7.617628574371338,\n",
              "  -7.330584526062012,\n",
              "  -6.995176792144775,\n",
              "  -7.2648725509643555,\n",
              "  -7.681300163269043,\n",
              "  -7.1226911544799805,\n",
              "  -7.318460941314697,\n",
              "  -7.616877555847168,\n",
              "  -6.965316295623779,\n",
              "  -7.259325981140137,\n",
              "  -7.386741638183594,\n",
              "  -7.270920753479004,\n",
              "  -7.551469802856445,\n",
              "  -7.423323154449463,\n",
              "  -7.314582347869873,\n",
              "  -7.278799533843994,\n",
              "  -7.393364906311035,\n",
              "  -7.131436347961426,\n",
              "  -7.262087821960449,\n",
              "  -7.38703727722168,\n",
              "  -7.230414867401123,\n",
              "  -7.238327980041504,\n",
              "  -7.409055709838867,\n",
              "  -7.281980991363525,\n",
              "  -6.980426788330078,\n",
              "  -7.474510669708252,\n",
              "  -7.177278995513916,\n",
              "  -7.542006969451904,\n",
              "  -7.430920124053955,\n",
              "  -7.357624053955078,\n",
              "  -7.251248359680176,\n",
              "  -7.115992546081543,\n",
              "  -7.422489166259766,\n",
              "  -7.092336654663086,\n",
              "  -7.022719860076904,\n",
              "  -6.95767068862915,\n",
              "  -7.249779224395752,\n",
              "  -7.159074783325195,\n",
              "  -7.082794666290283,\n",
              "  -7.302656650543213,\n",
              "  -7.201391696929932,\n",
              "  -7.322018623352051,\n",
              "  -7.469831466674805,\n",
              "  -7.33961296081543,\n",
              "  -7.219664096832275,\n",
              "  -7.155316352844238,\n",
              "  -6.900455474853516,\n",
              "  -7.430747032165527,\n",
              "  -6.95295524597168,\n",
              "  -7.467639923095703,\n",
              "  -6.693129062652588,\n",
              "  -7.333777904510498,\n",
              "  -7.074520587921143,\n",
              "  -7.118534564971924,\n",
              "  -7.087194442749023,\n",
              "  -7.19998025894165,\n",
              "  -7.160529136657715,\n",
              "  -7.561229228973389,\n",
              "  -7.079044342041016,\n",
              "  -7.316103935241699,\n",
              "  -7.170954704284668,\n",
              "  -7.187934875488281,\n",
              "  -7.399344444274902,\n",
              "  -7.245889663696289,\n",
              "  -6.839908599853516,\n",
              "  -7.324899673461914,\n",
              "  -7.318681240081787,\n",
              "  -7.18949556350708,\n",
              "  -7.064545631408691,\n",
              "  -7.344367980957031,\n",
              "  -7.516765594482422,\n",
              "  -7.393536567687988,\n",
              "  -7.46519136428833,\n",
              "  -7.212954044342041,\n",
              "  -7.361368656158447,\n",
              "  -7.513140678405762,\n",
              "  -6.966747760772705,\n",
              "  -7.112118244171143,\n",
              "  -6.985881328582764,\n",
              "  -7.293840408325195,\n",
              "  -7.0909600257873535,\n",
              "  -7.059104919433594,\n",
              "  -7.296266078948975,\n",
              "  -7.175398826599121,\n",
              "  -7.289177417755127,\n",
              "  -7.211140155792236,\n",
              "  -7.06644868850708,\n",
              "  -7.2718353271484375,\n",
              "  -7.349620342254639,\n",
              "  -7.289695739746094,\n",
              "  -7.474530220031738,\n",
              "  -6.686697006225586,\n",
              "  -7.12471866607666,\n",
              "  -7.148351192474365,\n",
              "  -7.149146556854248,\n",
              "  -7.006503582000732,\n",
              "  -7.1659255027771,\n",
              "  -6.979057788848877,\n",
              "  -7.723407745361328,\n",
              "  -7.171741008758545,\n",
              "  -7.6031975746154785,\n",
              "  -7.483510971069336,\n",
              "  -7.2086968421936035,\n",
              "  -7.189389228820801,\n",
              "  -6.87446403503418,\n",
              "  -7.308724880218506,\n",
              "  -7.577805519104004,\n",
              "  -7.313190937042236,\n",
              "  -7.340593338012695,\n",
              "  -7.411319732666016,\n",
              "  -7.546138763427734,\n",
              "  -6.9650163650512695,\n",
              "  -7.771251678466797,\n",
              "  -7.3447370529174805,\n",
              "  -6.991724967956543,\n",
              "  -7.050684452056885,\n",
              "  -7.345148086547852,\n",
              "  -7.536789894104004,\n",
              "  -7.370731353759766,\n",
              "  -7.523568153381348,\n",
              "  -7.315032958984375,\n",
              "  -7.1669440269470215,\n",
              "  -7.262940406799316,\n",
              "  -7.4740753173828125,\n",
              "  -7.171669006347656,\n",
              "  -6.9284563064575195,\n",
              "  -7.369256019592285,\n",
              "  -7.240876197814941,\n",
              "  -7.3361968994140625,\n",
              "  -6.975087642669678,\n",
              "  -7.425135135650635,\n",
              "  -6.9572434425354,\n",
              "  -7.189046382904053,\n",
              "  -7.0298895835876465,\n",
              "  -7.361050128936768,\n",
              "  -7.066605567932129,\n",
              "  -7.127838134765625,\n",
              "  -7.256416320800781,\n",
              "  -7.144833087921143,\n",
              "  -7.600654602050781,\n",
              "  -7.470407485961914,\n",
              "  -7.517834186553955,\n",
              "  -7.601852893829346,\n",
              "  -7.3636884689331055,\n",
              "  -7.225089073181152,\n",
              "  -6.9261345863342285,\n",
              "  -7.096405982971191,\n",
              "  -7.118226051330566,\n",
              "  -7.288202285766602,\n",
              "  -7.26779317855835,\n",
              "  -7.361545562744141,\n",
              "  -7.02256965637207,\n",
              "  -6.96123743057251,\n",
              "  -7.183351039886475,\n",
              "  -7.466504096984863,\n",
              "  -7.252837181091309,\n",
              "  -7.410321235656738,\n",
              "  -7.453920364379883,\n",
              "  -7.2304463386535645,\n",
              "  -7.21187162399292,\n",
              "  -7.1802263259887695,\n",
              "  -6.969366550445557,\n",
              "  -7.250768184661865,\n",
              "  -7.127891540527344,\n",
              "  -7.240535736083984,\n",
              "  -7.021566390991211,\n",
              "  -7.02305269241333,\n",
              "  -7.17764949798584,\n",
              "  -6.836060523986816,\n",
              "  -7.67177677154541,\n",
              "  -6.984362602233887,\n",
              "  -7.057917594909668,\n",
              "  -7.198048114776611,\n",
              "  -7.315116882324219,\n",
              "  -7.298600196838379,\n",
              "  -7.144222259521484,\n",
              "  -7.098690032958984,\n",
              "  -7.185244083404541,\n",
              "  -7.26486349105835,\n",
              "  -7.169243812561035,\n",
              "  -7.406734943389893,\n",
              "  -7.106565475463867,\n",
              "  -7.528829574584961,\n",
              "  -6.963596343994141,\n",
              "  -7.414902687072754,\n",
              "  -7.463470935821533,\n",
              "  -7.374734401702881,\n",
              "  -7.363927841186523,\n",
              "  -7.158053398132324,\n",
              "  -7.070102691650391,\n",
              "  -7.365085601806641,\n",
              "  -6.973453044891357,\n",
              "  -7.179757118225098,\n",
              "  -7.4753618240356445,\n",
              "  -7.37962532043457,\n",
              "  -7.001655578613281,\n",
              "  -7.250010013580322,\n",
              "  -7.384281635284424,\n",
              "  -7.533566951751709,\n",
              "  -7.484801292419434,\n",
              "  -7.227678298950195,\n",
              "  -7.14302396774292,\n",
              "  -7.35493278503418,\n",
              "  -7.27591609954834,\n",
              "  -7.609445571899414,\n",
              "  -6.862695693969727,\n",
              "  -7.234394550323486,\n",
              "  -7.279574394226074,\n",
              "  -7.48305606842041,\n",
              "  -7.158487796783447,\n",
              "  -7.319602012634277,\n",
              "  -7.110131740570068,\n",
              "  -7.306009292602539,\n",
              "  -7.073163986206055,\n",
              "  -7.078333854675293,\n",
              "  -7.085849285125732,\n",
              "  -7.041421890258789,\n",
              "  -7.386442184448242,\n",
              "  -7.272902965545654,\n",
              "  -7.102321624755859,\n",
              "  -7.197061538696289,\n",
              "  -7.0038299560546875,\n",
              "  -7.3616228103637695,\n",
              "  -7.273784160614014,\n",
              "  -7.281752109527588,\n",
              "  -7.19923210144043,\n",
              "  -6.9395341873168945,\n",
              "  -7.3347649574279785,\n",
              "  -7.038036823272705,\n",
              "  -7.331955909729004,\n",
              "  -7.122969627380371,\n",
              "  -7.054495811462402,\n",
              "  -7.20076847076416,\n",
              "  -7.452136993408203,\n",
              "  -7.433382987976074,\n",
              "  -7.463641166687012,\n",
              "  -7.256852149963379,\n",
              "  -7.4050726890563965,\n",
              "  -7.646018981933594,\n",
              "  -7.760924816131592,\n",
              "  -7.205141544342041,\n",
              "  -7.059688568115234,\n",
              "  -7.151745796203613,\n",
              "  -6.995426654815674,\n",
              "  -7.316063404083252,\n",
              "  -7.226539611816406,\n",
              "  -7.2122039794921875,\n",
              "  -7.564406871795654,\n",
              "  -7.184212684631348,\n",
              "  -7.039325714111328,\n",
              "  -7.513646602630615,\n",
              "  -7.313645362854004,\n",
              "  -7.145884990692139,\n",
              "  -7.41266393661499,\n",
              "  -7.134120941162109,\n",
              "  -7.3835978507995605,\n",
              "  -7.460017681121826,\n",
              "  -7.338464736938477,\n",
              "  -7.205307960510254,\n",
              "  -6.900976657867432,\n",
              "  -7.044234275817871,\n",
              "  -7.1332879066467285,\n",
              "  -7.250657558441162,\n",
              "  -7.301755428314209,\n",
              "  -7.552196025848389,\n",
              "  -7.269940376281738,\n",
              "  -7.366800308227539,\n",
              "  -7.424576282501221,\n",
              "  -7.072394847869873,\n",
              "  -6.963745594024658,\n",
              "  -7.381260395050049,\n",
              "  -6.904967784881592,\n",
              "  -7.167247295379639,\n",
              "  -6.964133262634277,\n",
              "  -7.211617469787598,\n",
              "  -7.028511047363281,\n",
              "  -7.002531051635742,\n",
              "  -7.099163055419922,\n",
              "  -6.974881649017334,\n",
              "  -7.1310319900512695,\n",
              "  -6.8113322257995605,\n",
              "  -7.26743221282959,\n",
              "  -7.562164783477783,\n",
              "  -6.8094658851623535,\n",
              "  -7.122282981872559,\n",
              "  -7.142509460449219,\n",
              "  -7.0665059089660645,\n",
              "  -7.481668472290039,\n",
              "  -7.199426651000977,\n",
              "  -7.104043483734131,\n",
              "  -7.575847148895264,\n",
              "  -7.271899700164795,\n",
              "  -7.428465843200684,\n",
              "  -7.1832499504089355,\n",
              "  -7.080334663391113,\n",
              "  -7.38742733001709,\n",
              "  -7.105607032775879,\n",
              "  -7.0091400146484375,\n",
              "  -7.328202724456787,\n",
              "  -7.359449863433838,\n",
              "  -7.178736209869385,\n",
              "  -7.1893439292907715,\n",
              "  -7.188291072845459,\n",
              "  -7.492094993591309,\n",
              "  -6.974039077758789,\n",
              "  -7.420698642730713,\n",
              "  -7.431416034698486,\n",
              "  -7.176231384277344,\n",
              "  -7.3045172691345215,\n",
              "  -7.4639458656311035,\n",
              "  -7.29480504989624,\n",
              "  -7.327073097229004,\n",
              "  -7.170724868774414,\n",
              "  -7.249957084655762,\n",
              "  -7.339507102966309,\n",
              "  -7.404926300048828,\n",
              "  -7.13072395324707,\n",
              "  -7.038998603820801,\n",
              "  -7.43953800201416,\n",
              "  -7.294090270996094,\n",
              "  -7.066516876220703,\n",
              "  -7.024511337280273,\n",
              "  -7.223001003265381,\n",
              "  -7.291325569152832,\n",
              "  -7.049251079559326,\n",
              "  -7.03463077545166,\n",
              "  -7.478704452514648,\n",
              "  -7.413233280181885,\n",
              "  -7.454856872558594,\n",
              "  -7.456928730010986,\n",
              "  -7.325929164886475,\n",
              "  -6.648335933685303,\n",
              "  -7.14082670211792,\n",
              "  -7.357885360717773,\n",
              "  -7.293583869934082,\n",
              "  -7.358081817626953,\n",
              "  -7.242445945739746,\n",
              "  -7.241077899932861,\n",
              "  -7.3465576171875,\n",
              "  -7.452628135681152,\n",
              "  -7.225165843963623,\n",
              "  -7.33937931060791,\n",
              "  -7.653511047363281,\n",
              "  -7.337236404418945,\n",
              "  -7.127814292907715,\n",
              "  -6.928298473358154,\n",
              "  -7.31424617767334,\n",
              "  -7.05818510055542,\n",
              "  -7.013719081878662,\n",
              "  -7.094305515289307,\n",
              "  -7.301793098449707,\n",
              "  -7.250346660614014,\n",
              "  -7.122818470001221,\n",
              "  -7.251520156860352,\n",
              "  -7.2121710777282715,\n",
              "  -7.060132026672363,\n",
              "  -7.049602508544922,\n",
              "  -7.213933944702148,\n",
              "  -7.351194858551025,\n",
              "  -6.928126811981201,\n",
              "  -7.086975574493408,\n",
              "  -7.282971382141113,\n",
              "  -7.383744716644287,\n",
              "  -7.343928337097168,\n",
              "  -7.099295139312744,\n",
              "  -7.289536476135254,\n",
              "  -7.435539245605469,\n",
              "  -7.163745403289795,\n",
              "  -7.240650177001953,\n",
              "  -7.490350723266602,\n",
              "  -6.895687580108643,\n",
              "  -7.331986427307129,\n",
              "  -7.285411357879639,\n",
              "  -7.115048885345459,\n",
              "  -7.1903557777404785,\n",
              "  -7.056935787200928,\n",
              "  -7.354929447174072,\n",
              "  -7.480047225952148,\n",
              "  -7.30922269821167,\n",
              "  -7.246387004852295,\n",
              "  -7.224791526794434,\n",
              "  -7.398483753204346,\n",
              "  -7.2599687576293945,\n",
              "  -7.416993141174316,\n",
              "  -7.0447540283203125,\n",
              "  -7.436898231506348,\n",
              "  -7.041558265686035,\n",
              "  -7.2603230476379395,\n",
              "  -6.888552188873291,\n",
              "  -7.3926100730896,\n",
              "  -7.150190830230713,\n",
              "  -7.372984886169434,\n",
              "  -7.180075645446777,\n",
              "  -7.590694427490234,\n",
              "  -6.975663661956787,\n",
              "  -7.263916015625,\n",
              "  -7.308286190032959,\n",
              "  -7.054848670959473,\n",
              "  -7.141740322113037,\n",
              "  -7.235564231872559,\n",
              "  -7.114469051361084,\n",
              "  -7.2716803550720215,\n",
              "  -7.411647319793701,\n",
              "  -7.067715644836426,\n",
              "  -7.330615043640137,\n",
              "  -7.339725494384766,\n",
              "  -7.081201553344727,\n",
              "  -7.338649749755859,\n",
              "  -7.352411270141602,\n",
              "  -7.3709259033203125,\n",
              "  -7.40939998626709,\n",
              "  -7.016454696655273,\n",
              "  -6.909122467041016,\n",
              "  -7.379941463470459,\n",
              "  -7.345473289489746,\n",
              "  -7.189651966094971,\n",
              "  -7.244022846221924,\n",
              "  -6.95016622543335,\n",
              "  -6.762763977050781,\n",
              "  -7.355378150939941,\n",
              "  -7.279730796813965,\n",
              "  -7.156859397888184,\n",
              "  -7.209911346435547,\n",
              "  -6.921417713165283,\n",
              "  -7.17275333404541,\n",
              "  -7.357174396514893,\n",
              "  -7.4391350746154785,\n",
              "  -7.230803966522217,\n",
              "  -7.248596668243408,\n",
              "  -7.644692420959473,\n",
              "  -7.352889060974121,\n",
              "  -7.472225189208984,\n",
              "  -7.495628356933594,\n",
              "  -7.169580936431885,\n",
              "  -6.932801723480225,\n",
              "  -7.228632926940918,\n",
              "  -7.236899375915527,\n",
              "  -7.30763053894043,\n",
              "  -7.307784080505371,\n",
              "  -7.368146896362305,\n",
              "  -7.610940456390381,\n",
              "  -7.368668556213379,\n",
              "  -7.183122158050537,\n",
              "  -7.007974147796631,\n",
              "  -7.131734848022461,\n",
              "  -7.213677883148193,\n",
              "  -7.496346950531006,\n",
              "  -7.24212646484375,\n",
              "  -7.193650722503662,\n",
              "  -7.077475070953369,\n",
              "  -7.223639011383057,\n",
              "  -7.656237602233887,\n",
              "  -7.518597602844238,\n",
              "  -7.1577558517456055,\n",
              "  -7.033602237701416,\n",
              "  -7.306270122528076,\n",
              "  -7.639377593994141,\n",
              "  -7.6050591468811035,\n",
              "  -7.110312461853027,\n",
              "  -7.245753765106201,\n",
              "  -7.498702049255371,\n",
              "  -7.075976371765137,\n",
              "  -7.301918029785156,\n",
              "  -7.47318172454834,\n",
              "  -7.213127136230469,\n",
              "  -7.175893783569336,\n",
              "  -7.0854034423828125,\n",
              "  -6.901096820831299,\n",
              "  -6.948753356933594,\n",
              "  -7.088096618652344,\n",
              "  -7.291491985321045,\n",
              "  -7.173427581787109,\n",
              "  -7.351341247558594,\n",
              "  -7.710894584655762,\n",
              "  -6.886628150939941,\n",
              "  -7.305314540863037,\n",
              "  -7.16585111618042,\n",
              "  -7.383520126342773,\n",
              "  -7.213738441467285,\n",
              "  -7.159567356109619,\n",
              "  -7.516751766204834,\n",
              "  -7.112610816955566,\n",
              "  -7.147477626800537,\n",
              "  -6.817224979400635,\n",
              "  -7.335650444030762,\n",
              "  -7.4475250244140625,\n",
              "  -7.3772783279418945,\n",
              "  -7.451923370361328,\n",
              "  -6.841258525848389,\n",
              "  -7.317354202270508,\n",
              "  -7.481200695037842,\n",
              "  -7.456534385681152,\n",
              "  -7.417474269866943,\n",
              "  -7.435640811920166,\n",
              "  -7.226923942565918,\n",
              "  -6.844150543212891,\n",
              "  -7.431881904602051,\n",
              "  -7.150961875915527,\n",
              "  -7.194202899932861,\n",
              "  -7.369197845458984,\n",
              "  -7.059576988220215,\n",
              "  -7.205464839935303,\n",
              "  -7.381193161010742,\n",
              "  -6.964306831359863,\n",
              "  -7.284458160400391,\n",
              "  -7.314030647277832,\n",
              "  -7.524129867553711,\n",
              "  -7.398922443389893,\n",
              "  -7.316673755645752,\n",
              "  -7.433444023132324,\n",
              "  -7.478468418121338,\n",
              "  -7.4301862716674805,\n",
              "  -7.284183025360107,\n",
              "  -7.243463516235352,\n",
              "  -7.061458110809326,\n",
              "  -7.6173858642578125,\n",
              "  -6.841925621032715,\n",
              "  -7.427160739898682,\n",
              "  -7.2381086349487305,\n",
              "  -7.087848663330078,\n",
              "  -7.403106689453125,\n",
              "  -7.667107582092285,\n",
              "  -7.064657688140869,\n",
              "  -6.891005039215088,\n",
              "  -7.240043640136719,\n",
              "  -7.366292476654053,\n",
              "  -6.947924613952637,\n",
              "  -7.130884647369385,\n",
              "  -7.187954902648926,\n",
              "  -6.918641567230225,\n",
              "  -7.260212421417236,\n",
              "  -7.359136581420898,\n",
              "  -7.179854869842529,\n",
              "  -7.324974060058594,\n",
              "  -7.1791863441467285,\n",
              "  -7.045447826385498,\n",
              "  -7.204708576202393,\n",
              "  -7.33836555480957,\n",
              "  -7.316341876983643,\n",
              "  -7.235548973083496,\n",
              "  -7.2733964920043945,\n",
              "  -7.222285270690918,\n",
              "  -7.275942802429199,\n",
              "  -7.452084541320801,\n",
              "  -7.519518852233887,\n",
              "  -7.558443069458008,\n",
              "  -7.023521900177002,\n",
              "  -7.362209320068359,\n",
              "  -6.983650207519531,\n",
              "  -7.082257270812988,\n",
              "  -7.2791266441345215,\n",
              "  -7.223942279815674,\n",
              "  -7.427756309509277,\n",
              "  -7.4293084144592285,\n",
              "  -7.030309677124023,\n",
              "  -7.398303031921387,\n",
              "  -7.130619049072266,\n",
              "  -7.256571292877197,\n",
              "  -7.20404052734375,\n",
              "  -7.078457832336426,\n",
              "  -6.892165660858154,\n",
              "  -7.373948097229004,\n",
              "  -7.472942352294922,\n",
              "  -7.49110221862793,\n",
              "  -7.242998123168945,\n",
              "  -7.231478214263916,\n",
              "  -7.272985935211182,\n",
              "  -7.197732925415039,\n",
              "  -7.483334541320801,\n",
              "  -7.411539077758789,\n",
              "  -7.158652305603027,\n",
              "  -7.32835054397583,\n",
              "  -7.116270065307617,\n",
              "  -7.4329514503479,\n",
              "  -7.299526691436768,\n",
              "  -7.482754707336426,\n",
              "  -7.534564971923828,\n",
              "  -7.16980504989624,\n",
              "  -7.5238566398620605,\n",
              "  -7.2627482414245605,\n",
              "  -7.295309066772461,\n",
              "  -7.3267011642456055,\n",
              "  -7.270129203796387,\n",
              "  -7.088161945343018,\n",
              "  -7.42545747756958,\n",
              "  -6.967304229736328,\n",
              "  -6.950099945068359,\n",
              "  -7.412778854370117,\n",
              "  -7.358295917510986,\n",
              "  -7.479231834411621,\n",
              "  -7.224462032318115,\n",
              "  -7.284972190856934,\n",
              "  -7.063566207885742,\n",
              "  -7.334476947784424,\n",
              "  -6.858274459838867,\n",
              "  -7.298583984375,\n",
              "  -7.204380512237549,\n",
              "  -7.268913745880127,\n",
              "  -7.401087760925293,\n",
              "  -7.709771156311035,\n",
              "  -7.1512980461120605,\n",
              "  -7.333876132965088,\n",
              "  -7.438026428222656,\n",
              "  -7.157339572906494,\n",
              "  -7.366542339324951,\n",
              "  -7.184098243713379,\n",
              "  -6.775670051574707,\n",
              "  -7.095685005187988,\n",
              "  -7.235982418060303,\n",
              "  -7.251470565795898,\n",
              "  -7.428977012634277,\n",
              "  -7.135228157043457,\n",
              "  -7.244036674499512,\n",
              "  -7.217253684997559,\n",
              "  -7.158841133117676,\n",
              "  -7.199827671051025,\n",
              "  -7.364668369293213,\n",
              "  -7.349539756774902,\n",
              "  -7.441633224487305,\n",
              "  -7.009006023406982,\n",
              "  -7.448707580566406,\n",
              "  -7.068716049194336,\n",
              "  -7.417631149291992,\n",
              "  -7.171753883361816,\n",
              "  -7.495383262634277,\n",
              "  -7.448950290679932,\n",
              "  -7.1742024421691895,\n",
              "  -7.486012935638428,\n",
              "  -7.145923614501953,\n",
              "  -7.334001064300537,\n",
              "  -7.49180269241333,\n",
              "  -7.29700231552124,\n",
              "  -7.681576251983643,\n",
              "  -7.171334266662598,\n",
              "  -7.126371383666992,\n",
              "  -7.606067180633545,\n",
              "  -7.170644283294678,\n",
              "  -7.29827356338501,\n",
              "  -7.20458459854126,\n",
              "  -7.257924556732178,\n",
              "  -7.1937336921691895,\n",
              "  -7.641831398010254,\n",
              "  -7.270956039428711,\n",
              "  -7.165518760681152,\n",
              "  -7.454109191894531,\n",
              "  -7.510641098022461,\n",
              "  -7.151407241821289,\n",
              "  -7.549105644226074,\n",
              "  -7.447003364562988,\n",
              "  -7.43063497543335,\n",
              "  -7.5644097328186035,\n",
              "  -7.14963960647583,\n",
              "  -7.217658996582031,\n",
              "  -7.434653282165527,\n",
              "  -7.527599334716797,\n",
              "  -7.3692545890808105,\n",
              "  -7.116125583648682,\n",
              "  -7.364466190338135,\n",
              "  -7.265316963195801,\n",
              "  -7.238376617431641,\n",
              "  -7.748138427734375,\n",
              "  -7.127127647399902,\n",
              "  -7.498291969299316,\n",
              "  -7.092867851257324,\n",
              "  -7.367487907409668,\n",
              "  -6.981930255889893,\n",
              "  -7.684615135192871,\n",
              "  -7.1060380935668945,\n",
              "  -7.258304119110107,\n",
              "  -6.898346424102783,\n",
              "  -7.234710693359375,\n",
              "  -7.319488525390625,\n",
              "  -7.266525745391846,\n",
              "  -7.3780903816223145,\n",
              "  -7.1661505699157715,\n",
              "  -7.120601654052734,\n",
              "  -6.940621376037598,\n",
              "  -7.014832019805908,\n",
              "  -7.284252166748047,\n",
              "  -7.5874128341674805,\n",
              "  -7.19484281539917,\n",
              "  -7.271828651428223,\n",
              "  -7.260828018188477,\n",
              "  -7.27202033996582,\n",
              "  -7.225076675415039,\n",
              "  -7.26385498046875,\n",
              "  -7.506111145019531,\n",
              "  -7.282087326049805,\n",
              "  -7.15187406539917,\n",
              "  ...]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtuFjTH_6p5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf2e37ba-072b-40d8-ab95-4c2ad0bac7ca"
      },
      "source": [
        "# Select the index with highest softmax probabilities\n",
        "# See https://pytorch.org/docs/stable/torch.html#torch.max\n",
        "torch.max(softmax(lin2(h_x)), 1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([-6.6483], grad_fn=<MaxBackward0>), indices=tensor([642]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcfWwZk67rG",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-train-cbow\"></a>\n",
        "\n",
        "# Now, we train the CBOW model for real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRig9kyI61Xn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b1e0350-1772-45f2-b718-529ea65acddf"
      },
      "source": [
        "# First we split the data into training and testing.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
        "len(tokenized_text_train), len(tokenized_text_test)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfzjsjoq7Cvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Put the input context indices into the embeddings\n",
        "        # then squeeze it into a single dimension vector with tensor.view((1,-1))\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        # Put the embedding input through linear layer,\n",
        "        # then an activation function to create the hidden layer.\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        # Put the hidden layer through a second linear layer,\n",
        "        out = self.linear2(hid)\n",
        "        # then a last layer activation function to generate\n",
        "        # pobabilities, hint https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNm40ApS77al",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4ba30ae-936d-4372-9268-9ae75828d39f"
      },
      "source": [
        "embd_size = 100\n",
        "learning_rate = 0.003\n",
        "hidden_size = 100\n",
        "window_size = 2\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the dataset.\n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
        "vocab_size = len(w2v_dataset.vocab)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "# Hint: the CBOW model object you've created.\n",
        "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "\n",
        "model = nn.DataParallel(model).to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "for _e in tqdm(range(num_epochs)):\n",
        "    epoch_loss = []\n",
        "    for sent_idx in range(w2v_dataset._len):\n",
        "        for w2v_io in w2v_dataset[sent_idx]:\n",
        "            # Zero gradient.\n",
        "            optimizer.zero_grad()\n",
        "            # Retrieve the inputs and outputs.\n",
        "            x, y = w2v_io['x'], w2v_io['y']\n",
        "            x = tensor(x).to(device)\n",
        "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
        "            # Calculate the log probability of the context embeddings.\n",
        "            logprobs = model(x)\n",
        "            # This unsqueeze thing is really a feature/bug... -_-\n",
        "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss.append(float(loss))\n",
        "    # Save model after every epoch.\n",
        "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
        "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
        "  \n",
        "# Save model after last epoch.\n",
        "#torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [07:24<00:00,  4.41s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lGV7nj9Q4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "1129d19d-eb10-4c20-b26f-54846910b135"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set(rc={'figure.figsize':(12, 8)})\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAHUCAYAAADY9fvpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBQAAgKMReAEAAOBo/x9xbhcdpFWdDAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STJnFmVfp_D7",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
        "\n",
        "# Apply and Evaluate the CBOW Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JbTG84xBeiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9f3614a-ef5a-4eea-b50d-bd2607bbc2d9"
      },
      "source": [
        "from lazyme import color_str\n",
        "\n",
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            # Remember how to get the best prediction output? \n",
        "            # Hint: https://pytorch.org/docs/stable/torch.html#torch.max\n",
        "            _, prediction =  torch.max(model(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91mthat\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mlexicon\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[91mand\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91mmore\u001b[0m ( or\n",
            "\u001b[92m(\u001b[0m \t\t a word \u001b[91mword\u001b[0m or bigram\n",
            "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mand\u001b[0m bigram ,\n",
            "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mplan\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91m1999\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[91mfor\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mof\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mmethods\u001b[0m for many\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mbetween\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mrejected\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91menormous\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mfor\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m;\u001b[0m a ,\n",
            "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mverb\u001b[0m , p\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mor\u001b[0m methods are\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91massociated\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mfor\u001b[0m just those\n",
            "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mof\u001b[0m errors that\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[92mdo\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[91moften\u001b[0m wish to\n",
            "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91minappropriate\u001b[0m any scf\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91min\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mwould\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91msome\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91mjected\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91m______\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mmethods\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91m2000\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mpurchases\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mthen\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
            "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mover\u001b[0m out to\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mlanguage\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mclasses\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91msubset\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91m______\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91m‘\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mof\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91minformation\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91m______\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mprobability\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mstatistical\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mf\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mrandom\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mgenerated\u001b[0m , this\n",
            "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mamerican\u001b[0m out not\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mport\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mexplained\u001b[0m case .\n",
            "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91marbitrary\u001b[0m carroll 1997\n",
            "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
            "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mvery\u001b[0m corpora .\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mof\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mrandom\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mcritique\u001b[0m : is\n",
            "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mand\u001b[0m ⫺ 0.5\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mdata\u001b[0m greater than\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91m______\u001b[0m value ?\n",
            "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mnot\u001b[0m of statistical\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[92mnatural\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[92mlanguage\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mtask\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mstood\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mwith\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91m1999\u001b[0m h0 .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
            "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mis\u001b[0m by an\n",
            "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mrandomness\u001b[0m , or\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthe\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91m)\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mbe\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91ma-m\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91muse\u001b[0m of the\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mof\u001b[0m often an\n",
            "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mis\u001b[0m way to\n",
            "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mbuild\u001b[0m ; the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mis\u001b[0m where the\n",
            "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mevidence\u001b[0m is overlooked\n",
            "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
            "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
            "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
            "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[91m2005\u001b[0m ) ,\n",
            "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91m______\u001b[0m non-random and\n",
            "\u001b[92mnon-random\u001b[0m \t language is \u001b[91mnot\u001b[0m and hence\n",
            "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
            "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mrandom\u001b[0m , when\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[91msampson\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mand\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mbe\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mso\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
            "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m issue :\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mif\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mthere\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mhas\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91mthat\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mthe\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mrepeat\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mvery\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mis\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mnot\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mof\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91m______\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mused\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mbeen\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m(\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mshould\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mor\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mthat\u001b[0m hypothesis test\n",
            "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
            "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mwith\u001b[0m the fact\n",
            "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mdifferent\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mprobability\u001b[0m concern the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mis\u001b[0m between a\n",
            "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mthat\u001b[0m a linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91msubset\u001b[0m of a\n",
            "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mestablish\u001b[0m the relation\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91me\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mthat\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mlow\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mmle\u001b[0m and its\n",
            "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91m’\u001b[0m , as\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[92mrather\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mthe\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[92merror\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91mwhich\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mother\u001b[0m is then\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mby\u001b[0m the hypothesis\n",
            "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mcorrell\u001b[0m , be\n",
            "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mmarked\u001b[0m as :\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91mthen\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[91msame\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91m______\u001b[0m greater than\n",
            "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
            "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[91mof\u001b[0m 0.5 ?\n",
            "\u001b[92m1\u001b[0m \t\t with just \u001b[91m0.5\u001b[0m % of\n",
            "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91meach\u001b[0m of them\n",
            "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91m______\u001b[0m them ,\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mrandomness\u001b[0m , devastate\n",
            "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthis\u001b[0m one of\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mthese\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91massociation\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m(\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthen\u001b[0m we have\n",
            "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mnumber\u001b[0m of data\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mamerican\u001b[0m thresholding methods\n",
            "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mtwo\u001b[0m distinguish associated\n",
            "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91ma\u001b[0m associated scfs\n",
            "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mbe\u001b[0m scfs from\n",
            "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m______\u001b[0m from noise\n",
            "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mof\u001b[0m noise .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaYievSyqZsU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "abc576cf-606b-4261-f88d-727a569e6e1d"
      },
      "source": [
        "print(torch.max(model(x), 1))\n",
        "model(x)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([-0.6653], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([30], device='cuda:0'))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -8.6813,  -7.7840, -14.2867,  ...,  -9.4362, -11.2442, -10.4594]],\n",
              "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsKur-twqfnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b94805db-a9e2-4120-f13d-7d41480522e4"
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.28085106382978725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4rlSLIrs2pr",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-load-model\"></a>\n",
        "\n",
        "# Go back to the 5th Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcXHTSnlqn99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1020af93-4ca3-4a94-bff3-3d519d4e3268"
      },
      "source": [
        "model_5 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
        "model_5 = torch.nn.DataParallel(model_5)\n",
        "model_5.load_state_dict(torch.load('cbow_checkpoint_5.pt'))\n",
        "model_5.eval()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): CBOW(\n",
              "    (embeddings): Embedding(1303, 100)\n",
              "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
              "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlG1DRJHqt3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a781c1e2-ff42-494c-d620-65d11a7e050d"
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        \n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            _, prediction =  torch.max(model_5(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91mthat\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91m______\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[91mand\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91m2\u001b[0m ( or\n",
            "\u001b[92m(\u001b[0m \t\t a word \u001b[91mof\u001b[0m or bigram\n",
            "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mand\u001b[0m bigram ,\n",
            "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mthe\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mto\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[91mfor\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mof\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mof\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mthe\u001b[0m for many\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91mlanguage\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnot\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91mthe\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mrandom\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mand\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mand\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m)\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m______\u001b[0m a ,\n",
            "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mrandom\u001b[0m , p\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91mevents\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mthe\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mand\u001b[0m just those\n",
            "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mof\u001b[0m errors that\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mbe\u001b[0m wish to\n",
            "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mbe\u001b[0m any scf\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91min\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[92mthere\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91ma\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91m______\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91m______\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mthe\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mand\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mof\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91min\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
            "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mto\u001b[0m out to\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mthe\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91m______\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mthey\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91m______\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mto\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mnull\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mof\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91m______\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mprobability\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91m:\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mof\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91ma\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mthe\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mcorpora\u001b[0m , this\n",
            "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mto\u001b[0m out not\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mof\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mvery\u001b[0m case .\n",
            "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mand\u001b[0m carroll 1997\n",
            "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91mand\u001b[0m of subcategorization\n",
            "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mvery\u001b[0m corpora .\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mof\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mrandom\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mnull\u001b[0m : is\n",
            "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mand\u001b[0m ⫺ 0.5\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mrandom\u001b[0m greater than\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[92mthe\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91mrandom\u001b[0m value ?\n",
            "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mand\u001b[0m of statistical\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mof\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mlanguage\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91m)\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mas\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mis\u001b[0m h0 .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
            "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mis\u001b[0m by an\n",
            "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mthe\u001b[0m , or\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthe\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91m)\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mtwo\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtwo\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91mprobability\u001b[0m of the\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mof\u001b[0m often an\n",
            "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mis\u001b[0m way to\n",
            "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91m)\u001b[0m ; the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mis\u001b[0m where the\n",
            "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mlanguage\u001b[0m is overlooked\n",
            "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
            "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m2\u001b[0m ( 1\n",
            "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
            "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
            "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91m______\u001b[0m non-random and\n",
            "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
            "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
            "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mrandom\u001b[0m , when\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[91m(\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mwe\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mand\u001b[0m at linguistic\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mand\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[91mis\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mthe\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mand\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthe\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
            "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mrandom\u001b[0m issue :\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mwe\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mand\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91m______\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mthe\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mcorpus\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mthe\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mis\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mvery\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mof\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91m______\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mfor\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91min\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m(\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mdata\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mto\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[92mand\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
            "\u001b[92mhypothesis\u001b[0m \t and the \u001b[91mbritish\u001b[0m test con-\n",
            "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mwith\u001b[0m the fact\n",
            "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mvery\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91m______\u001b[0m concern the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mis\u001b[0m between a\n",
            "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mof\u001b[0m a linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mof\u001b[0m of a\n",
            "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mfor\u001b[0m the relation\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mof\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mthe\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91mevents\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfor\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mof\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mof\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
            "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91m’\u001b[0m , as\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mand\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mthe\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91m(\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mrandom\u001b[0m is then\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mby\u001b[0m the hypothesis\n",
            "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandom\u001b[0m , be\n",
            "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mand\u001b[0m as :\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91mthat\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91m______\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[91mnull\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mfor\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91m______\u001b[0m greater than\n",
            "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[91mand\u001b[0m than 0.5\n",
            "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
            "\u001b[92m1\u001b[0m \t\t with just \u001b[91min\u001b[0m % of\n",
            "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91ma\u001b[0m of them\n",
            "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mthe\u001b[0m them ,\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mthe\u001b[0m , devastate\n",
            "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthe\u001b[0m one of\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91mof\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthe\u001b[0m we have\n",
            "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mnumber\u001b[0m of data\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mamerican\u001b[0m thresholding methods\n",
            "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mthe\u001b[0m distinguish associated\n",
            "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mbe\u001b[0m associated scfs\n",
            "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mbe\u001b[0m scfs from\n",
            "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m______\u001b[0m from noise\n",
            "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mfor\u001b[0m noise .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3dj17lRtGtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "310a0256-d772-4bfc-a615-6b87832db7b3"
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)\n",
        "model(x).shape[1] == len(w2v_dataset.vocab)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2297872340425532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QGgLystN_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}