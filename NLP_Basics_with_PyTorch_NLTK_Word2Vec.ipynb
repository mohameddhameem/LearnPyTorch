{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Basics with PyTorch - NLTK - Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5Mj8VJ_mwpLM",
        "fuHeSkMiw1PB",
        "tdcgrrdky6P6",
        "9WiIkqfO1ndl",
        "6RCDPaz12EyN",
        "GhLyudWp2Uy1",
        "BHreIlYP3e2z",
        "uPxvwu4w3qYP",
        "n09Ap2Nq4oZ1",
        "974oU-pE5JYa"
      ],
      "authorship_tag": "ABX9TyMHbeXlkoI1fUT/iFCqI5Ae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohameddhameem/LearnPyTorch/blob/master/NLP_Basics_with_PyTorch_NLTK_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70OkV4V5paqN",
        "colab_type": "text"
      },
      "source": [
        "# Basics of NLP with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVd7SxQtplrw",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "\n",
        "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
        "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
        "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
        "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
        "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
        "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
        "<br><br>\n",
        "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
        "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
        "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
        "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
        "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
        "    - <a href=\"#section-3-1-4-fill-cbow\">The CBOW model</a>\n",
        "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
        "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
        "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
        "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
        "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
        "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
        "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
        "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
        "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
        "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
        "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
        "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
        "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
        "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZGAC0BHp0eZ",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0\"></a>\n",
        "# 3.0. Data Preparation\n",
        "\n",
        "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
        "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
        "\n",
        "There are already several other libraries that help with loading text datasets, e.g. \n",
        "\n",
        " - FastAI https://docs.fast.ai/text.data.html\n",
        " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
        " - Torch Text https://github.com/pytorch/text#data\n",
        " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
        " - SpaCy https://github.com/explosion/thinc\n",
        " \n",
        "\n",
        "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cltZ9XdoqGP1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-1\"></a>\n",
        "## 3.0.1  Vocabulary\n",
        "\n",
        "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmM3NtDFwFEY",
        "colab_type": "code",
        "outputId": "98f6ec8c-5b78-4e08-de82-49ff484e7b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!pip install sklearn torch tqdm nltk lazyme ansi requests gensim tsundoku\n",
        "!python -m nltk.downloader movie_reviews punkt\n",
        "from IPython.display import display, Markdown, Latex\n",
        "from tsundoku.word2vec_hints import *"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: lazyme in /usr/local/lib/python3.6/dist-packages (0.0.23)\n",
            "Requirement already satisfied: ansi in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tsundoku in /usr/local/lib/python3.6/dist-packages (0.0.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.5)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.6/dist-packages (from tsundoku) (5.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.11.15)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (2.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.4.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (45.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (1.0.18)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->tsundoku) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->tsundoku) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->tsundoku) (0.1.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOfn_lHQpfJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from itertools import chain\n",
        "\n",
        "from tqdm import tqdm\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNksrIzDqLWF",
        "colab_type": "code",
        "outputId": "29ef7b1b-c5f9-453f-b8db-f77e6c83c6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try: # Use the default NLTK tokenizer.\n",
        "    from nltk import word_tokenize, sent_tokenize \n",
        "    # Testing whether it works. \n",
        "    # Sometimes it doesn't work on some machines because of setup issues.\n",
        "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
        "    print('It Works')\n",
        "except: # Use a naive sentence tokenizer and toktok.\n",
        "    import re\n",
        "    from nltk.tokenize import ToktokTokenizer\n",
        "    # See https://stackoverflow.com/a/25736515/610569\n",
        "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
        "    # Use the toktok tokenizer that requires no dependencies.\n",
        "    toktok = ToktokTokenizer()\n",
        "    word_tokenize = word_tokenize = toktok.tokenize\n",
        "    print('It Still Works')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It Works\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeJwCwxbqpQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "uniq_tokens = set(chain(*tokenized_text))\n",
        "\n",
        "vocab = {}   # Assign indices to every word.\n",
        "idx2tok = {} # Also keep an dict of index to words.\n",
        "for i, token in enumerate(uniq_tokens):\n",
        "    vocab[token] = i\n",
        "    idx2tok[i] = token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uKEXqgq21y",
        "colab_type": "code",
        "outputId": "ccb94d6a-d69b-41cc-fef1-9fceb7182c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 33,\n",
              " ')': 79,\n",
              " ',': 52,\n",
              " '.': 49,\n",
              " 'a': 2,\n",
              " 'able': 5,\n",
              " 'almost': 39,\n",
              " 'always': 16,\n",
              " 'and': 48,\n",
              " 'arbitrary': 64,\n",
              " 'are': 71,\n",
              " 'associations': 31,\n",
              " 'at': 70,\n",
              " 'be': 34,\n",
              " 'been': 17,\n",
              " 'between': 13,\n",
              " 'choose': 41,\n",
              " 'corpora': 44,\n",
              " 'corpus': 69,\n",
              " 'data': 4,\n",
              " 'demonstrably': 1,\n",
              " 'do': 12,\n",
              " 'does': 54,\n",
              " 'enough': 55,\n",
              " 'essentially': 56,\n",
              " 'establish': 80,\n",
              " 'evidence': 30,\n",
              " 'experimental': 72,\n",
              " 'fact': 28,\n",
              " 'frequencies': 47,\n",
              " 'frequently': 81,\n",
              " 'has': 15,\n",
              " 'have': 45,\n",
              " 'hence': 61,\n",
              " 'how': 25,\n",
              " 'hypothesis': 51,\n",
              " 'in': 75,\n",
              " 'inference': 11,\n",
              " 'is': 68,\n",
              " 'it': 46,\n",
              " 'language': 43,\n",
              " 'led': 29,\n",
              " 'linguistic': 73,\n",
              " 'literature': 26,\n",
              " 'look': 58,\n",
              " 'misleading': 10,\n",
              " 'moreover': 50,\n",
              " 'never': 60,\n",
              " 'non-random': 14,\n",
              " 'not': 27,\n",
              " 'null': 59,\n",
              " 'of': 84,\n",
              " 'often': 7,\n",
              " 'or': 8,\n",
              " 'phenomena': 85,\n",
              " 'posits': 76,\n",
              " 'present': 38,\n",
              " 'randomly': 63,\n",
              " 'randomness': 0,\n",
              " 'relation': 86,\n",
              " 'results': 67,\n",
              " 'review': 57,\n",
              " 'shall': 62,\n",
              " 'show': 77,\n",
              " 'so': 6,\n",
              " 'statistical': 32,\n",
              " 'studies': 22,\n",
              " 'support': 82,\n",
              " 'systematically': 35,\n",
              " 'testing': 23,\n",
              " 'that': 42,\n",
              " 'the': 20,\n",
              " 'there': 53,\n",
              " 'to': 3,\n",
              " 'true': 74,\n",
              " 'two': 9,\n",
              " 'unhelpful': 36,\n",
              " 'used': 78,\n",
              " 'users': 21,\n",
              " 'uses': 37,\n",
              " 'we': 66,\n",
              " 'when': 65,\n",
              " 'where': 18,\n",
              " 'which': 40,\n",
              " 'will': 24,\n",
              " 'word': 19,\n",
              " 'words': 83}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbBzwCehrATy",
        "colab_type": "code",
        "outputId": "5c3e57b9-f4a2-4789-ac45-8e764c68ee4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Retrieve the index of the word 'corpora'\n",
        "vocab['corpora']"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDAfYLSLrEKA",
        "colab_type": "code",
        "outputId": "c7d31a67-155c-4431-8229-966cb7ea11d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# The indexed representation of the first sentence.\n",
        "\n",
        "sent0 = tokenized_text[0]\n",
        "print('sent0', sent0)\n",
        "[vocab[token] for token in sent0] "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent0 ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[43, 21, 60, 41, 83, 63, 52, 48, 43, 68, 56, 14, 49]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARvZ0wFarXi6",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-1-a\"></a>\n",
        "\n",
        "### Pet Peeve (Gensim)\n",
        "\n",
        "`gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
        "\n",
        "Using `gensim`, I would have written the above as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BMX_O3HrJH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "vocab = Dictionary(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_T-LSAXrl7n",
        "colab_type": "code",
        "outputId": "1056e1f7-ef1e-4157-faea-b0662405c512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Note the key-value order is different of gensim from the native Python's\n",
        "dict(vocab.items())"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ',',\n",
              " 1: '.',\n",
              " 2: 'and',\n",
              " 3: 'choose',\n",
              " 4: 'essentially',\n",
              " 5: 'is',\n",
              " 6: 'language',\n",
              " 7: 'never',\n",
              " 8: 'non-random',\n",
              " 9: 'randomly',\n",
              " 10: 'users',\n",
              " 11: 'words',\n",
              " 12: 'a',\n",
              " 13: 'hypothesis',\n",
              " 14: 'null',\n",
              " 15: 'posits',\n",
              " 16: 'randomness',\n",
              " 17: 'statistical',\n",
              " 18: 'testing',\n",
              " 19: 'uses',\n",
              " 20: 'which',\n",
              " 21: 'at',\n",
              " 22: 'be',\n",
              " 23: 'corpora',\n",
              " 24: 'hence',\n",
              " 25: 'in',\n",
              " 26: 'linguistic',\n",
              " 27: 'look',\n",
              " 28: 'phenomena',\n",
              " 29: 'the',\n",
              " 30: 'true',\n",
              " 31: 'we',\n",
              " 32: 'when',\n",
              " 33: 'will',\n",
              " 34: '(',\n",
              " 35: ')',\n",
              " 36: 'able',\n",
              " 37: 'almost',\n",
              " 38: 'always',\n",
              " 39: 'data',\n",
              " 40: 'enough',\n",
              " 41: 'establish',\n",
              " 42: 'it',\n",
              " 43: 'moreover',\n",
              " 44: 'not',\n",
              " 45: 'shall',\n",
              " 46: 'that',\n",
              " 47: 'there',\n",
              " 48: 'to',\n",
              " 49: 'where',\n",
              " 50: 'arbitrary',\n",
              " 51: 'between',\n",
              " 52: 'corpus',\n",
              " 53: 'demonstrably',\n",
              " 54: 'do',\n",
              " 55: 'does',\n",
              " 56: 'fact',\n",
              " 57: 'frequently',\n",
              " 58: 'have',\n",
              " 59: 'inference',\n",
              " 60: 'relation',\n",
              " 61: 'so',\n",
              " 62: 'studies',\n",
              " 63: 'support',\n",
              " 64: 'two',\n",
              " 65: 'are',\n",
              " 66: 'associations',\n",
              " 67: 'evidence',\n",
              " 68: 'experimental',\n",
              " 69: 'frequencies',\n",
              " 70: 'how',\n",
              " 71: 'of',\n",
              " 72: 'present',\n",
              " 73: 'systematically',\n",
              " 74: 'word',\n",
              " 75: 'been',\n",
              " 76: 'has',\n",
              " 77: 'led',\n",
              " 78: 'literature',\n",
              " 79: 'misleading',\n",
              " 80: 'often',\n",
              " 81: 'or',\n",
              " 82: 'results',\n",
              " 83: 'review',\n",
              " 84: 'show',\n",
              " 85: 'unhelpful',\n",
              " 86: 'used'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2KbCph1rqJs",
        "colab_type": "code",
        "outputId": "7bf27c48-fa4e-4547-a226-5e4b556678a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab.token2id['corpora']"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IWY7H80ruKu",
        "colab_type": "code",
        "outputId": "66b81406-e1f5-4048-fdb7-ac76097ce63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab.doc2idx(sent0)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LR3hTmr5FG",
        "colab_type": "text"
      },
      "source": [
        "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzuaIDNr_D2",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2\"></a>\n",
        "\n",
        "# 3.0.2 Dataset\n",
        "\n",
        "Lets try creating a `torch.utils.data.Dataset` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aJ1f0z9rzRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Text(Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self.vocab = Dictionary(tokenized_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        # Hint: You want to return a vectorized sentence here.\n",
        "        return {'x': self.vectorize(self.sents[index])}\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkicubMs1R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BELOE IS FROM MY LECTURER. FEEL FREE TO TRY\n",
        "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
        "##hint_dataset_vectorize()\n",
        "##code_text_dataset_vectorize()\n",
        "\n",
        "# Option 2: \"I give up just, run the code for me\" \n",
        "# Uncomment the next two lines, if you really gave up... \n",
        "#full_code_text_dataset_vectorize()\n",
        "#from tsundoku.word2vec import Text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V98xzXBZvLt1",
        "colab_type": "code",
        "outputId": "7c766a9b-8585-4833-b9a1-1d491c70cff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "tokenized_text[5]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " 'present',\n",
              " 'experimental',\n",
              " 'evidence',\n",
              " 'of',\n",
              " 'how',\n",
              " 'arbitrary',\n",
              " 'associations',\n",
              " 'between',\n",
              " 'word',\n",
              " 'frequencies',\n",
              " 'and',\n",
              " 'corpora',\n",
              " 'are',\n",
              " 'systematically',\n",
              " 'non-random',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nTtQnJivdpe",
        "colab_type": "code",
        "outputId": "d3766b35-5565-48ec-c85c-4ef77d753571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_dataset = Text(tokenized_text)\n",
        "text_dataset[5] # First sentence. Representation of above text"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': [31, 72, 68, 67, 71, 70, 50, 66, 51, 74, 69, 2, 23, 65, 73, 8, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mj8VJ_mwpLM",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2-return-dict\"></a>\n",
        "\n",
        "### Return `dict` in `__getitem__()`\n",
        "\n",
        "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
        "\n",
        "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYAQWlCweKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LabeledText(Dataset):\n",
        "    def __init__(self, tokenized_texts, labels):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self.labels = labels # Sentence level labels.\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        return {'x': self.vectorize(self.sents[index]), 'y': self.labels[index]}\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuHeSkMiw1PB",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2-labeleddata\"></a>\n",
        "\n",
        "### Lets try the `LabeledDataset` on a movie review corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKyJmgC1wvwM",
        "colab_type": "code",
        "outputId": "248518bc-0ace-4e44-d537-ca788981801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "documents = []\n",
        "labels = []\n",
        "\n",
        "for fileid in tqdm(movie_reviews.fileids()):\n",
        "    label = fileid.split('/')[0]\n",
        "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
        "    documents.append(doc)\n",
        "    labels.append(label)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 23/2000 [00:00<00:08, 227.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 47/2000 [00:00<00:08, 229.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▎         | 70/2000 [00:00<00:08, 226.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 93/2000 [00:00<00:08, 227.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 114/2000 [00:00<00:08, 217.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 139/2000 [00:00<00:08, 221.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 159/2000 [00:00<00:08, 212.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 185/2000 [00:00<00:08, 223.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 209/2000 [00:00<00:07, 227.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 232/2000 [00:01<00:07, 228.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 256/2000 [00:01<00:07, 229.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 279/2000 [00:01<00:07, 221.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 306/2000 [00:01<00:07, 233.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▋        | 330/2000 [00:01<00:07, 230.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 354/2000 [00:01<00:07, 226.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 377/2000 [00:01<00:07, 216.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 402/2000 [00:01<00:07, 224.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██▏       | 425/2000 [00:01<00:07, 213.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 447/2000 [00:02<00:07, 213.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▎       | 472/2000 [00:02<00:06, 221.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▍       | 495/2000 [00:02<00:06, 218.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 517/2000 [00:02<00:06, 216.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 540/2000 [00:02<00:06, 217.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 563/2000 [00:02<00:06, 219.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 586/2000 [00:02<00:06, 220.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 610/2000 [00:02<00:06, 225.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 633/2000 [00:02<00:06, 217.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 655/2000 [00:02<00:06, 212.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 681/2000 [00:03<00:05, 224.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 704/2000 [00:03<00:05, 225.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▋      | 728/2000 [00:03<00:05, 229.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 752/2000 [00:03<00:05, 222.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 775/2000 [00:03<00:05, 213.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|███▉      | 797/2000 [00:03<00:05, 213.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 819/2000 [00:03<00:05, 213.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 843/2000 [00:03<00:05, 219.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 868/2000 [00:03<00:05, 225.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▍     | 893/2000 [00:04<00:04, 230.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 918/2000 [00:04<00:04, 233.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 942/2000 [00:04<00:04, 232.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 966/2000 [00:04<00:04, 227.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 989/2000 [00:04<00:04, 224.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 1012/2000 [00:04<00:04, 220.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 1035/2000 [00:04<00:04, 204.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 1060/2000 [00:04<00:04, 214.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 1083/2000 [00:04<00:04, 217.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▌    | 1105/2000 [00:04<00:04, 216.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▋    | 1127/2000 [00:05<00:04, 212.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 1152/2000 [00:05<00:03, 221.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 1175/2000 [00:05<00:03, 220.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 1202/2000 [00:05<00:03, 232.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████▏   | 1228/2000 [00:05<00:03, 239.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 1253/2000 [00:05<00:03, 232.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 1277/2000 [00:05<00:03, 219.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▌   | 1301/2000 [00:05<00:03, 224.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 1324/2000 [00:05<00:03, 218.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 1347/2000 [00:06<00:03, 209.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▊   | 1374/2000 [00:06<00:02, 222.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|██████▉   | 1399/2000 [00:06<00:02, 227.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 1422/2000 [00:06<00:02, 221.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 1445/2000 [00:06<00:02, 213.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 1467/2000 [00:06<00:02, 214.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 74%|███████▍  | 1489/2000 [00:06<00:02, 212.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▌  | 1511/2000 [00:06<00:02, 200.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 77%|███████▋  | 1532/2000 [00:06<00:02, 200.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 78%|███████▊  | 1554/2000 [00:07<00:02, 203.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 79%|███████▉  | 1575/2000 [00:07<00:02, 200.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 80%|███████▉  | 1598/2000 [00:07<00:01, 202.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 81%|████████  | 1619/2000 [00:07<00:01, 201.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 82%|████████▏ | 1640/2000 [00:07<00:01, 194.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 83%|████████▎ | 1662/2000 [00:07<00:01, 199.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 84%|████████▍ | 1683/2000 [00:07<00:01, 192.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 85%|████████▌ | 1703/2000 [00:07<00:01, 187.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 86%|████████▋ | 1725/2000 [00:07<00:01, 195.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 87%|████████▋ | 1745/2000 [00:08<00:01, 191.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 88%|████████▊ | 1769/2000 [00:08<00:01, 201.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 90%|████████▉ | 1790/2000 [00:08<00:01, 202.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 91%|█████████ | 1811/2000 [00:08<00:00, 200.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 92%|█████████▏| 1834/2000 [00:08<00:00, 208.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 93%|█████████▎| 1858/2000 [00:08<00:00, 215.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 94%|█████████▍| 1880/2000 [00:08<00:00, 212.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 95%|█████████▌| 1902/2000 [00:08<00:00, 213.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 96%|█████████▌| 1924/2000 [00:08<00:00, 215.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 97%|█████████▋| 1946/2000 [00:08<00:00, 209.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 98%|█████████▊| 1968/2000 [00:09<00:00, 207.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 99%|█████████▉| 1989/2000 [00:09<00:00, 207.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 2000/2000 [00:09<00:00, 216.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwEZ5VmNw5xo",
        "colab_type": "code",
        "outputId": "d8eb6f8f-442b-4adf-f09c-02769149852c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(documents[0])"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'s\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '``', 'sorta', '``', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'did', \"n't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'s\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '``', 'normal', '``', 'but', 'then', 'downshifts', 'into', 'this', '``', 'fantasy', '``', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'s\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'do', \"n't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'s\", 'biggest', 'problem', '.', 'it', \"'s\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'did', \"n't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '``', 'into', 'it', '``', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'do', \"n't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'ve\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '``', 'the', 'suits', '``', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'s\", 'unraveling', '.', 'overall', ',', 'the', 'film', 'does', \"n't\", 'stick', 'because', 'it', 'does', \"n't\", 'entertain', ',', 'it', \"'s\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'s\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'s\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwziDJdFw-of",
        "colab_type": "code",
        "outputId": "410ff606-deb9-4fca-8219-0ffde2e347ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "labeled_dataset = LabeledText(documents, labels)\n",
        "print(labeled_dataset[0])  # First review in the data."
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'x': [243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5], 'y': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnNPd0OxEyR",
        "colab_type": "code",
        "outputId": "a116a0e7-51ad-432b-f3e5-273dd20df631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(labeled_dataset[0]['x'])  # First review in vectorized index format."
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqLJVcrCxLdh",
        "colab_type": "code",
        "outputId": "911ca65e-26eb-4b8a-8cac-8b1e03326e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(labeled_dataset[0]['y'])  # Label of the first review in the data. "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985OjlPTxUlk",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1\"></a>\n",
        "\n",
        "# 3.1 Word2Vec Training\n",
        "\n",
        "Word2Vec has two training variants:\n",
        "\n",
        " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
        " - **Skip-grams**: Predict context words given center word.\n",
        "  \n",
        "Visually, they look like this:\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm9cdGM2xhND",
        "colab_type": "text"
      },
      "source": [
        "Fig. 1. The skip-gram model. Both the input vector xx and the output yy are one-hot encoded word representations. <br>The hidden layer is the word embedding of size NN.\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_b0MdnzyXRm",
        "colab_type": "text"
      },
      "source": [
        "Fig. 2. The CBOW model. Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer. Other symbols have the same meanings as in Fig 1.\n",
        "\n",
        "(Pretty network images above are from [https://lilianweng.github.io](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-continuous-bag-of-words-cbow))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdcgrrdky6P6",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-1\"></a>\n",
        "\n",
        "## 3.1.1. CBOW\n",
        "\n",
        "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE-og_THxOWi",
        "colab_type": "code",
        "outputId": "acf861d5-681b-4f87-9070-4149e46f6d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from lazyme import per_window, per_chunk\n",
        "\n",
        "xx =[1,2,3,4]\n",
        "list(per_window(xx, n=2))\n",
        "list(per_chunk(xx, n=3))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 3), (4, None, None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suLX0NBvzFn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def per_window(sequence, n=1):\n",
        "    \"\"\"\n",
        "    From http://stackoverflow.com/q/42220614/610569\n",
        "        >>> list(per_window([1,2,3,4], n=2))\n",
        "        [(1, 2), (2, 3), (3, 4)]\n",
        "        >>> list(per_window([1,2,3,4], n=3))\n",
        "        [(1, 2, 3), (2, 3, 4)]\n",
        "    \"\"\"\n",
        "    start, stop = 0, n\n",
        "    seq = list(sequence)\n",
        "    while stop <= len(seq):\n",
        "        yield seq[start:stop]\n",
        "        start += 1\n",
        "        stop += 1\n",
        "\n",
        "def cbow_iterator(tokens, window_size):\n",
        "    n = window_size * 2 + 1\n",
        "    for window in per_window(tokens, n):\n",
        "        target = window.pop(window_size)\n",
        "        yield window, target   # X = window ; Y = target. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyf9slOh06S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
        "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNZ4NW2r1IFE",
        "colab_type": "code",
        "outputId": "484472ab-45ca-4516-d463-2e9da866e9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(cbow_iterator(sent0, 2)) \n",
        "#the first part is X and target is Y\n",
        "#X => 'language', 'users', 'choose', 'words'\n",
        "#y => 'never'"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['language', 'users', 'choose', 'words'], 'never'),\n",
              " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
              " (['never', 'choose', 'randomly', ','], 'words'),\n",
              " (['choose', 'words', ',', 'and'], 'randomly'),\n",
              " (['words', 'randomly', 'and', 'language'], ','),\n",
              " (['randomly', ',', 'language', 'is'], 'and'),\n",
              " ([',', 'and', 'is', 'essentially'], 'language'),\n",
              " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
              " (['language', 'is', 'non-random', '.'], 'essentially')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4upcyLol1KF9",
        "colab_type": "code",
        "outputId": "af5ec489-6eaa-4d8f-d627-c86738582435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(cbow_iterator(sent0, 3)) "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
              " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
              " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
              " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
              " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
              " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
              " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiIkqfO1ndl",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-2\"></a>\n",
        "\n",
        "## 3.1.2. Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGmQ1741i-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def skipgram_iterator(tokens, window_size):\n",
        "    n = window_size * 2 + 1 \n",
        "    for i, window in enumerate(per_window(tokens, n)):\n",
        "        target = window.pop(window_size)\n",
        "        # Generate positive samples.\n",
        "        for context_word in window:\n",
        "            yield target, context_word, 1\n",
        "        # Generate negative samples.\n",
        "        for _ in range(n-1):\n",
        "            leftovers = tokens[:i] + tokens[i+n:]\n",
        "            yield target, random.choice(leftovers), 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tw3CKO81ruM",
        "colab_type": "code",
        "outputId": "b24885eb-3b69-48cb-cb08-54bda75e2ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(skipgram_iterator(sent0, 2))\n",
        "#1 is positive sample and 0 negative"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('never', 'language', 1),\n",
              " ('never', 'users', 1),\n",
              " ('never', 'choose', 1),\n",
              " ('never', 'words', 1),\n",
              " ('never', 'essentially', 0),\n",
              " ('never', 'essentially', 0),\n",
              " ('never', 'non-random', 0),\n",
              " ('never', ',', 0),\n",
              " ('choose', 'users', 1),\n",
              " ('choose', 'never', 1),\n",
              " ('choose', 'words', 1),\n",
              " ('choose', 'randomly', 1),\n",
              " ('choose', 'essentially', 0),\n",
              " ('choose', 'and', 0),\n",
              " ('choose', 'essentially', 0),\n",
              " ('choose', 'language', 0),\n",
              " ('words', 'never', 1),\n",
              " ('words', 'choose', 1),\n",
              " ('words', 'randomly', 1),\n",
              " ('words', ',', 1),\n",
              " ('words', 'users', 0),\n",
              " ('words', 'essentially', 0),\n",
              " ('words', 'users', 0),\n",
              " ('words', 'and', 0),\n",
              " ('randomly', 'choose', 1),\n",
              " ('randomly', 'words', 1),\n",
              " ('randomly', ',', 1),\n",
              " ('randomly', 'and', 1),\n",
              " ('randomly', '.', 0),\n",
              " ('randomly', 'is', 0),\n",
              " ('randomly', 'never', 0),\n",
              " ('randomly', 'users', 0),\n",
              " (',', 'words', 1),\n",
              " (',', 'randomly', 1),\n",
              " (',', 'and', 1),\n",
              " (',', 'language', 1),\n",
              " (',', 'essentially', 0),\n",
              " (',', 'essentially', 0),\n",
              " (',', 'is', 0),\n",
              " (',', 'is', 0),\n",
              " ('and', 'randomly', 1),\n",
              " ('and', ',', 1),\n",
              " ('and', 'language', 1),\n",
              " ('and', 'is', 1),\n",
              " ('and', '.', 0),\n",
              " ('and', 'essentially', 0),\n",
              " ('and', 'essentially', 0),\n",
              " ('and', 'users', 0),\n",
              " ('language', ',', 1),\n",
              " ('language', 'and', 1),\n",
              " ('language', 'is', 1),\n",
              " ('language', 'essentially', 1),\n",
              " ('language', 'randomly', 0),\n",
              " ('language', 'never', 0),\n",
              " ('language', 'never', 0),\n",
              " ('language', 'randomly', 0),\n",
              " ('is', 'and', 1),\n",
              " ('is', 'language', 1),\n",
              " ('is', 'essentially', 1),\n",
              " ('is', 'non-random', 1),\n",
              " ('is', 'never', 0),\n",
              " ('is', 'never', 0),\n",
              " ('is', 'words', 0),\n",
              " ('is', 'users', 0),\n",
              " ('essentially', 'language', 1),\n",
              " ('essentially', 'is', 1),\n",
              " ('essentially', 'non-random', 1),\n",
              " ('essentially', '.', 1),\n",
              " ('essentially', ',', 0),\n",
              " ('essentially', 'never', 0),\n",
              " ('essentially', 'choose', 0),\n",
              " ('essentially', 'and', 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCDPaz12EyN",
        "colab_type": "text"
      },
      "source": [
        "## Cut-away: What is `partial`?\n",
        "\n",
        "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYt9mXzG1tk4",
        "colab_type": "code",
        "outputId": "51ce588b-391b-4e01-f5e2-cbfc35c785c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "# Generates bigrams\n",
        "list(ngrams('this is a sentence'.split(), n=2))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcbr82d2Ipu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
        "bigrams = partial(ngrams, n=2)\n",
        "trigrams = partial(ngrams, n=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iKOZYNH2NYf",
        "colab_type": "code",
        "outputId": "1dcf6bcf-3b10-45b3-a1f0-e4687c2759df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(trigrams('this is a sentence'.split()))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3dOFOLO2PeD",
        "colab_type": "code",
        "outputId": "ed5085b2-346b-4009-8b1e-2d6520d8bbbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(bigrams('this is a sentence'.split()))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLyudWp2Uy1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-3\"></a>\n",
        "\n",
        "## 3.1.3 Word2Vec Dataset\n",
        "\n",
        "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
        "\n",
        "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es7gpYNM2Qx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecText(Dataset):\n",
        "    def __init__(self, tokenized_texts, window_size, variant):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self._len = len(self.sents)\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "        self.window_size = window_size\n",
        "        self.variant = variant\n",
        "        if variant.lower() == 'cbow':\n",
        "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
        "        elif variant.lower() == 'skipgram':\n",
        "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "\n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        vectorized_sent = self.vectorize(self.sents[index])\n",
        "        return list(self._iterator(vectorized_sent))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized.\n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
        "        return self.vocab.doc2idx(tokens)\n",
        "\n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]\n",
        "\n",
        "    def cbow_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1\n",
        "        for window in per_window(tokens, n):\n",
        "            target = window.pop(window_size)\n",
        "            yield {'x':window, 'y':target}   # X = window ; Y = target. \n",
        "\n",
        "    def skipgram_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1 \n",
        "        for i, window in enumerate(per_window(tokens, n)):\n",
        "            target = window.pop(window_size)\n",
        "            # Generate positive samples.\n",
        "            for context_word in window:\n",
        "                yield {'x':(target, context_word), 'y':1}\n",
        "            # Generate negative samples.\n",
        "            for _ in range(n-1):\n",
        "                leftovers = tokens[:i] + tokens[i+n:]\n",
        "                yield {'x': (target, random.choice(leftovers)), 'y':0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHreIlYP3e2z",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-3-hint\"></a>\n",
        "## Hints for the cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DWMTG8l3fUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
        "##hint_word2vec_dataset()\n",
        "\n",
        "# Option 2: \"I give up just, run the code for me\" \n",
        "# Uncomment the next two lines, if you really gave up... \n",
        "##full_code_word2vec_dataset()\n",
        "##from tsundoku.word2vec import Word2VecText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPxvwu4w3qYP",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-hint\"></a>\n",
        "\n",
        "## 3.1.4. Train a CBOW model\n",
        "\n",
        "### Lets Get Some Data\n",
        "\n",
        "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxICbkpt3iaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os,requests, io #codecs\n",
        "\n",
        "\n",
        "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
        "if os.path.isfile('language-never-random.txt'):\n",
        "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
        "        text = fin.read()\n",
        "else:\n",
        "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "    text = requests.get(url).content.decode('utf8')\n",
        "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "        fout.write(text)\n",
        "\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
        "window_size = 2\n",
        "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxe-EDyT36OX",
        "colab_type": "code",
        "outputId": "8c30b0ac-604b-4067-ba35-7678dc8327d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish that it is not true. In\n",
            "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
            "tion between two phenomena is demonstrably non-random, does not sup-\n",
            "port the inference that it is not arbitrary. We present experimental evidence\n",
            "of how arbitrary associations between word frequencies and corpora are\n",
            "systematically non-random. We review literature in which hypothesis test-\n",
            "ing has been used, and show how it has often led to unhelpful or mislead-\n",
            "ing results.\n",
            "Keywords: 쎲쎲쎲\n",
            "\n",
            "1. Int\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqHLaRwm4IOH",
        "colab_type": "code",
        "outputId": "70aa2976-3144-4c46-abce-0495fa8a6908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Sanity check, lets take a look at the data.\n",
        "print(tokenized_text[0])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxUNZmu-4O11",
        "colab_type": "code",
        "outputId": "6eab6745-445b-49eb-d31e-237b24581613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzY04ie74TKK",
        "colab_type": "code",
        "outputId": "1ee28736-8d50-4244-a06c-abbcc258ade6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from lazyme import color_str\n",
        "\n",
        "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
        "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
        "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
        "    target = vocab.get(int(y), '<unk>')\n",
        "\n",
        "    if not prediction:\n",
        "        predicted_word = '______'\n",
        "    else:\n",
        "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
        "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
        "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
        "    \n",
        "\n",
        "sent_idx = 10\n",
        "window_size = 2\n",
        "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
        "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
        "for w2v_io in w2v_dataset[sent_idx]:\n",
        "    context, target = w2v_io['x'], w2v_io['y']\n",
        "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
        "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
            "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
            "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
            "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
            "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
            "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
            "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
            "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
            "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
            "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
            "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
            "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
            "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
            "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
            "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
            "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
            "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
            "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
            "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
            "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
            "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
            "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
            "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
            "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
            "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
            "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
            "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
            "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
            "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
            "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
            "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
            "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09Ap2Nq4oZ1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-cbow-model\"></a>\n",
        "\n",
        "## The CBOW Model\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"600\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VLFQA3T44RR",
        "colab_type": "text"
      },
      "source": [
        "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN142ipB4g9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974oU-pE5JYa",
        "colab_type": "text"
      },
      "source": [
        "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
        "\n",
        "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aohU8BTc5F0w",
        "colab_type": "code",
        "outputId": "39d8d3fc-8bbf-4120-a40c-3fa126b9d888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "w2v_dataset[0]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'x': [10, 8, 0, 7], 'y': 11},\n",
              " {'x': [8, 11, 7, 0], 'y': 0},\n",
              " {'x': [11, 0, 0, 7], 'y': 7},\n",
              " {'x': [0, 7, 7, 0], 'y': 0},\n",
              " {'x': [7, 0, 0, 13], 'y': 7},\n",
              " {'x': [0, 7, 13, 3], 'y': 0},\n",
              " {'x': [7, 0, 3, 9], 'y': 13},\n",
              " {'x': [0, 13, 9, 2], 'y': 3},\n",
              " {'x': [13, 3, 2, 10], 'y': 9},\n",
              " {'x': [3, 9, 10, 15], 'y': 2},\n",
              " {'x': [9, 2, 15, 11], 'y': 10},\n",
              " {'x': [2, 10, 11, 5], 'y': 15},\n",
              " {'x': [10, 15, 5, 16], 'y': 11},\n",
              " {'x': [15, 11, 16, 14], 'y': 5},\n",
              " {'x': [11, 5, 14, 0], 'y': 16},\n",
              " {'x': [5, 16, 0, 4], 'y': 14},\n",
              " {'x': [16, 14, 4, 10], 'y': 0},\n",
              " {'x': [14, 0, 10, 8], 'y': 4},\n",
              " {'x': [0, 4, 8, 6], 'y': 10},\n",
              " {'x': [4, 10, 6, 12], 'y': 8},\n",
              " {'x': [10, 8, 12, 1], 'y': 6}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPbUZnBd5Lpj",
        "colab_type": "code",
        "outputId": "5554c414-9a54-4878-8b99-41c34f03b894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Lets take a look at the first output.\n",
        "x, y = w2v_dataset[0][0]['x'],  w2v_dataset[0][0]['y']\n",
        "\n",
        "x = tensor(x)\n",
        "y = autograd.Variable(tensor(y, dtype=torch.long))\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10,  8,  0,  7])\n",
            "tensor(11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPyMs7z5RJF",
        "colab_type": "code",
        "outputId": "e5451c6e-8ae4-4dd9-8b7c-a754635497f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "embd_size = 5\n",
        "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
        "emb.state_dict()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight', tensor([[-1.0389,  0.4558, -0.2694, -0.7621, -0.4968],\n",
              "                      [ 0.1052,  0.4597,  1.8189,  0.5457,  0.7650],\n",
              "                      [ 1.5616,  0.7761, -0.8973, -1.4528, -1.1162],\n",
              "                      ...,\n",
              "                      [ 1.8932,  0.4954,  0.7708, -0.5620, -0.0594],\n",
              "                      [ 0.8307, -1.9312, -1.0439, -0.4424,  0.2973],\n",
              "                      [-1.0408,  0.0428,  0.3712, -1.7976, -0.3400]]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4PxmYTw5cg7",
        "colab_type": "code",
        "outputId": "e41d75ba-25ff-4515-fa50-3306eabfe196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(emb.state_dict()['weight'].shape)\n",
        "emb.state_dict()['weight']"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1388, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.0389,  0.4558, -0.2694, -0.7621, -0.4968],\n",
              "        [ 0.1052,  0.4597,  1.8189,  0.5457,  0.7650],\n",
              "        [ 1.5616,  0.7761, -0.8973, -1.4528, -1.1162],\n",
              "        ...,\n",
              "        [ 1.8932,  0.4954,  0.7708, -0.5620, -0.0594],\n",
              "        [ 0.8307, -1.9312, -1.0439, -0.4424,  0.2973],\n",
              "        [-1.0408,  0.0428,  0.3712, -1.7976, -0.3400]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WEktdC5fWW",
        "colab_type": "code",
        "outputId": "17e8a30d-c9e7-4dc7-bd0d-54e1329f67fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(emb(x).shape)\n",
        "print(emb(x))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5])\n",
            "tensor([[-0.1586,  0.7450, -0.2607,  0.7055,  0.4880],\n",
            "        [ 1.3816, -0.4296, -0.5728, -0.5179, -1.4015],\n",
            "        [-1.0389,  0.4558, -0.2694, -0.7621, -0.4968],\n",
            "        [-0.8136,  0.9049, -1.7248,  1.7375,  2.3500]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDUv-3Vq5jRl",
        "colab_type": "code",
        "outputId": "da096c19-1197-45be-8320-b908dbb22a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(emb(x).view(1, -1).shape)\n",
        "emb(x).view(1, -1)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1586,  0.7450, -0.2607,  0.7055,  0.4880,  1.3816, -0.4296, -0.5728,\n",
              "         -0.5179, -1.4015, -1.0389,  0.4558, -0.2694, -0.7621, -0.4968, -0.8136,\n",
              "          0.9049, -1.7248,  1.7375,  2.3500]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrjn4g0-5v8a",
        "colab_type": "code",
        "outputId": "2a1530c5-dfe8-4a87-b74a-af06e0a2368e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "hidden_size = 100\n",
        "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
        "print(lin1.state_dict())"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('weight', tensor([[ 3.3665e-02, -7.9619e-02,  1.5802e-01,  ..., -1.8659e-04,\n",
            "          1.2807e-01, -1.1676e-01],\n",
            "        [-1.4369e-01,  8.9479e-02,  6.7302e-02,  ..., -1.6359e-01,\n",
            "         -2.0428e-01, -1.2002e-01],\n",
            "        [-1.7826e-01, -5.9036e-02,  1.6889e-01,  ..., -9.3645e-02,\n",
            "         -2.1992e-01,  1.4566e-01],\n",
            "        ...,\n",
            "        [-9.4876e-02,  1.0334e-01, -1.5599e-01,  ...,  1.4868e-01,\n",
            "          2.1274e-01,  9.6756e-02],\n",
            "        [-9.1977e-02, -8.1725e-03,  1.9821e-01,  ...,  9.3849e-02,\n",
            "         -1.9110e-01,  2.0146e-01],\n",
            "        [ 9.7269e-02,  1.4654e-01, -1.6749e-01,  ...,  3.1749e-02,\n",
            "          1.4092e-01,  1.0996e-02]])), ('bias', tensor([ 0.0232, -0.1101, -0.1309,  0.1480, -0.0885, -0.0143,  0.0247, -0.1472,\n",
            "        -0.1247, -0.0385,  0.1833, -0.1288, -0.0904,  0.0342,  0.2028,  0.2019,\n",
            "        -0.0050, -0.2037,  0.0819, -0.0855, -0.0117,  0.0164, -0.0919,  0.1876,\n",
            "        -0.0386,  0.0115, -0.1046, -0.1330, -0.1366,  0.2034,  0.1057,  0.0362,\n",
            "        -0.1822, -0.1268,  0.0107,  0.1052,  0.1735,  0.0481, -0.2092, -0.1976,\n",
            "         0.0690, -0.1983,  0.0354, -0.0226, -0.2129, -0.1115, -0.0964, -0.0039,\n",
            "        -0.1571, -0.0858,  0.0499, -0.0779, -0.1458, -0.1736, -0.1933, -0.1611,\n",
            "         0.0138,  0.0073, -0.1880, -0.1936, -0.1090,  0.0332,  0.0869, -0.1787,\n",
            "         0.0305,  0.0931, -0.0654, -0.1527, -0.1074, -0.1504,  0.0674, -0.2138,\n",
            "         0.2058, -0.0012,  0.0987, -0.1813, -0.2144,  0.0849, -0.1616, -0.1801,\n",
            "         0.1272, -0.0871,  0.0381,  0.1678, -0.2164,  0.0928,  0.2201,  0.1933,\n",
            "         0.1663, -0.0645,  0.1881,  0.0608,  0.1575,  0.1665,  0.1470,  0.1842,\n",
            "        -0.0808,  0.1412,  0.1440, -0.0921]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q53JRdWz6H3p",
        "colab_type": "code",
        "outputId": "91e08e61-6a91-44c8-dd0e-61ba89bc0681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(lin1.state_dict()['weight'].shape)\n",
        "print(lin1.state_dict()['weight'])"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 20])\n",
            "tensor([[ 3.3665e-02, -7.9619e-02,  1.5802e-01,  ..., -1.8659e-04,\n",
            "          1.2807e-01, -1.1676e-01],\n",
            "        [-1.4369e-01,  8.9479e-02,  6.7302e-02,  ..., -1.6359e-01,\n",
            "         -2.0428e-01, -1.2002e-01],\n",
            "        [-1.7826e-01, -5.9036e-02,  1.6889e-01,  ..., -9.3645e-02,\n",
            "         -2.1992e-01,  1.4566e-01],\n",
            "        ...,\n",
            "        [-9.4876e-02,  1.0334e-01, -1.5599e-01,  ...,  1.4868e-01,\n",
            "          2.1274e-01,  9.6756e-02],\n",
            "        [-9.1977e-02, -8.1725e-03,  1.9821e-01,  ...,  9.3849e-02,\n",
            "         -1.9110e-01,  2.0146e-01],\n",
            "        [ 9.7269e-02,  1.4654e-01, -1.6749e-01,  ...,  3.1749e-02,\n",
            "          1.4092e-01,  1.0996e-02]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD17_G0Q6N21",
        "colab_type": "code",
        "outputId": "9e6f77c4-dcda-455e-acc3-890cf847238a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(lin1(emb(x).view(1, -1)).shape)\n",
        "lin1(emb(x).view(1, -1))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0977, -0.2259,  0.2664, -1.0108, -0.9035,  0.0017,  0.5119,  0.4829,\n",
              "         -0.3609, -1.1217,  0.5906,  0.0196,  0.7969, -0.0878, -0.7761,  0.2873,\n",
              "          0.5175, -0.1545,  0.2390, -0.1139, -0.7776,  0.1824,  0.7666,  0.3836,\n",
              "         -0.1953,  0.1580,  0.3667, -0.4639, -0.1786,  0.4279, -0.3276,  0.2555,\n",
              "         -0.5937,  0.6548, -0.3770,  0.5014, -1.0476,  0.3206, -0.8453, -0.1216,\n",
              "          0.8064, -0.2301, -0.2055, -0.5542, -0.2040, -0.1586, -0.1117,  0.3845,\n",
              "         -0.8415,  0.1954,  0.7411, -0.2238,  0.4665,  0.3371, -0.3621, -1.0271,\n",
              "          0.0272,  0.2232, -0.2392,  0.1173, -0.1705, -1.5540, -1.2457, -0.0910,\n",
              "         -0.9061, -0.4954, -0.0985,  0.4724, -0.8281, -0.1481,  1.2097, -0.2926,\n",
              "          0.9970, -0.3515,  1.1349,  0.4363, -1.2660,  0.0485, -1.1125, -0.3228,\n",
              "          0.2475, -0.5957, -0.2681,  0.4056, -0.1752, -0.3116, -0.0437,  0.4766,\n",
              "         -0.7983,  0.2782,  0.0809, -0.6995,  0.4915, -0.3835,  0.2923, -0.4806,\n",
              "          0.2826,  0.3580, -0.1145,  0.6491]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DhwHGqf6TrK",
        "colab_type": "code",
        "outputId": "db6dc5d6-924d-453f-c128-5a74c519e4d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "relu = nn.ReLU()\n",
        "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
        "relu(lin1(emb(x).view(1, -1)))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.2664, 0.0000, 0.0000, 0.0017, 0.5119, 0.4829, 0.0000,\n",
              "         0.0000, 0.5906, 0.0196, 0.7969, 0.0000, 0.0000, 0.2873, 0.5175, 0.0000,\n",
              "         0.2390, 0.0000, 0.0000, 0.1824, 0.7666, 0.3836, 0.0000, 0.1580, 0.3667,\n",
              "         0.0000, 0.0000, 0.4279, 0.0000, 0.2555, 0.0000, 0.6548, 0.0000, 0.5014,\n",
              "         0.0000, 0.3206, 0.0000, 0.0000, 0.8064, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.3845, 0.0000, 0.1954, 0.7411, 0.0000, 0.4665, 0.3371,\n",
              "         0.0000, 0.0000, 0.0272, 0.2232, 0.0000, 0.1173, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.4724, 0.0000, 0.0000, 1.2097, 0.0000,\n",
              "         0.9970, 0.0000, 1.1349, 0.4363, 0.0000, 0.0485, 0.0000, 0.0000, 0.2475,\n",
              "         0.0000, 0.0000, 0.4056, 0.0000, 0.0000, 0.0000, 0.4766, 0.0000, 0.2782,\n",
              "         0.0809, 0.0000, 0.4915, 0.0000, 0.2923, 0.0000, 0.2826, 0.3580, 0.0000,\n",
              "         0.6491]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcMI7Gcf6YV0",
        "colab_type": "code",
        "outputId": "d9b9775a-e99a-4434-98c2-2978ee57bdd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
        "print(lin2.state_dict()['weight'].shape)\n",
        "lin2.state_dict()['weight']"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1388, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0142,  0.0050,  0.0052,  ..., -0.0060,  0.0826, -0.0432],\n",
              "        [ 0.0598,  0.0282, -0.0906,  ...,  0.0028,  0.0259, -0.0248],\n",
              "        [-0.0596,  0.0520,  0.0350,  ...,  0.0205,  0.0460,  0.0939],\n",
              "        ...,\n",
              "        [-0.0259, -0.0167,  0.0490,  ..., -0.0468,  0.0850,  0.0240],\n",
              "        [-0.0292, -0.0862, -0.0495,  ...,  0.0911,  0.0123, -0.0286],\n",
              "        [-0.0842,  0.0250, -0.0675,  ..., -0.0707, -0.0147, -0.0953]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFasnpS36ciE",
        "colab_type": "code",
        "outputId": "7a8dfd30-b885-47b0-92ea-568b47a27872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "h_x = relu(lin1(emb(x).view(1, -1)))\n",
        "print(lin2(h_x).shape)\n",
        "lin2(h_x)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1388])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0650,  0.1935, -0.1688,  ..., -0.0387,  0.3921, -0.5814]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_a33B3p6lrZ",
        "colab_type": "code",
        "outputId": "2e074cc8-625c-4bf3-fd33-cc6bcff11732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "softmax = nn.LogSoftmax(dim=1)\n",
        "softmax(lin2(h_x)).detach().numpy().tolist()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-7.199182510375977,\n",
              "  -7.070715427398682,\n",
              "  -7.432940483093262,\n",
              "  -7.502559661865234,\n",
              "  -7.135555267333984,\n",
              "  -7.051440238952637,\n",
              "  -7.309675693511963,\n",
              "  -7.469589710235596,\n",
              "  -7.144767761230469,\n",
              "  -7.4249958992004395,\n",
              "  -7.141622543334961,\n",
              "  -7.509169578552246,\n",
              "  -7.25118350982666,\n",
              "  -7.0581278800964355,\n",
              "  -6.99705171585083,\n",
              "  -7.293349266052246,\n",
              "  -6.98480749130249,\n",
              "  -6.953399181365967,\n",
              "  -6.986053943634033,\n",
              "  -7.283415794372559,\n",
              "  -7.542573928833008,\n",
              "  -7.353916168212891,\n",
              "  -7.472623825073242,\n",
              "  -7.218685150146484,\n",
              "  -7.2019453048706055,\n",
              "  -7.346446990966797,\n",
              "  -7.310818195343018,\n",
              "  -7.217321872711182,\n",
              "  -7.322483062744141,\n",
              "  -7.24486780166626,\n",
              "  -6.666690349578857,\n",
              "  -7.194486618041992,\n",
              "  -7.051487922668457,\n",
              "  -7.1688618659973145,\n",
              "  -7.274130821228027,\n",
              "  -7.449261665344238,\n",
              "  -7.331083297729492,\n",
              "  -7.52595329284668,\n",
              "  -6.938161373138428,\n",
              "  -6.774359226226807,\n",
              "  -7.082128524780273,\n",
              "  -7.262214660644531,\n",
              "  -7.207281589508057,\n",
              "  -7.337949752807617,\n",
              "  -7.467275142669678,\n",
              "  -7.134647846221924,\n",
              "  -7.265178680419922,\n",
              "  -7.20494270324707,\n",
              "  -7.63803768157959,\n",
              "  -7.001863479614258,\n",
              "  -7.129901885986328,\n",
              "  -7.3819451332092285,\n",
              "  -7.59791898727417,\n",
              "  -7.072638511657715,\n",
              "  -7.341893672943115,\n",
              "  -7.718964099884033,\n",
              "  -7.138903617858887,\n",
              "  -7.284820079803467,\n",
              "  -7.006067276000977,\n",
              "  -6.986067295074463,\n",
              "  -7.108621120452881,\n",
              "  -7.004823684692383,\n",
              "  -7.45585298538208,\n",
              "  -7.269381046295166,\n",
              "  -7.195338249206543,\n",
              "  -7.297359466552734,\n",
              "  -7.246575355529785,\n",
              "  -7.172113418579102,\n",
              "  -7.407415866851807,\n",
              "  -7.684563636779785,\n",
              "  -6.92307710647583,\n",
              "  -7.046538829803467,\n",
              "  -7.211242198944092,\n",
              "  -7.30469274520874,\n",
              "  -7.3576979637146,\n",
              "  -7.638847827911377,\n",
              "  -7.202401161193848,\n",
              "  -7.352988243103027,\n",
              "  -7.2946953773498535,\n",
              "  -7.436051368713379,\n",
              "  -7.303964138031006,\n",
              "  -7.554337501525879,\n",
              "  -7.24672269821167,\n",
              "  -7.172592639923096,\n",
              "  -7.545470237731934,\n",
              "  -7.717876434326172,\n",
              "  -7.058773517608643,\n",
              "  -7.119503974914551,\n",
              "  -6.7940287590026855,\n",
              "  -7.109738349914551,\n",
              "  -7.345181465148926,\n",
              "  -7.6548662185668945,\n",
              "  -7.527037143707275,\n",
              "  -7.125404357910156,\n",
              "  -6.913970470428467,\n",
              "  -6.86588191986084,\n",
              "  -6.961314678192139,\n",
              "  -7.132955551147461,\n",
              "  -7.370361804962158,\n",
              "  -7.155825614929199,\n",
              "  -7.047044277191162,\n",
              "  -7.359050750732422,\n",
              "  -7.181945323944092,\n",
              "  -7.06300163269043,\n",
              "  -7.376287937164307,\n",
              "  -6.967510223388672,\n",
              "  -7.589241027832031,\n",
              "  -7.082881927490234,\n",
              "  -7.561622619628906,\n",
              "  -7.667147636413574,\n",
              "  -7.092681407928467,\n",
              "  -7.2310028076171875,\n",
              "  -7.144033908843994,\n",
              "  -7.472316741943359,\n",
              "  -7.367842674255371,\n",
              "  -7.402371406555176,\n",
              "  -7.072274208068848,\n",
              "  -7.274277210235596,\n",
              "  -7.222853660583496,\n",
              "  -6.912365436553955,\n",
              "  -7.069720268249512,\n",
              "  -7.473659038543701,\n",
              "  -7.401575088500977,\n",
              "  -6.934920310974121,\n",
              "  -7.326239109039307,\n",
              "  -7.235548973083496,\n",
              "  -7.1971540451049805,\n",
              "  -7.197129249572754,\n",
              "  -6.84737491607666,\n",
              "  -7.086883544921875,\n",
              "  -7.232292175292969,\n",
              "  -7.335543632507324,\n",
              "  -7.5127129554748535,\n",
              "  -7.586780071258545,\n",
              "  -7.260738372802734,\n",
              "  -7.242795467376709,\n",
              "  -7.315262317657471,\n",
              "  -7.077113151550293,\n",
              "  -7.184788227081299,\n",
              "  -7.302927494049072,\n",
              "  -7.098897933959961,\n",
              "  -7.10723876953125,\n",
              "  -7.086048603057861,\n",
              "  -7.336437702178955,\n",
              "  -7.286827564239502,\n",
              "  -6.923379421234131,\n",
              "  -7.07765531539917,\n",
              "  -7.350895881652832,\n",
              "  -7.0824809074401855,\n",
              "  -7.255969524383545,\n",
              "  -7.506227016448975,\n",
              "  -7.263924598693848,\n",
              "  -7.08511209487915,\n",
              "  -7.215078830718994,\n",
              "  -7.097707748413086,\n",
              "  -7.273107528686523,\n",
              "  -7.172130107879639,\n",
              "  -7.687312602996826,\n",
              "  -7.276230812072754,\n",
              "  -7.069209575653076,\n",
              "  -7.174448013305664,\n",
              "  -6.9129958152771,\n",
              "  -7.032238006591797,\n",
              "  -7.277698993682861,\n",
              "  -7.6058502197265625,\n",
              "  -7.297726631164551,\n",
              "  -7.531149864196777,\n",
              "  -7.110586643218994,\n",
              "  -7.068671226501465,\n",
              "  -7.416383743286133,\n",
              "  -7.42615270614624,\n",
              "  -6.834729194641113,\n",
              "  -6.981551170349121,\n",
              "  -7.580985069274902,\n",
              "  -7.360281467437744,\n",
              "  -7.124268531799316,\n",
              "  -7.168705463409424,\n",
              "  -7.241336345672607,\n",
              "  -7.252207279205322,\n",
              "  -7.050995349884033,\n",
              "  -7.107569217681885,\n",
              "  -7.457540512084961,\n",
              "  -7.127503871917725,\n",
              "  -7.503390312194824,\n",
              "  -7.163876056671143,\n",
              "  -7.438722610473633,\n",
              "  -7.2639570236206055,\n",
              "  -7.325522422790527,\n",
              "  -7.089610576629639,\n",
              "  -7.268285274505615,\n",
              "  -7.480783939361572,\n",
              "  -7.357827663421631,\n",
              "  -7.086298942565918,\n",
              "  -7.394703388214111,\n",
              "  -7.008011817932129,\n",
              "  -7.220012664794922,\n",
              "  -7.344491004943848,\n",
              "  -7.250124454498291,\n",
              "  -7.2747087478637695,\n",
              "  -7.635982036590576,\n",
              "  -7.352258205413818,\n",
              "  -7.048211574554443,\n",
              "  -7.494211196899414,\n",
              "  -7.0965471267700195,\n",
              "  -7.013978004455566,\n",
              "  -7.522703647613525,\n",
              "  -7.336707592010498,\n",
              "  -7.047713279724121,\n",
              "  -7.2226033210754395,\n",
              "  -7.33993673324585,\n",
              "  -7.528631210327148,\n",
              "  -7.28114652633667,\n",
              "  -6.896076679229736,\n",
              "  -7.011866569519043,\n",
              "  -7.10582971572876,\n",
              "  -7.122448921203613,\n",
              "  -6.9921345710754395,\n",
              "  -7.071268558502197,\n",
              "  -7.2258405685424805,\n",
              "  -7.170327663421631,\n",
              "  -7.351353168487549,\n",
              "  -7.340765953063965,\n",
              "  -7.450222492218018,\n",
              "  -7.508593559265137,\n",
              "  -7.355740070343018,\n",
              "  -7.132697582244873,\n",
              "  -7.61845588684082,\n",
              "  -7.216464519500732,\n",
              "  -7.131529331207275,\n",
              "  -7.1095099449157715,\n",
              "  -7.050068378448486,\n",
              "  -7.4605231285095215,\n",
              "  -7.021799087524414,\n",
              "  -7.3617143630981445,\n",
              "  -7.175791263580322,\n",
              "  -7.588837146759033,\n",
              "  -7.013445854187012,\n",
              "  -7.08626651763916,\n",
              "  -7.0371856689453125,\n",
              "  -7.297652244567871,\n",
              "  -7.157552719116211,\n",
              "  -7.260792255401611,\n",
              "  -6.896083831787109,\n",
              "  -6.812305450439453,\n",
              "  -7.176941871643066,\n",
              "  -7.364978790283203,\n",
              "  -7.633736610412598,\n",
              "  -7.241308689117432,\n",
              "  -7.3570170402526855,\n",
              "  -7.221656322479248,\n",
              "  -7.660017013549805,\n",
              "  -7.282673358917236,\n",
              "  -7.329424858093262,\n",
              "  -7.033370494842529,\n",
              "  -7.2596893310546875,\n",
              "  -7.29327392578125,\n",
              "  -7.393415927886963,\n",
              "  -7.159940242767334,\n",
              "  -7.162850379943848,\n",
              "  -7.379093170166016,\n",
              "  -6.9981794357299805,\n",
              "  -7.076922416687012,\n",
              "  -7.323297023773193,\n",
              "  -7.6691389083862305,\n",
              "  -7.313684463500977,\n",
              "  -7.2121500968933105,\n",
              "  -7.840810775756836,\n",
              "  -7.370574951171875,\n",
              "  -7.20072603225708,\n",
              "  -7.331588268280029,\n",
              "  -7.359118461608887,\n",
              "  -7.382368087768555,\n",
              "  -7.107268810272217,\n",
              "  -7.692768096923828,\n",
              "  -7.231034278869629,\n",
              "  -7.291359901428223,\n",
              "  -7.383741855621338,\n",
              "  -7.244074821472168,\n",
              "  -7.014541149139404,\n",
              "  -7.273740768432617,\n",
              "  -7.335334300994873,\n",
              "  -7.306700706481934,\n",
              "  -7.191019058227539,\n",
              "  -7.010864734649658,\n",
              "  -7.0777177810668945,\n",
              "  -7.3552374839782715,\n",
              "  -6.866742134094238,\n",
              "  -7.520420551300049,\n",
              "  -7.38477897644043,\n",
              "  -7.352785110473633,\n",
              "  -7.107726573944092,\n",
              "  -7.397836208343506,\n",
              "  -7.413583755493164,\n",
              "  -7.638075828552246,\n",
              "  -6.794464111328125,\n",
              "  -7.431479454040527,\n",
              "  -7.353490829467773,\n",
              "  -7.187237739562988,\n",
              "  -7.39656400680542,\n",
              "  -6.795964241027832,\n",
              "  -7.724820137023926,\n",
              "  -7.184302806854248,\n",
              "  -7.299961090087891,\n",
              "  -6.971170425415039,\n",
              "  -7.526994705200195,\n",
              "  -7.316669464111328,\n",
              "  -7.42549467086792,\n",
              "  -6.915970325469971,\n",
              "  -7.503936290740967,\n",
              "  -7.478081703186035,\n",
              "  -7.842895030975342,\n",
              "  -7.447965621948242,\n",
              "  -7.628289222717285,\n",
              "  -7.099000930786133,\n",
              "  -7.398757457733154,\n",
              "  -7.70414400100708,\n",
              "  -6.843661785125732,\n",
              "  -7.409040451049805,\n",
              "  -7.433097839355469,\n",
              "  -7.176577568054199,\n",
              "  -7.307056903839111,\n",
              "  -7.178911209106445,\n",
              "  -7.104545593261719,\n",
              "  -6.908905982971191,\n",
              "  -7.507011413574219,\n",
              "  -7.302157402038574,\n",
              "  -7.33745813369751,\n",
              "  -7.133864402770996,\n",
              "  -7.046637535095215,\n",
              "  -7.343221187591553,\n",
              "  -7.325740337371826,\n",
              "  -7.337482452392578,\n",
              "  -7.2684407234191895,\n",
              "  -7.265804290771484,\n",
              "  -6.9962568283081055,\n",
              "  -7.293020725250244,\n",
              "  -7.411079406738281,\n",
              "  -7.1595683097839355,\n",
              "  -7.429030418395996,\n",
              "  -7.84988260269165,\n",
              "  -7.319888114929199,\n",
              "  -7.206300258636475,\n",
              "  -7.448176383972168,\n",
              "  -7.1578688621521,\n",
              "  -6.8672709465026855,\n",
              "  -7.1681976318359375,\n",
              "  -7.019655704498291,\n",
              "  -7.464483737945557,\n",
              "  -7.253335475921631,\n",
              "  -7.009235858917236,\n",
              "  -7.141767501831055,\n",
              "  -7.19437313079834,\n",
              "  -7.572628021240234,\n",
              "  -7.245506763458252,\n",
              "  -7.471404552459717,\n",
              "  -7.32177734375,\n",
              "  -7.102579593658447,\n",
              "  -6.919036388397217,\n",
              "  -7.091187477111816,\n",
              "  -7.650650501251221,\n",
              "  -7.440847873687744,\n",
              "  -7.362460613250732,\n",
              "  -6.984218597412109,\n",
              "  -7.276635646820068,\n",
              "  -7.16584587097168,\n",
              "  -7.353059768676758,\n",
              "  -7.458205699920654,\n",
              "  -7.193152904510498,\n",
              "  -7.426337718963623,\n",
              "  -7.221636772155762,\n",
              "  -7.020404815673828,\n",
              "  -7.19510555267334,\n",
              "  -7.3051252365112305,\n",
              "  -7.326834201812744,\n",
              "  -7.412170886993408,\n",
              "  -7.662701606750488,\n",
              "  -7.534420967102051,\n",
              "  -7.198593616485596,\n",
              "  -7.4805097579956055,\n",
              "  -7.222714424133301,\n",
              "  -7.59526252746582,\n",
              "  -7.280490398406982,\n",
              "  -7.488658905029297,\n",
              "  -7.3834147453308105,\n",
              "  -7.436706066131592,\n",
              "  -7.497785568237305,\n",
              "  -7.264886856079102,\n",
              "  -7.375692367553711,\n",
              "  -7.251705169677734,\n",
              "  -6.853765964508057,\n",
              "  -7.3574628829956055,\n",
              "  -7.004663467407227,\n",
              "  -7.263056755065918,\n",
              "  -7.364260196685791,\n",
              "  -7.074688911437988,\n",
              "  -7.039860725402832,\n",
              "  -7.16870641708374,\n",
              "  -7.2182111740112305,\n",
              "  -7.575558662414551,\n",
              "  -7.192621231079102,\n",
              "  -6.825882911682129,\n",
              "  -7.101099967956543,\n",
              "  -7.384588241577148,\n",
              "  -7.206119537353516,\n",
              "  -7.139981746673584,\n",
              "  -7.068044662475586,\n",
              "  -7.244289398193359,\n",
              "  -7.221675872802734,\n",
              "  -7.404447078704834,\n",
              "  -7.539587020874023,\n",
              "  -7.191866874694824,\n",
              "  -7.077197551727295,\n",
              "  -7.102449417114258,\n",
              "  -7.072108268737793,\n",
              "  -7.39059591293335,\n",
              "  -6.9457316398620605,\n",
              "  -7.662012100219727,\n",
              "  -7.011348247528076,\n",
              "  -7.334400177001953,\n",
              "  -7.310722827911377,\n",
              "  -7.401053428649902,\n",
              "  -7.142351150512695,\n",
              "  -7.3697662353515625,\n",
              "  -7.2887773513793945,\n",
              "  -7.187860488891602,\n",
              "  -7.395047187805176,\n",
              "  -7.0641069412231445,\n",
              "  -7.412966251373291,\n",
              "  -7.58815860748291,\n",
              "  -7.438971042633057,\n",
              "  -7.311569690704346,\n",
              "  -7.205547332763672,\n",
              "  -7.014074325561523,\n",
              "  -7.273208141326904,\n",
              "  -7.1356329917907715,\n",
              "  -7.565232753753662,\n",
              "  -7.108163833618164,\n",
              "  -7.052709102630615,\n",
              "  -7.1668853759765625,\n",
              "  -7.04585075378418,\n",
              "  -7.430610179901123,\n",
              "  -6.904876708984375,\n",
              "  -7.3207244873046875,\n",
              "  -7.082180023193359,\n",
              "  -6.9907379150390625,\n",
              "  -6.865591526031494,\n",
              "  -7.082278251647949,\n",
              "  -7.255889892578125,\n",
              "  -7.0868611335754395,\n",
              "  -7.554018020629883,\n",
              "  -7.268392562866211,\n",
              "  -7.323362350463867,\n",
              "  -7.237934589385986,\n",
              "  -7.217595100402832,\n",
              "  -7.307725429534912,\n",
              "  -7.082854270935059,\n",
              "  -7.889678001403809,\n",
              "  -7.08353853225708,\n",
              "  -7.0652570724487305,\n",
              "  -7.338856220245361,\n",
              "  -7.512245178222656,\n",
              "  -7.374061584472656,\n",
              "  -7.121822357177734,\n",
              "  -6.847006797790527,\n",
              "  -7.321990013122559,\n",
              "  -7.1351518630981445,\n",
              "  -7.313877105712891,\n",
              "  -7.216727256774902,\n",
              "  -6.695535659790039,\n",
              "  -7.495771884918213,\n",
              "  -7.578980922698975,\n",
              "  -7.5166425704956055,\n",
              "  -7.184883117675781,\n",
              "  -7.282114028930664,\n",
              "  -7.116682529449463,\n",
              "  -7.372460842132568,\n",
              "  -7.342747688293457,\n",
              "  -6.913812637329102,\n",
              "  -7.240047454833984,\n",
              "  -7.380471229553223,\n",
              "  -7.310262680053711,\n",
              "  -7.1192803382873535,\n",
              "  -7.008793354034424,\n",
              "  -7.074216842651367,\n",
              "  -7.259839057922363,\n",
              "  -7.160664081573486,\n",
              "  -7.004992961883545,\n",
              "  -7.209906578063965,\n",
              "  -7.434596061706543,\n",
              "  -7.327068328857422,\n",
              "  -7.324731826782227,\n",
              "  -7.293239593505859,\n",
              "  -7.366214275360107,\n",
              "  -7.323646545410156,\n",
              "  -7.376987934112549,\n",
              "  -7.337629318237305,\n",
              "  -7.796454906463623,\n",
              "  -7.332733631134033,\n",
              "  -7.064727306365967,\n",
              "  -7.291779518127441,\n",
              "  -7.513595104217529,\n",
              "  -7.41000509262085,\n",
              "  -7.128941535949707,\n",
              "  -6.956016540527344,\n",
              "  -7.114042282104492,\n",
              "  -7.088147163391113,\n",
              "  -7.097965240478516,\n",
              "  -7.380186080932617,\n",
              "  -7.232852458953857,\n",
              "  -7.22804069519043,\n",
              "  -7.285955905914307,\n",
              "  -7.037126064300537,\n",
              "  -7.040713310241699,\n",
              "  -7.169122695922852,\n",
              "  -6.986281394958496,\n",
              "  -7.46632194519043,\n",
              "  -7.133676052093506,\n",
              "  -7.351590633392334,\n",
              "  -7.28334903717041,\n",
              "  -7.22610330581665,\n",
              "  -7.468005180358887,\n",
              "  -6.865049362182617,\n",
              "  -7.541806697845459,\n",
              "  -7.259804725646973,\n",
              "  -7.156105041503906,\n",
              "  -7.504584312438965,\n",
              "  -7.627208232879639,\n",
              "  -7.512438774108887,\n",
              "  -7.270166873931885,\n",
              "  -7.103250980377197,\n",
              "  -7.409217834472656,\n",
              "  -6.964263439178467,\n",
              "  -6.941399574279785,\n",
              "  -7.545660018920898,\n",
              "  -7.2880659103393555,\n",
              "  -7.303735733032227,\n",
              "  -7.5257368087768555,\n",
              "  -7.329187393188477,\n",
              "  -7.146812438964844,\n",
              "  -6.914811134338379,\n",
              "  -7.176157474517822,\n",
              "  -7.314124584197998,\n",
              "  -6.843785762786865,\n",
              "  -7.380319118499756,\n",
              "  -7.203856945037842,\n",
              "  -7.5501861572265625,\n",
              "  -7.1161699295043945,\n",
              "  -6.989686965942383,\n",
              "  -7.485179901123047,\n",
              "  -6.9463043212890625,\n",
              "  -7.344447135925293,\n",
              "  -6.885662078857422,\n",
              "  -7.3064799308776855,\n",
              "  -7.146557331085205,\n",
              "  -7.310506820678711,\n",
              "  -7.4040985107421875,\n",
              "  -7.225369453430176,\n",
              "  -7.322343826293945,\n",
              "  -7.109358310699463,\n",
              "  -7.142853736877441,\n",
              "  -6.903156280517578,\n",
              "  -7.317811489105225,\n",
              "  -7.459712982177734,\n",
              "  -7.008944034576416,\n",
              "  -7.017910480499268,\n",
              "  -7.347832679748535,\n",
              "  -7.272327423095703,\n",
              "  -7.242536544799805,\n",
              "  -7.314883708953857,\n",
              "  -7.412046909332275,\n",
              "  -7.103951454162598,\n",
              "  -7.549583911895752,\n",
              "  -7.325613975524902,\n",
              "  -7.213922023773193,\n",
              "  -7.3686065673828125,\n",
              "  -7.567840576171875,\n",
              "  -6.92803955078125,\n",
              "  -7.43615198135376,\n",
              "  -7.673901081085205,\n",
              "  -7.040221214294434,\n",
              "  -7.379118919372559,\n",
              "  -7.53557825088501,\n",
              "  -7.106666564941406,\n",
              "  -7.200977325439453,\n",
              "  -7.365093231201172,\n",
              "  -7.327728748321533,\n",
              "  -7.044990062713623,\n",
              "  -7.087360858917236,\n",
              "  -7.27212381362915,\n",
              "  -7.482725143432617,\n",
              "  -7.065123558044434,\n",
              "  -6.876776218414307,\n",
              "  -7.259955883026123,\n",
              "  -6.988342761993408,\n",
              "  -7.320801734924316,\n",
              "  -7.463099956512451,\n",
              "  -7.266157150268555,\n",
              "  -7.416046619415283,\n",
              "  -7.206478595733643,\n",
              "  -7.316964149475098,\n",
              "  -7.015746593475342,\n",
              "  -7.455726146697998,\n",
              "  -7.44948673248291,\n",
              "  -7.456411361694336,\n",
              "  -7.110403060913086,\n",
              "  -7.547218322753906,\n",
              "  -7.026899337768555,\n",
              "  -7.288097858428955,\n",
              "  -7.292947769165039,\n",
              "  -7.350737571716309,\n",
              "  -7.173065185546875,\n",
              "  -7.6905927658081055,\n",
              "  -6.94019889831543,\n",
              "  -7.329001426696777,\n",
              "  -7.356138706207275,\n",
              "  -7.1818623542785645,\n",
              "  -7.340267181396484,\n",
              "  -7.361937046051025,\n",
              "  -7.32913064956665,\n",
              "  -6.94884729385376,\n",
              "  -7.172908782958984,\n",
              "  -6.950699329376221,\n",
              "  -7.311214923858643,\n",
              "  -7.349412441253662,\n",
              "  -7.0532331466674805,\n",
              "  -6.801152229309082,\n",
              "  -7.649286270141602,\n",
              "  -7.194447040557861,\n",
              "  -7.225221633911133,\n",
              "  -7.39055871963501,\n",
              "  -6.738446235656738,\n",
              "  -7.56466007232666,\n",
              "  -7.506568431854248,\n",
              "  -7.465134620666504,\n",
              "  -7.678043365478516,\n",
              "  -6.94447660446167,\n",
              "  -7.451783180236816,\n",
              "  -7.2905144691467285,\n",
              "  -7.481486797332764,\n",
              "  -7.084629058837891,\n",
              "  -7.168842792510986,\n",
              "  -7.455717086791992,\n",
              "  -7.663901329040527,\n",
              "  -7.040245532989502,\n",
              "  -7.561558723449707,\n",
              "  -7.488295078277588,\n",
              "  -7.102230548858643,\n",
              "  -7.169443130493164,\n",
              "  -7.683671474456787,\n",
              "  -7.2545061111450195,\n",
              "  -7.380476474761963,\n",
              "  -7.54978084564209,\n",
              "  -7.206201553344727,\n",
              "  -6.870316505432129,\n",
              "  -7.221411228179932,\n",
              "  -7.2648606300354,\n",
              "  -6.905770301818848,\n",
              "  -7.087437629699707,\n",
              "  -6.985249996185303,\n",
              "  -7.409655570983887,\n",
              "  -7.378744602203369,\n",
              "  -7.360201835632324,\n",
              "  -7.225789546966553,\n",
              "  -7.581818103790283,\n",
              "  -7.35734748840332,\n",
              "  -7.017732620239258,\n",
              "  -7.146984100341797,\n",
              "  -7.425905704498291,\n",
              "  -7.173887252807617,\n",
              "  -7.018039703369141,\n",
              "  -7.4184746742248535,\n",
              "  -7.140829086303711,\n",
              "  -7.725516319274902,\n",
              "  -7.207568168640137,\n",
              "  -6.998082637786865,\n",
              "  -7.490823268890381,\n",
              "  -7.149270534515381,\n",
              "  -7.113491058349609,\n",
              "  -6.753781795501709,\n",
              "  -7.453950881958008,\n",
              "  -6.925407886505127,\n",
              "  -7.388526439666748,\n",
              "  -7.376235485076904,\n",
              "  -7.646210670471191,\n",
              "  -7.5042877197265625,\n",
              "  -7.033548831939697,\n",
              "  -7.15421724319458,\n",
              "  -7.35392427444458,\n",
              "  -6.941567420959473,\n",
              "  -7.319091320037842,\n",
              "  -7.233253479003906,\n",
              "  -7.095091342926025,\n",
              "  -6.916129112243652,\n",
              "  -6.611989498138428,\n",
              "  -7.3490166664123535,\n",
              "  -7.448729515075684,\n",
              "  -7.117600917816162,\n",
              "  -7.263365268707275,\n",
              "  -7.270105361938477,\n",
              "  -7.304532051086426,\n",
              "  -7.4742231369018555,\n",
              "  -7.04640531539917,\n",
              "  -7.493197441101074,\n",
              "  -7.144132614135742,\n",
              "  -7.400793075561523,\n",
              "  -7.022928237915039,\n",
              "  -7.518481731414795,\n",
              "  -7.350675582885742,\n",
              "  -7.155996322631836,\n",
              "  -6.7693772315979,\n",
              "  -7.676271915435791,\n",
              "  -7.120509147644043,\n",
              "  -7.667868614196777,\n",
              "  -7.414089679718018,\n",
              "  -7.366307258605957,\n",
              "  -7.248619079589844,\n",
              "  -7.443048477172852,\n",
              "  -6.919548034667969,\n",
              "  -7.065291404724121,\n",
              "  -7.008553981781006,\n",
              "  -7.104700565338135,\n",
              "  -6.749078750610352,\n",
              "  -7.083937644958496,\n",
              "  -7.249456882476807,\n",
              "  -7.355103492736816,\n",
              "  -7.2531609535217285,\n",
              "  -7.159185886383057,\n",
              "  -7.404764652252197,\n",
              "  -7.081369400024414,\n",
              "  -7.2834649085998535,\n",
              "  -7.097648620605469,\n",
              "  -7.162301540374756,\n",
              "  -7.085582256317139,\n",
              "  -7.2662529945373535,\n",
              "  -7.021241664886475,\n",
              "  -7.205560684204102,\n",
              "  -7.383851528167725,\n",
              "  -7.538008689880371,\n",
              "  -7.314424514770508,\n",
              "  -7.431806564331055,\n",
              "  -7.272232532501221,\n",
              "  -7.3069329261779785,\n",
              "  -7.687909126281738,\n",
              "  -7.267093181610107,\n",
              "  -7.188405990600586,\n",
              "  -7.290305137634277,\n",
              "  -7.273244857788086,\n",
              "  -7.290119647979736,\n",
              "  -6.824636459350586,\n",
              "  -7.151191711425781,\n",
              "  -7.364804267883301,\n",
              "  -7.4847092628479,\n",
              "  -6.937663555145264,\n",
              "  -7.309928894042969,\n",
              "  -7.262615203857422,\n",
              "  -6.836398124694824,\n",
              "  -6.855852127075195,\n",
              "  -7.553637504577637,\n",
              "  -7.586400032043457,\n",
              "  -7.618767261505127,\n",
              "  -7.206387996673584,\n",
              "  -6.704020023345947,\n",
              "  -7.685050964355469,\n",
              "  -7.313335418701172,\n",
              "  -7.506692886352539,\n",
              "  -7.065731048583984,\n",
              "  -7.280860424041748,\n",
              "  -7.23176908493042,\n",
              "  -7.329136848449707,\n",
              "  -7.355301380157471,\n",
              "  -7.365120887756348,\n",
              "  -7.1103644371032715,\n",
              "  -7.632428169250488,\n",
              "  -7.05849552154541,\n",
              "  -7.193112373352051,\n",
              "  -7.25161075592041,\n",
              "  -7.02782678604126,\n",
              "  -7.132176399230957,\n",
              "  -7.2112507820129395,\n",
              "  -7.224806308746338,\n",
              "  -7.351840496063232,\n",
              "  -7.359005451202393,\n",
              "  -7.213351249694824,\n",
              "  -7.117124557495117,\n",
              "  -7.395916938781738,\n",
              "  -7.260793209075928,\n",
              "  -7.028103828430176,\n",
              "  -7.45759391784668,\n",
              "  -7.119842052459717,\n",
              "  -7.033142566680908,\n",
              "  -6.975295543670654,\n",
              "  -7.416734218597412,\n",
              "  -6.770723819732666,\n",
              "  -7.367390155792236,\n",
              "  -7.22278356552124,\n",
              "  -7.062422275543213,\n",
              "  -7.23472261428833,\n",
              "  -7.612632751464844,\n",
              "  -7.010219573974609,\n",
              "  -7.2003560066223145,\n",
              "  -7.288379192352295,\n",
              "  -7.330589294433594,\n",
              "  -7.108681678771973,\n",
              "  -7.443480014801025,\n",
              "  -7.392064094543457,\n",
              "  -7.082174777984619,\n",
              "  -7.275176525115967,\n",
              "  -6.927926063537598,\n",
              "  -7.116638660430908,\n",
              "  -7.314086437225342,\n",
              "  -6.901785850524902,\n",
              "  -7.388014793395996,\n",
              "  -7.4570817947387695,\n",
              "  -7.224140644073486,\n",
              "  -7.135969161987305,\n",
              "  -7.369172096252441,\n",
              "  -6.993865489959717,\n",
              "  -6.9116082191467285,\n",
              "  -7.0778117179870605,\n",
              "  -7.1217145919799805,\n",
              "  -7.362266540527344,\n",
              "  -7.272881984710693,\n",
              "  -7.115985870361328,\n",
              "  -7.135786056518555,\n",
              "  -7.087973594665527,\n",
              "  -7.085000514984131,\n",
              "  -7.312248229980469,\n",
              "  -7.258463382720947,\n",
              "  -7.515289783477783,\n",
              "  -7.2655463218688965,\n",
              "  -7.053305625915527,\n",
              "  -7.415190696716309,\n",
              "  -7.473562240600586,\n",
              "  -7.405084133148193,\n",
              "  -7.164470672607422,\n",
              "  -7.414924144744873,\n",
              "  -7.450104713439941,\n",
              "  -7.167479515075684,\n",
              "  -7.453184127807617,\n",
              "  -7.318782329559326,\n",
              "  -7.109014511108398,\n",
              "  -6.953543186187744,\n",
              "  -6.826827049255371,\n",
              "  -7.26253604888916,\n",
              "  -7.703887939453125,\n",
              "  -7.427504539489746,\n",
              "  -7.252021789550781,\n",
              "  -7.427945137023926,\n",
              "  -7.537241458892822,\n",
              "  -7.139878749847412,\n",
              "  -7.403933525085449,\n",
              "  -7.7286553382873535,\n",
              "  -6.983469486236572,\n",
              "  -7.457596778869629,\n",
              "  -7.364773273468018,\n",
              "  -7.39475154876709,\n",
              "  -7.6404547691345215,\n",
              "  -7.348612308502197,\n",
              "  -7.343467712402344,\n",
              "  -7.617247581481934,\n",
              "  -6.937297344207764,\n",
              "  -7.245468616485596,\n",
              "  -7.507546901702881,\n",
              "  -7.3401713371276855,\n",
              "  -7.309807777404785,\n",
              "  -7.2531023025512695,\n",
              "  -7.3186516761779785,\n",
              "  -7.4448981285095215,\n",
              "  -7.05446195602417,\n",
              "  -7.039074420928955,\n",
              "  -6.993809223175049,\n",
              "  -7.331134796142578,\n",
              "  -7.021711826324463,\n",
              "  -7.150407791137695,\n",
              "  -7.3631591796875,\n",
              "  -7.205323219299316,\n",
              "  -7.370913505554199,\n",
              "  -7.107656002044678,\n",
              "  -7.510072708129883,\n",
              "  -7.519161224365234,\n",
              "  -7.095831871032715,\n",
              "  -7.069659233093262,\n",
              "  -6.955608367919922,\n",
              "  -7.48043966293335,\n",
              "  -6.819596290588379,\n",
              "  -7.138005256652832,\n",
              "  -7.1605329513549805,\n",
              "  -7.287569046020508,\n",
              "  -7.408103942871094,\n",
              "  -6.963370323181152,\n",
              "  -7.2358856201171875,\n",
              "  -6.979931831359863,\n",
              "  -7.156761169433594,\n",
              "  -7.190177917480469,\n",
              "  -6.868198871612549,\n",
              "  -7.289946556091309,\n",
              "  -6.9918928146362305,\n",
              "  -7.403949737548828,\n",
              "  -7.281961441040039,\n",
              "  -7.239253997802734,\n",
              "  -7.267149925231934,\n",
              "  -7.108067989349365,\n",
              "  -7.2535552978515625,\n",
              "  -7.53272819519043,\n",
              "  -6.95409631729126,\n",
              "  -6.905971527099609,\n",
              "  -7.255071640014648,\n",
              "  -7.025372505187988,\n",
              "  -7.419416427612305,\n",
              "  -7.419334888458252,\n",
              "  -7.2324724197387695,\n",
              "  -7.311883449554443,\n",
              "  -7.498557090759277,\n",
              "  -7.318296909332275,\n",
              "  -7.309085845947266,\n",
              "  -7.28561544418335,\n",
              "  -7.373824119567871,\n",
              "  -7.513821125030518,\n",
              "  -7.220406532287598,\n",
              "  -7.312179088592529,\n",
              "  -7.197006702423096,\n",
              "  -7.343059062957764,\n",
              "  -7.270815849304199,\n",
              "  -6.960455894470215,\n",
              "  -7.828762054443359,\n",
              "  -7.3140459060668945,\n",
              "  -7.413385391235352,\n",
              "  -7.232797145843506,\n",
              "  -7.00535774230957,\n",
              "  -7.241596698760986,\n",
              "  -7.189296722412109,\n",
              "  -7.320751667022705,\n",
              "  -7.226227283477783,\n",
              "  -7.30900239944458,\n",
              "  -6.918707847595215,\n",
              "  -7.267316818237305,\n",
              "  -7.335977554321289,\n",
              "  -7.061344146728516,\n",
              "  -7.26424503326416,\n",
              "  -7.438076019287109,\n",
              "  -7.251097679138184,\n",
              "  -7.3974928855896,\n",
              "  -7.197418689727783,\n",
              "  -7.559014320373535,\n",
              "  -7.317327976226807,\n",
              "  -7.2118611335754395,\n",
              "  -7.5230631828308105,\n",
              "  -7.529818534851074,\n",
              "  -7.163195610046387,\n",
              "  -7.249971866607666,\n",
              "  -7.322088241577148,\n",
              "  -7.417126178741455,\n",
              "  -7.254178524017334,\n",
              "  -7.703040599822998,\n",
              "  -7.249009132385254,\n",
              "  -7.279636383056641,\n",
              "  -7.292547225952148,\n",
              "  -7.226620674133301,\n",
              "  -7.270879745483398,\n",
              "  -7.387332916259766,\n",
              "  -7.396403789520264,\n",
              "  -7.147486686706543,\n",
              "  -7.301698207855225,\n",
              "  -7.1373419761657715,\n",
              "  -7.491940498352051,\n",
              "  -7.04467248916626,\n",
              "  -7.109954357147217,\n",
              "  -7.087457656860352,\n",
              "  -7.272479057312012,\n",
              "  -6.857279300689697,\n",
              "  -7.146613597869873,\n",
              "  -7.265466213226318,\n",
              "  -7.132102966308594,\n",
              "  -7.036698818206787,\n",
              "  -7.522179126739502,\n",
              "  -7.065703868865967,\n",
              "  -6.933516979217529,\n",
              "  -6.929693222045898,\n",
              "  -7.147615432739258,\n",
              "  -7.408334732055664,\n",
              "  -7.349559307098389,\n",
              "  -7.2065935134887695,\n",
              "  -7.156430244445801,\n",
              "  -7.36550760269165,\n",
              "  -6.934291839599609,\n",
              "  -7.20101261138916,\n",
              "  -7.285659313201904,\n",
              "  -7.387397766113281,\n",
              "  -7.686378479003906,\n",
              "  -7.489968299865723,\n",
              "  -7.290344715118408,\n",
              "  -7.282848358154297,\n",
              "  -7.080985069274902,\n",
              "  -7.432412147521973,\n",
              "  -7.368369102478027,\n",
              "  -7.218288421630859,\n",
              "  -7.7539167404174805,\n",
              "  -7.139800548553467,\n",
              "  -7.119908809661865,\n",
              "  -7.4716386795043945,\n",
              "  ...]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtuFjTH_6p5D",
        "colab_type": "code",
        "outputId": "0b14a8fb-291c-40f1-c119-9046cfd8e1a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Select the index with highest softmax probabilities\n",
        "# See https://pytorch.org/docs/stable/torch.html#torch.max\n",
        "torch.max(softmax(lin2(h_x)), 1)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([-6.6120], grad_fn=<MaxBackward0>), indices=tensor([693]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcfWwZk67rG",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-train-cbow\"></a>\n",
        "\n",
        "# Now, we train the CBOW model for real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRig9kyI61Xn",
        "colab_type": "code",
        "outputId": "8ac507a7-2a85-4920-9ac2-31a3d3a250a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First we split the data into training and testing.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
        "len(tokenized_text_train), len(tokenized_text_test)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfzjsjoq7Cvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Put the input context indices into the embeddings\n",
        "        # then squeeze it into a single dimension vector with tensor.view((1,-1))\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        # Put the embedding input through linear layer,\n",
        "        # then an activation function to create the hidden layer.\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        # Put the hidden layer through a second linear layer,\n",
        "        out = self.linear2(hid)\n",
        "        # then a last layer activation function to generate\n",
        "        # pobabilities, hint https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNm40ApS77al",
        "colab_type": "code",
        "outputId": "425edb4e-6941-40f6-d977-df1bc29d2680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embd_size = 100\n",
        "learning_rate = 0.003\n",
        "hidden_size = 100\n",
        "window_size = 2\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the dataset.\n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
        "vocab_size = len(w2v_dataset.vocab)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "# Hint: the CBOW model object you've created.\n",
        "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "\n",
        "model = nn.DataParallel(model).to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "for _e in tqdm(range(num_epochs)):\n",
        "    epoch_loss = []\n",
        "    for sent_idx in range(w2v_dataset._len):\n",
        "        for w2v_io in w2v_dataset[sent_idx]:\n",
        "            # Zero gradient.\n",
        "            optimizer.zero_grad()\n",
        "            # Retrieve the inputs and outputs.\n",
        "            x, y = w2v_io['x'], w2v_io['y']\n",
        "            x = tensor(x).to(device)\n",
        "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
        "            # Calculate the log probability of the context embeddings.\n",
        "            logprobs = model(x)\n",
        "            # This unsqueeze thing is really a feature/bug... -_-\n",
        "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss.append(float(loss))\n",
        "    # Save model after every epoch.\n",
        "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
        "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
        "  \n",
        "# Save model after last epoch.\n",
        "#torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 1/100 [00:04<07:40,  4.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 2/100 [00:09<07:30,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 3/100 [00:13<07:23,  4.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 4/100 [00:18<07:17,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 5/100 [00:22<07:16,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 6/100 [00:27<07:12,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 7/100 [00:31<07:02,  4.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 8/100 [00:36<06:56,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 9/100 [00:40<06:54,  4.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 10/100 [00:45<06:48,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 11/100 [00:49<06:41,  4.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 12/100 [00:54<06:36,  4.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 13/100 [00:58<06:32,  4.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 14/100 [01:03<06:35,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 15/100 [01:08<06:27,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 16/100 [01:12<06:21,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 17/100 [01:17<06:16,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 18/100 [01:21<06:13,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 19/100 [01:26<06:11,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 20/100 [01:30<06:04,  4.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 21/100 [01:35<05:57,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 22/100 [01:39<05:51,  4.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 23/100 [01:44<05:49,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 24/100 [01:49<05:44,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 25/100 [01:53<05:38,  4.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 26/100 [01:57<05:33,  4.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 27/100 [02:02<05:32,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 28/100 [02:07<05:26,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 29/100 [02:11<05:20,  4.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 30/100 [02:16<05:17,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 31/100 [02:20<05:17,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 32/100 [02:25<05:14,  4.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 33/100 [02:30<05:08,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 34/100 [02:34<05:03,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 35/100 [02:39<04:56,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 36/100 [02:43<04:55,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 37/100 [02:48<04:49,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 38/100 [02:53<04:44,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 39/100 [02:57<04:39,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 40/100 [03:02<04:37,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 41/100 [03:06<04:30,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 42/100 [03:11<04:24,  4.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 43/100 [03:15<04:20,  4.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 44/100 [03:20<04:16,  4.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 45/100 [03:25<04:15,  4.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 46/100 [03:29<04:08,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 47/100 [03:34<04:03,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 48/100 [03:39<03:58,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 49/100 [03:43<03:55,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 50/100 [03:48<03:51,  4.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 51/100 [03:53<03:48,  4.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 52/100 [03:57<03:45,  4.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 53/100 [04:02<03:45,  4.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 54/100 [04:07<03:36,  4.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▌    | 55/100 [04:12<03:30,  4.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 56/100 [04:16<03:24,  4.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 57/100 [04:21<03:22,  4.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 58/100 [04:26<03:21,  4.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 59/100 [04:30<03:13,  4.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 60/100 [04:35<03:06,  4.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████    | 61/100 [04:40<03:00,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 62/100 [04:44<02:57,  4.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 63/100 [04:49<02:51,  4.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 64/100 [04:53<02:45,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▌   | 65/100 [04:58<02:40,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 66/100 [05:03<02:39,  4.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 67/100 [05:07<02:32,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 68/100 [05:12<02:28,  4.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 69/100 [05:17<02:22,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 70/100 [05:21<02:19,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 71/100 [05:26<02:15,  4.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 72/100 [05:31<02:09,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 73/100 [05:35<02:04,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 74%|███████▍  | 74/100 [05:40<01:59,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 75%|███████▌  | 75/100 [05:44<01:56,  4.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▌  | 76/100 [05:49<01:50,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 77%|███████▋  | 77/100 [05:53<01:45,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 78%|███████▊  | 78/100 [05:58<01:40,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 79%|███████▉  | 79/100 [06:03<01:37,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 80/100 [06:07<01:32,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 81%|████████  | 81/100 [06:12<01:27,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 82%|████████▏ | 82/100 [06:17<01:22,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 83%|████████▎ | 83/100 [06:21<01:18,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 84%|████████▍ | 84/100 [06:26<01:14,  4.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 85%|████████▌ | 85/100 [06:31<01:09,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 86%|████████▌ | 86/100 [06:35<01:04,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 87%|████████▋ | 87/100 [06:40<00:59,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 88%|████████▊ | 88/100 [06:44<00:55,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 89%|████████▉ | 89/100 [06:49<00:50,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 90/100 [06:53<00:45,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 91%|█████████ | 91/100 [06:58<00:41,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 92%|█████████▏| 92/100 [07:03<00:37,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 93%|█████████▎| 93/100 [07:07<00:32,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 94%|█████████▍| 94/100 [07:12<00:27,  4.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 95%|█████████▌| 95/100 [07:16<00:22,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 96%|█████████▌| 96/100 [07:21<00:18,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 97%|█████████▋| 97/100 [07:26<00:13,  4.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 98%|█████████▊| 98/100 [07:30<00:09,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 99%|█████████▉| 99/100 [07:35<00:04,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 100/100 [07:39<00:00,  4.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lGV7nj9Q4H",
        "colab_type": "code",
        "outputId": "fdb2119f-bd6f-4a33-dda8-e2250a2cbcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set(rc={'figure.figsize':(12, 8)})\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAHUCAYAAADY9fvpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAHC1/w9GtPfP0mVOdQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STJnFmVfp_D7",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
        "\n",
        "# Apply and Evaluate the CBOW Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JbTG84xBeiY",
        "colab_type": "code",
        "outputId": "74bb3479-65d4-4058-a4a7-b575ba0ba394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from lazyme import color_str\n",
        "\n",
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            # Remember how to get the best prediction output? \n",
        "            # Hint: https://pytorch.org/docs/stable/torch.html#torch.max\n",
        "            _, prediction =  torch.max(model(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91m:\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91mcorpus\u001b[0m ( or\n",
            "\u001b[92m(\u001b[0m \t\t a word \u001b[91m______\u001b[0m or bigram\n",
            "\u001b[92mor\u001b[0m \t\t word ( \u001b[92mor\u001b[0m bigram ,\n",
            "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91mmornings\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91muncorrelated\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mthis\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[92mis\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[91mthat\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mlarge\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91massociation\u001b[0m for many\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91mrelation\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mfrequency\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mfalse\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91m______\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mas\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
            "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91m1977\u001b[0m , p\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mwhen\u001b[0m methods are\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mstatistics\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mfor\u001b[0m just those\n",
            "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mi\u001b[0m errors that\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[92mthey\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[92mdo\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[92mnot\u001b[0m wish to\n",
            "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mwithout\u001b[0m any scf\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91min\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mphenomena\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91ma\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91m______\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91min\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91man\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mway\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mnot\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
            "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mc\u001b[0m out to\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91m______\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91ma\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mis\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mword\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mfrom\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mn\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91msize\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mthan\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mwords\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mthey\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mthe\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mare\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mstatistic\u001b[0m , this\n",
            "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91m______\u001b[0m out not\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mmodel\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mthat\u001b[0m case .\n",
            "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91maccurate\u001b[0m carroll 1997\n",
            "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
            "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mframes\u001b[0m corpora .\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mthat\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91m______\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91muk\u001b[0m : is\n",
            "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91me\u001b[0m ⫺ 0.5\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mfar\u001b[0m greater than\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[92mthe\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91m______\u001b[0m value ?\n",
            "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mterms\u001b[0m of statistical\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91m______\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mand\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mrandom\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mdistribution\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[91mif\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mthey\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
            "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mor\u001b[0m by an\n",
            "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mtasks\u001b[0m , or\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mand\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mpossible\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mnull\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtheoretical\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91muse\u001b[0m of the\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91m______\u001b[0m often an\n",
            "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mas\u001b[0m way to\n",
            "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mestimate\u001b[0m ; the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
            "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
            "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
            "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
            "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
            "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
            "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mbetween\u001b[0m non-random and\n",
            "\u001b[92mnon-random\u001b[0m \t language is \u001b[91ma\u001b[0m and hence\n",
            "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mlanguage\u001b[0m hence ,\n",
            "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91marbitrary\u001b[0m , when\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mfallon\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mfor\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mresearch\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mwhich\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
            "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m issue :\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91m:\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mare\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91mand\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mrandom\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mcorpus\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mliterature\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mcan\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[91ma\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mand\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mor\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mis\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mmore\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m______\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91malgorithmically\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mthe\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mmary\u001b[0m hypothesis test\n",
            "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
            "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mfor\u001b[0m the fact\n",
            "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mused\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mrandom\u001b[0m concern the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91m:\u001b[0m between a\n",
            "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mover\u001b[0m a linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
            "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mestimate\u001b[0m the relation\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mare\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mcorpus\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[92mfor\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfrom\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91mtwo\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mrandom\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
            "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mevents\u001b[0m , as\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mchance\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mwith\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mwords\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mstatistic\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91mwhich\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[92mlanguage\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mevidence\u001b[0m is then\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mby\u001b[0m the hypothesis\n",
            "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91marbitrary\u001b[0m , be\n",
            "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mis\u001b[0m as :\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91mfor\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[91mnull\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mis\u001b[0m greater than\n",
            "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[91mrather\u001b[0m than 0.5\n",
            "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
            "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
            "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mcritique\u001b[0m of them\n",
            "\u001b[92mof\u001b[0m \t\t 1 % \u001b[92mof\u001b[0m them ,\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91msalience\u001b[0m , devastate\n",
            "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthat\u001b[0m one of\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mtwo\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91massociation\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mamerican\u001b[0m we have\n",
            "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91musers\u001b[0m of data\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91m______\u001b[0m thresholding methods\n",
            "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mfor\u001b[0m distinguish associated\n",
            "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mbe\u001b[0m associated scfs\n",
            "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91m______\u001b[0m scfs from\n",
            "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mscf\u001b[0m from noise\n",
            "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mand\u001b[0m noise .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaYievSyqZsU",
        "colab_type": "code",
        "outputId": "c0ab30c7-b306-4934-a208-e4cb36cab65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(torch.max(model(x), 1))\n",
        "model(x)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([-0.6825], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([4], device='cuda:0'))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-12.1450, -10.9362, -15.8681,  ..., -14.3846, -13.8161, -13.5591]],\n",
              "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsKur-twqfnn",
        "colab_type": "code",
        "outputId": "df8cd0b1-7ca6-447a-e75d-7cf62a6c0aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2936170212765957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4rlSLIrs2pr",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-load-model\"></a>\n",
        "\n",
        "# Go back to the 5th Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcXHTSnlqn99",
        "colab_type": "code",
        "outputId": "65f17af5-1882-4153-c4ab-83a95535b5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model_5 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
        "model_5 = torch.nn.DataParallel(model_5)\n",
        "model_5.load_state_dict(torch.load('cbow_checkpoint_5.pt'))\n",
        "model_5.eval()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): CBOW(\n",
              "    (embeddings): Embedding(1303, 100)\n",
              "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
              "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlG1DRJHqt3I",
        "colab_type": "code",
        "outputId": "57ed5f6f-4e97-4c55-8a43-67550bed7710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        \n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            _, prediction =  torch.max(model_5(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mthe\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[91mand\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91m______\u001b[0m ( or\n",
            "\u001b[92m(\u001b[0m \t\t a word \u001b[91m______\u001b[0m or bigram\n",
            "\u001b[92mor\u001b[0m \t\t word ( \u001b[91m2\u001b[0m bigram ,\n",
            "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mrandom\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91ma\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[92mis\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[91mthe\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91m______\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91m(\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mthe\u001b[0m for many\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91mtwo\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mand\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnever\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91mthe\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mrandom\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mwe\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m)\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
            "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mrandom\u001b[0m , p\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91mrandom\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mword\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mthen\u001b[0m just those\n",
            "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91m______\u001b[0m errors that\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[92mthey\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[92mnot\u001b[0m wish to\n",
            "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91ma\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91m______\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mis\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[92ma\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mrandom\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mnot\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mof\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
            "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mto\u001b[0m out to\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mthe\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mand\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mthe\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mthat\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mto\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mlanguage\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mhypothesis\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mwith\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mχ2\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91m______\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mthe\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mof\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91m)\u001b[0m , this\n",
            "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91m______\u001b[0m out not\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mthe\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mvery\u001b[0m case .\n",
            "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mas\u001b[0m carroll 1997\n",
            "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91mto\u001b[0m of subcategorization\n",
            "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mand\u001b[0m corpora .\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mof\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91m______\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91msame\u001b[0m : is\n",
            "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m)\u001b[0m ⫺ 0.5\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mas\u001b[0m greater than\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[92mthe\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91mand\u001b[0m value ?\n",
            "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mnumber\u001b[0m of statistical\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91m______\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mand\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mverb\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mrandom\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[91mto\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mthey\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
            "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mand\u001b[0m by an\n",
            "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mrandom\u001b[0m , or\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mand\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mand\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mto\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mnull\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtwo\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91muse\u001b[0m of the\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91m______\u001b[0m often an\n",
            "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mas\u001b[0m way to\n",
            "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91msamples\u001b[0m ; the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
            "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91msame\u001b[0m is overlooked\n",
            "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91mand\u001b[0m 16 (\n",
            "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91mof\u001b[0m ( 1\n",
            "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
            "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
            "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mbetween\u001b[0m non-random and\n",
            "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
            "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mthe\u001b[0m hence ,\n",
            "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mrandom\u001b[0m , when\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mare\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mand\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mhave\u001b[0m at linguistic\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91m______\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[91mthe\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mwe\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mthe\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mwhich\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mlanguage\u001b[0m is a\n",
            "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mas\u001b[0m issue :\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mbe\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91mand\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mrandom\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91m______\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91m______\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mthe\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91m______\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[91ma\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mand\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91m______\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mis\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mis\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m______\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[92mand\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mnull\u001b[0m hypothesis test\n",
            "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
            "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mfor\u001b[0m the fact\n",
            "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mword\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mrandom\u001b[0m concern the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91m______\u001b[0m between a\n",
            "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mas\u001b[0m a linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
            "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mreject\u001b[0m the relation\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mis\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91ma\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mas\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91mand\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mrandom\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
            "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mrandom\u001b[0m , as\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91ma\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mwith\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mand\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[91muse\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91mwhich\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[92mlanguage\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mthere\u001b[0m is then\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mand\u001b[0m the hypothesis\n",
            "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mever\u001b[0m , be\n",
            "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91m______\u001b[0m as :\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91mfor\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mand\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[91mnull\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mof\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mis\u001b[0m greater than\n",
            "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
            "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[91mthe\u001b[0m 0.5 ?\n",
            "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
            "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mnumber\u001b[0m of them\n",
            "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mand\u001b[0m them ,\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mdata\u001b[0m , devastate\n",
            "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthe\u001b[0m one of\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mrandom\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mto\u001b[0m we have\n",
            "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91ma\u001b[0m of data\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91m______\u001b[0m thresholding methods\n",
            "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mfor\u001b[0m distinguish associated\n",
            "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mbe\u001b[0m associated scfs\n",
            "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mand\u001b[0m scfs from\n",
            "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91ma\u001b[0m from noise\n",
            "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mand\u001b[0m noise .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3dj17lRtGtK",
        "colab_type": "code",
        "outputId": "4c7081f3-e6ec-445e-9d27-008905a9db08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)\n",
        "model(x).shape[1] == len(w2v_dataset.vocab)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.24680851063829787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Qi9npoVDWq",
        "colab_type": "text"
      },
      "source": [
        "#How to Handle Unknown Words? \n",
        "\n",
        "This is not the best way to handle unknown words, but we can simply assign an index for unknown words.\n",
        "\n",
        "**Hint:** Ensure that you have `gensim` version >= 3.7.0 first. Otherwise this part of the code won't work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QGgLystN_I",
        "colab_type": "code",
        "outputId": "0530a975-3655-4f9e-f425-75b755ec8ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python -m pip install -U pip\n",
        "!python -m pip install -U gensim>=3.7.0"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlnxD-rwVMI2",
        "colab_type": "code",
        "outputId": "779ad233-1c48-4625-c83e-2b2608113ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gensim\n",
        "gensim.__version__"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0RBfFXIVYud",
        "colab_type": "code",
        "outputId": "27faf370-03eb-49af-b52c-a46871e9b172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
        "dict(vocab.items())"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11E9hmCCVcjQ",
        "colab_type": "code",
        "outputId": "3ce00d56-0813-4ba9-8b43-77d2b96466cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
        "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
        "\n",
        "try:\n",
        "    special_tokens = {'<pad>': 0, '<unk>': 1} # we define our own special tokens for missing words\n",
        "    vocab.patch_with_special_tokens(special_tokens)\n",
        "except: # If gensim is not 3.7.0\n",
        "    pass\n",
        "    \n",
        "dict(vocab.items())"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<pad>',\n",
              " 1: '<unk>',\n",
              " 2: 'foo',\n",
              " 3: 'is',\n",
              " 4: 'sentence',\n",
              " 5: 'this',\n",
              " 6: 'a',\n",
              " 7: 'bar'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pWznO2bYCWQ",
        "colab_type": "text"
      },
      "source": [
        "#Lets Rewrite the `Word2VecText` Object\n",
        "\n",
        "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmtJmBH0XmTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecText(Dataset):\n",
        "    def __init__(self, tokenized_texts, window_size, variant):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self._len = len(self.sents)\n",
        "        \n",
        "        # Add the unknown word patch here.\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "        try:\n",
        "            special_tokens = {'<pad>': 0, '<unk>': 1}\n",
        "            self.vocab.patch_with_special_tokens(special_tokens)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.window_size = window_size\n",
        "        self.variant = variant\n",
        "        if variant.lower() == 'cbow':\n",
        "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
        "        elif variant.lower() == 'skipgram':\n",
        "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        vectorized_sent = self.vectorize(self.sents[index])\n",
        "        \n",
        "        return list(self._iterator(vectorized_sent))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]\n",
        "    \n",
        "    def cbow_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1\n",
        "        for window in per_window(tokens, n):\n",
        "            target = window.pop(window_size)\n",
        "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
        "            \n",
        "    def skipgram_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1 \n",
        "        for i, window in enumerate(per_window(tokens, n)):\n",
        "            focus = window.pop(window_size)\n",
        "            # Generate positive samples.\n",
        "            for context_word in window:\n",
        "                yield {'x': (focus, context_word), 'y':1}\n",
        "            # Generate negative samples.\n",
        "            for _ in range(n-1):\n",
        "                leftovers = tokens[:i] + tokens[i+n:]\n",
        "                if leftovers:\n",
        "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OSsvhCwYWz1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5\"></a>\n",
        "\n",
        "# Lets try the skipgram task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRCr4fPYSKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "    \n",
        "    def forward(self, focus, context):\n",
        "        embed_focus = self.embeddings(focus).view((1, -1))\n",
        "        embed_context = self.embeddings(context).view((1, -1))\n",
        "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
        "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
        "        log_probs = F.logsigmoid(score)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRlP8wMGYiDV",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5-foward\"></a>\n",
        "\n",
        "# Take a closer look at what's in the `forward()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuv-pGGzYeGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx1 = torch.rand(1,20)\n",
        "xx2 = torch.rand(1,20)\n",
        "\n",
        "xx1_numpy = xx1.detach().numpy()\n",
        "xx2_numpy = xx2.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwgV35oQYkS6",
        "colab_type": "code",
        "outputId": "3fe2dd05-36bc-4110-b403-9d507f882c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(xx1_numpy.shape)\n",
        "print(xx2_numpy.T.shape)\n",
        "print(np.dot(xx1_numpy, xx2_numpy.T))\n",
        "\n",
        "print(xx1.shape)\n",
        "print(torch.t(xx2).shape) \n",
        "print(torch.mm(xx1, torch.t(xx2)))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 20)\n",
            "(20, 1)\n",
            "[[7.6248436]]\n",
            "torch.Size([1, 20])\n",
            "torch.Size([20, 1])\n",
            "tensor([[7.6248]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68J7DuXcYz38",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5-train\"></a>\n",
        "\n",
        "# Train a Skipgram model (for real)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0CXh7nccJbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Utility code to monitor the GPU usage\n",
        "import subprocess\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-XLjsGiYqBl",
        "colab_type": "code",
        "outputId": "d60f5c6e-b078-4edd-ed94-d4609b34dea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embd_size = 100\n",
        "learning_rate = 0.03\n",
        "hidden_size = 30 #To reduce training time changing 300 to 30\n",
        "window_size = 3\n",
        "\n",
        "# Initialize the dataset.\n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=3, variant='skipgram')\n",
        "vocab_size = len(w2v_dataset.vocab)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Use the Skipgram object\n",
        "model = SkipGram(vocab_size, embd_size,).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "\n",
        "#model = nn.DataParallel(model) #You can easily run your operations on multiple GPUs by making your model run parallelly using DataParallel\n",
        "#refer - https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\n",
        "num_epochs = 10\n",
        "for _e in tqdm(range(num_epochs)):\n",
        "    epcoh_loss = 0\n",
        "    for sent_idx in range(w2v_dataset._len):\n",
        "        for w2v_io in w2v_dataset[sent_idx]:\n",
        "            # Retrieve the inputs and outputs.\n",
        "            x1, x2 = w2v_io['x']\n",
        "            x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
        "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float)).to(device)\n",
        "            # Zero gradient.\n",
        "            model.zero_grad()\n",
        "            # Calculate the log probability of the context embeddings.\n",
        "            logprobs = model(x1, x2)\n",
        "            # This unsqueeze thing is really a feature/bug... -_-\n",
        "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epcoh_loss += float(loss)\n",
        "    #print('GPU Memory ->', torch.cuda.get_device_properties(device).total_memory)\n",
        "    #print('GPU Space ->', get_gpu_memory_map())\n",
        "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
        "    losses.append(epcoh_loss)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 1/10 [00:36<05:27, 36.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 2/10 [01:12<04:51, 36.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 3/10 [01:50<04:16, 36.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 4/10 [02:26<03:39, 36.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 5/10 [03:02<03:02, 36.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 6/10 [03:38<02:25, 36.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 7/10 [04:15<01:49, 36.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 8/10 [04:51<01:12, 36.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 9/10 [05:27<00:36, 36.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 10/10 [06:03<00:00, 36.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSyLhD6cxKD",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5-evaluate\"></a>\n",
        "\n",
        "# Evaluate the model on the skipgram task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdsmNseFa5HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        model.zero_grad()\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x1, x2 = w2v_io['x']\n",
        "        x1, x2 = tensor(x1).to(device), tensor(x2).to(device) #make sure you are running on GPU or CPU.\n",
        "        y = w2v_io['y']\n",
        "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
        "        true_positive += int(prediction) == int(y)\n",
        "        all_data += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnioy9HCZ6dc",
        "colab_type": "code",
        "outputId": "f0e659f1-25a3-4da6-eecd-205501df043b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-qp6WZrB92",
        "colab_type": "text"
      },
      "source": [
        "#Use Pre-trained Embeddings for common NLP Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qikdZy9urJeG",
        "colab_type": "text"
      },
      "source": [
        "# Data & Environment setup.\n",
        "## Download the Collobert and Weston SENNA Embeddings\n",
        "\n",
        "\n",
        "If you're on a Mac or Linux, you can use the `!` bang commands in the next cell to get the data.\n",
        "\n",
        "```\n",
        "!pip install kaggle\n",
        "!mkdir -p .kaggle\n",
        "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
        "!chmod 600 .kaggle/kaggle.json\n",
        "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n",
        "```\n",
        "\n",
        "If you're on windows go to https://www.kaggle.com/alvations/vegetables-senna-embeddings and download the data files. \n",
        "\n",
        "What's most important are the \n",
        " - `.txt` file that contains the vocabulary list\n",
        " - `.npy` file that contains the binarized numpy array\n",
        " \n",
        "The rows of the numpy array corresponds to the vocabulary in the order from the `.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tck7ypIxdHkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "4f3dcca4-09d2-4adf-affa-45012c598863"
      },
      "source": [
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"natgillin\"\n",
        "os.environ['KAGGLE_KEY'] = \"54ae95ab760b52c3307ed4645c6c9b5d\"\n",
        "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Downloading vegetables-senna-embeddings.zip to .\n",
            "100% 129M/130M [00:03<00:00, 43.8MB/s]\n",
            "100% 130M/130M [00:03<00:00, 40.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hfj_ReYrNi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6ed10ece-e459-4242-d1a5-64e5426fba42"
      },
      "source": [
        "!unzip vegetables-senna-embeddings.zip"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  vegetables-senna-embeddings.zip\n",
            "  inflating: senna-embeddings/senna.wiki-reuters.lm2.50d.npy  \n",
            "  inflating: senna-embeddings/senna.wiki-reuters.lm2.50d.pkl  \n",
            "  inflating: senna-embeddings/senna.wiki-reuters.lm2.50d.tsv  \n",
            "  inflating: senna-embeddings/senna.wiki-reuters.lm2.50d.txt  \n",
            "  inflating: senna.wiki-reuters.lm2.50d.npy  \n",
            "  inflating: senna.wiki-reuters.lm2.50d.pkl  \n",
            "  inflating: senna.wiki-reuters.lm2.50d.tsv  \n",
            "  inflating: senna.wiki-reuters.lm2.50d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jmfbhfrQpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads the pretrained keys. \n",
        "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
        "  pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
        "vocab = Dictionary({})\n",
        "vocab.token2id = pretrained_keys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvrF2oF7rUYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecText(Dataset):\n",
        "    def __init__(self, tokenized_texts, window_size, variant):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self._len = len(self.sents)\n",
        "        \n",
        "        # Loads the pretrained keys. \n",
        "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
        "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
        "        self.vocab = Dictionary({})\n",
        "        self.vocab.token2id = pretrained_keys\n",
        "        \n",
        "        self.window_size = window_size\n",
        "        self.variant = variant\n",
        "        if variant.lower() == 'cbow':\n",
        "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
        "        elif variant.lower() == 'skipgram':\n",
        "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        vectorized_sent = self.vectorize(self.sents[index])\n",
        "        \n",
        "        return list(self._iterator(vectorized_sent))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]\n",
        "    \n",
        "    def cbow_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1\n",
        "        for window in per_window(tokens, n):\n",
        "            target = window.pop(window_size)\n",
        "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
        "            \n",
        "    def skipgram_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1 \n",
        "        for i, window in enumerate(per_window(tokens, n)):\n",
        "            focus = window.pop(window_size)\n",
        "            # Generate positive samples.\n",
        "            for context_word in window:\n",
        "                yield {'x': (focus, context_word), 'y':1}\n",
        "            # Generate negative samples.\n",
        "            for _ in range(n-1):\n",
        "                leftovers = tokens[:i] + tokens[i+n:]\n",
        "                if leftovers:\n",
        "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtbd9SibtQuG",
        "colab_type": "text"
      },
      "source": [
        "## Override the embeddings layer with the pre-trained weights.\n",
        "\n",
        "In PyTorch, the weights of the `nn.Embedding` object can be easily overwritten with `from_pretrained` function, see https://pytorch.org/docs/stable/nn.html#embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw-UeB_LtHpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, pretrained_npy):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
        "    \n",
        "    def forward(self, focus, context):\n",
        "        # Put the index of the focus word into the embedding layer.\n",
        "        embed_focus = self.embeddings(focus).view((1, -1))\n",
        "        # Put the index of the context word into the embedding layer.\n",
        "        embed_context = self.embeddings(context).view((1, -1))\n",
        "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
        "        # Do a matrix multiplication between the focus and context embedding\n",
        "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
        "        # Then put it through a log sigmoid activation function\n",
        "        # so that the output is between (log(0), log(1))\n",
        "        log_probs = F.logsigmoid(score)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg233UULtU1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c1f5b574-4188-4b4c-a66c-57e2a41c0958"
      },
      "source": [
        "np.load('senna.wiki-reuters.lm2.50d.npy')"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.03682 ,  1.77856 , -0.693547, ..., -0.10278 , -0.36428 ,\n",
              "        -0.64853 ],\n",
              "       [-2.19067 ,  1.16642 , -1.91385 , ...,  0.870654, -0.33808 ,\n",
              "        -0.41957 ],\n",
              "       [ 1.16672 ,  0.811884, -0.115492, ..., -0.104843,  2.26862 ,\n",
              "         1.21729 ],\n",
              "       ...,\n",
              "       [-0.483488,  2.00359 ,  0.186266, ..., -0.114528,  1.50755 ,\n",
              "        -1.25606 ],\n",
              "       [ 0.201604,  1.15796 ,  0.888882, ..., -1.28183 ,  0.465847,\n",
              "        -1.57974 ],\n",
              "       [-0.238824,  0.443876,  0.290836, ..., -0.802705, -0.318169,\n",
              "        -1.4733  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Sxb6SktYW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
        "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
        "pretrained_model = SkipGram(pretrained_npy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX3SRvAotjf9",
        "colab_type": "text"
      },
      "source": [
        "## Test Pretrained Embeddings on the Skipgram Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhgQGnjtgYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        pretrained_model.zero_grad()\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x1, x2 = w2v_io['x']\n",
        "        if -1 in (x1, x2): # Skip unknown words.\n",
        "            continue\n",
        "        x1, x2 = tensor(x1), tensor(x2)\n",
        "        y = w2v_io['y']\n",
        "        with torch.no_grad():\n",
        "            logprobs = pretrained_model(x1, x2)\n",
        "            _, prediction =  torch.max(logprobs, 1)    \n",
        "        true_positive += int(prediction) == int(y)\n",
        "        all_data += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpmM5fytnAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
        "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbqaK6czuMGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f4b1279-5294-4636-9072-dde62c06693a"
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlo5pVhnuWou",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-6-eval-cbow\"></a>\n",
        "## Test Pretrained Embeddings on the CBOW Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phH-JlqWuOCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7PEvaYTuZG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 5\n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
        "hidden_size = 300\n",
        "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEKT9Ik1ubvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef1d46dc-3890-4e25-97b9-d6dd5bd280f1"
      },
      "source": [
        "print(device)\n",
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x'])#.to(device)\n",
        "        y = tensor(w2v_io['y'])#.to(device)\n",
        "        \n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "        with torch.no_grad():\n",
        "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "\u001b[92m:\u001b[0m \t\t the problem is essentially this \u001b[91malert\u001b[0m if a word ( or\n",
            "\u001b[92mre-\u001b[0m \t\t the web is a vast \u001b[91mpioneers\u001b[0m source for many languages .\n",
            "\u001b[92mrandom\u001b[0m \t\t is that the association is \u001b[91meffeminate\u001b[0m , arbitrary , motivated or\n",
            "\u001b[92m,\u001b[0m \t\t that the association is random \u001b[91mreclassifications\u001b[0m arbitrary , motivated or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t arbitrary , motivated or pre- \u001b[91mugarte\u001b[0m ( r , a ,\n",
            "\u001b[92minevitably\u001b[0m \t however , their methods are \u001b[91mpriok\u001b[0m noisy , suffering , for\n",
            "\u001b[92mnoisy\u001b[0m \t\t , their methods are inevitably \u001b[91mcolumba\u001b[0m , suffering , for example\n",
            "\u001b[92m,\u001b[0m \t\t their methods are inevitably noisy \u001b[91mmoura\u001b[0m suffering , for example ,\n",
            "\u001b[92msuffering\u001b[0m \t methods are inevitably noisy , \u001b[91mlactic\u001b[0m , for example , from\n",
            "\u001b[92m,\u001b[0m \t\t are inevitably noisy , suffering \u001b[91mretailers\u001b[0m for example , from just\n",
            "\u001b[92mfor\u001b[0m \t\t inevitably noisy , suffering , \u001b[91mpalma\u001b[0m example , from just those\n",
            "\u001b[92mexample\u001b[0m \t noisy , suffering , for \u001b[91mmoldy\u001b[0m , from just those parser\n",
            "\u001b[92m,\u001b[0m \t\t , suffering , for example \u001b[91mcamarilla\u001b[0m from just those parser errors\n",
            "\u001b[92mfrom\u001b[0m \t\t suffering , for example , \u001b[91minductively\u001b[0m just those parser errors that\n",
            "\u001b[92mjust\u001b[0m \t\t , for example , from \u001b[91mtrion\u001b[0m those parser errors that the\n",
            "\u001b[92mthose\u001b[0m \t\t for example , from just \u001b[91mjars\u001b[0m parser errors that the whole\n",
            "\u001b[92mparser\u001b[0m \t\t example , from just those \u001b[91mbenchmarks\u001b[0m errors that the whole process\n",
            "\u001b[92merrors\u001b[0m \t\t , from just those parser \u001b[91mrosicrucians\u001b[0m that the whole process is\n",
            "\u001b[92mthat\u001b[0m \t\t from just those parser errors \u001b[91mnutritious\u001b[0m the whole process is designed\n",
            "\u001b[92mthe\u001b[0m \t\t just those parser errors that \u001b[91mmoldy\u001b[0m whole process is designed to\n",
            "\u001b[92mwhole\u001b[0m \t\t those parser errors that the \u001b[91mworkbook\u001b[0m process is designed to address\n",
            "\u001b[92mprocess\u001b[0m \t parser errors that the whole \u001b[91mmidtjylland\u001b[0m is designed to address ,\n",
            "\u001b[92mis\u001b[0m \t\t errors that the whole process \u001b[91majah\u001b[0m designed to address , and\n",
            "\u001b[92mdesigned\u001b[0m \t that the whole process is \u001b[91marista\u001b[0m to address , and they\n",
            "\u001b[92mto\u001b[0m \t\t the whole process is designed \u001b[91mexcavated\u001b[0m address , and they do\n",
            "\u001b[92maddress\u001b[0m \t whole process is designed to \u001b[91mstepson\u001b[0m , and they do not\n",
            "\u001b[92m,\u001b[0m \t\t process is designed to address \u001b[91meking\u001b[0m and they do not wish\n",
            "\u001b[92mand\u001b[0m \t\t is designed to address , \u001b[91mrohilla\u001b[0m they do not wish to\n",
            "\u001b[92mthey\u001b[0m \t\t designed to address , and \u001b[91mkaito\u001b[0m do not wish to accept\n",
            "\u001b[92mdo\u001b[0m \t\t to address , and they \u001b[91misraeli-born\u001b[0m not wish to accept any\n",
            "\u001b[92mnot\u001b[0m \t\t address , and they do \u001b[91mhear\u001b[0m wish to accept any scf\n",
            "\u001b[92mwish\u001b[0m \t\t , and they do not \u001b[91mp/e\u001b[0m to accept any scf for\n",
            "\u001b[92mto\u001b[0m \t\t and they do not wish \u001b[91mbritannica\u001b[0m accept any scf for which\n",
            "\u001b[92maccept\u001b[0m \t\t they do not wish to \u001b[91mamines\u001b[0m any scf for which there\n",
            "\u001b[92many\u001b[0m \t\t do not wish to accept \u001b[91mnorth/south\u001b[0m scf for which there is\n",
            "\u001b[92mscf\u001b[0m \t\t not wish to accept any \u001b[91mwyss\u001b[0m for which there is any\n",
            "\u001b[92mfor\u001b[0m \t\t wish to accept any scf \u001b[91mwinterkill\u001b[0m which there is any evidence\n",
            "\u001b[92mwhich\u001b[0m \t\t to accept any scf for \u001b[91misraeli-born\u001b[0m there is any evidence as\n",
            "\u001b[92mthere\u001b[0m \t\t accept any scf for which \u001b[91mchamplain\u001b[0m is any evidence as a\n",
            "\u001b[92mis\u001b[0m \t\t any scf for which there \u001b[91mladenburg\u001b[0m any evidence as a true\n",
            "\u001b[92many\u001b[0m \t\t scf for which there is \u001b[91m*note\u001b[0m evidence as a true scf\n",
            "\u001b[92mevidence\u001b[0m \t for which there is any \u001b[91mhananiah\u001b[0m as a true scf for\n",
            "\u001b[92mas\u001b[0m \t\t which there is any evidence \u001b[91mportraitist\u001b[0m a true scf for the\n",
            "\u001b[92ma\u001b[0m \t\t there is any evidence as \u001b[91msworn\u001b[0m true scf for the verb\n",
            "\u001b[92mtrue\u001b[0m \t\t is any evidence as a \u001b[91mmonth-to-date\u001b[0m scf for the verb .\n",
            "\u001b[92mthat\u001b[0m \t\t while it might seem plausible \u001b[91mmisadventures\u001b[0m oddities would in some way\n",
            "\u001b[92moddities\u001b[0m \t it might seem plausible that \u001b[91mletsie\u001b[0m would in some way balance\n",
            "\u001b[92mwould\u001b[0m \t\t might seem plausible that oddities \u001b[91mfreienwalde\u001b[0m in some way balance out\n",
            "\u001b[92min\u001b[0m \t\t seem plausible that oddities would \u001b[91msustains\u001b[0m some way balance out to\n",
            "\u001b[92msome\u001b[0m \t\t plausible that oddities would in \u001b[91mkickers\u001b[0m way balance out to give\n",
            "\u001b[92mway\u001b[0m \t\t that oddities would in some \u001b[91mprotrude\u001b[0m balance out to give a\n",
            "\u001b[92m<unk>\u001b[0m \t\t balance out to give a \u001b[91maddict\u001b[0m tion that was indistinguishable from\n",
            "\u001b[92mone\u001b[0m \t\t tion that was indistinguishable from \u001b[91mcrimp\u001b[0m where the individual words (\n",
            "\u001b[92mwhere\u001b[0m \t\t that was indistinguishable from one \u001b[91mucda\u001b[0m the individual words ( as\n",
            "\u001b[92mthe\u001b[0m \t\t was indistinguishable from one where \u001b[91mcongo-zaire\u001b[0m individual words ( as opposed\n",
            "\u001b[92mindividual\u001b[0m \t indistinguishable from one where the \u001b[91malmanack\u001b[0m words ( as opposed to\n",
            "\u001b[92mwords\u001b[0m \t\t from one where the individual \u001b[91mscheduling\u001b[0m ( as opposed to the\n",
            "\u001b[92m(\u001b[0m \t\t one where the individual words \u001b[91mmudra\u001b[0m as opposed to the texts\n",
            "\u001b[92mas\u001b[0m \t\t where the individual words ( \u001b[91mo.\u001b[0m opposed to the texts )\n",
            "\u001b[92mopposed\u001b[0m \t the individual words ( as \u001b[91mhunting\u001b[0m to the texts ) had\n",
            "\u001b[92mto\u001b[0m \t\t individual words ( as opposed \u001b[91mlowen\u001b[0m the texts ) had been\n",
            "\u001b[92mthe\u001b[0m \t\t words ( as opposed to \u001b[91ma00\u001b[0m texts ) had been randomly\n",
            "\u001b[92mtexts\u001b[0m \t\t ( as opposed to the \u001b[91mchigi\u001b[0m ) had been randomly selected\n",
            "\u001b[92m)\u001b[0m \t\t as opposed to the texts \u001b[91mmeddling\u001b[0m had been randomly selected ,\n",
            "\u001b[92mhad\u001b[0m \t\t opposed to the texts ) \u001b[91mpoorest\u001b[0m been randomly selected , this\n",
            "\u001b[92mbeen\u001b[0m \t\t to the texts ) had \u001b[91mcore\u001b[0m randomly selected , this turns\n",
            "\u001b[92mrandomly\u001b[0m \t the texts ) had been \u001b[91meloy\u001b[0m selected , this turns out\n",
            "\u001b[92mselected\u001b[0m \t texts ) had been randomly \u001b[91mverona\u001b[0m , this turns out not\n",
            "\u001b[92m,\u001b[0m \t\t ) had been randomly selected \u001b[91mabadi\u001b[0m this turns out not to\n",
            "\u001b[92mthis\u001b[0m \t\t had been randomly selected , \u001b[91munifying\u001b[0m turns out not to be\n",
            "\u001b[92mturns\u001b[0m \t\t been randomly selected , this \u001b[91msepals\u001b[0m out not to be the\n",
            "\u001b[92mout\u001b[0m \t\t randomly selected , this turns \u001b[91mchampaign\u001b[0m not to be the case\n",
            "\u001b[92mnot\u001b[0m \t\t selected , this turns out \u001b[91mmidtjylland\u001b[0m to be the case .\n",
            "\u001b[92mvaries\u001b[0m \t\t however where the sample size \u001b[91mgerstner\u001b[0m by an order of magnitude\n",
            "\u001b[92mby\u001b[0m \t\t where the sample size varies \u001b[91mdickie\u001b[0m an order of magnitude ,\n",
            "\u001b[92man\u001b[0m \t\t the sample size varies by \u001b[91mvalentino\u001b[0m order of magnitude , or\n",
            "\u001b[92morder\u001b[0m \t\t sample size varies by an \u001b[91mrationalistic\u001b[0m of magnitude , or where\n",
            "\u001b[92mof\u001b[0m \t\t size varies by an order \u001b[91mhanko\u001b[0m magnitude , or where it\n",
            "\u001b[92mmagnitude\u001b[0m \t varies by an order of \u001b[91mbiwako\u001b[0m , or where it is\n",
            "\u001b[92m,\u001b[0m \t\t by an order of magnitude \u001b[91minter-african\u001b[0m or where it is enormous\n",
            "\u001b[92mor\u001b[0m \t\t an order of magnitude , \u001b[91mcloverdale\u001b[0m where it is enormous ,\n",
            "\u001b[92mwhere\u001b[0m \t\t order of magnitude , or \u001b[91mworkbook\u001b[0m it is enormous , it\n",
            "\u001b[92mit\u001b[0m \t\t of magnitude , or where \u001b[91mestb\u001b[0m is enormous , it is\n",
            "\u001b[92mis\u001b[0m \t\t magnitude , or where it \u001b[91mruth\u001b[0m enormous , it is wrong\n",
            "\u001b[92menormous\u001b[0m \t , or where it is \u001b[91molathe\u001b[0m , it is wrong to\n",
            "\u001b[92m,\u001b[0m \t\t or where it is enormous \u001b[91msusanti\u001b[0m it is wrong to identify\n",
            "\u001b[92mit\u001b[0m \t\t where it is enormous , \u001b[91mvosper\u001b[0m is wrong to identify the\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of the conference of \u001b[91mrelocations\u001b[0m south-central sas users group ,\n",
            "\u001b[92man\u001b[0m \t\t making false assumptions is often \u001b[91mfelicitas\u001b[0m ingenious way to proceed ;\n",
            "\u001b[92mingenious\u001b[0m \t false assumptions is often an \u001b[91msplenic\u001b[0m way to proceed ; the\n",
            "\u001b[92mway\u001b[0m \t\t assumptions is often an ingenious \u001b[91mfleet\u001b[0m to proceed ; the problem\n",
            "\u001b[92mto\u001b[0m \t\t is often an ingenious way \u001b[91msilke\u001b[0m proceed ; the problem arises\n",
            "\u001b[92mproceed\u001b[0m \t often an ingenious way to \u001b[91mecma\u001b[0m ; the problem arises where\n",
            "\u001b[92m;\u001b[0m \t\t an ingenious way to proceed \u001b[91mpianoforte\u001b[0m the problem arises where the\n",
            "\u001b[92mthe\u001b[0m \t\t ingenious way to proceed ; \u001b[91maviemore\u001b[0m problem arises where the literal\n",
            "\u001b[92mproblem\u001b[0m \t way to proceed ; the \u001b[91mcore\u001b[0m arises where the literal falsity\n",
            "\u001b[92marises\u001b[0m \t\t to proceed ; the problem \u001b[91midlc\u001b[0m where the literal falsity of\n",
            "\u001b[92mwhere\u001b[0m \t\t proceed ; the problem arises \u001b[91mmudra\u001b[0m the literal falsity of the\n",
            "\u001b[92mthe\u001b[0m \t\t ; the problem arises where \u001b[91mprefect\u001b[0m literal falsity of the assumption\n",
            "\u001b[92mliteral\u001b[0m \t the problem arises where the \u001b[91mpatric\u001b[0m falsity of the assumption is\n",
            "\u001b[92mfalsity\u001b[0m \t problem arises where the literal \u001b[91moestfold\u001b[0m of the assumption is overlooked\n",
            "\u001b[92mof\u001b[0m \t\t arises where the literal falsity \u001b[91mpriok\u001b[0m the assumption is overlooked ,\n",
            "\u001b[92mthe\u001b[0m \t\t where the literal falsity of \u001b[91msocialista\u001b[0m assumption is overlooked , and\n",
            "\u001b[92m<unk>\u001b[0m \t\t assumption is overlooked , and \u001b[91mmoldy\u001b[0m ate inferences are drawn .\n",
            "\u001b[92m<unk>\u001b[0m \t\t when we look at linguistic \u001b[91mtaxiway\u001b[0m ena in corpora , the\n",
            "\u001b[92mnull\u001b[0m \t\t ena in corpora , the \u001b[91mjac\u001b[0m hypothesis will never be true\n",
            "\u001b[92mhypothesis\u001b[0m \t in corpora , the null \u001b[91msailing\u001b[0m will never be true .\n",
            "\u001b[92menough\u001b[0m \t\t we do not always have \u001b[91mriparian\u001b[0m data to reject the null\n",
            "\u001b[92mdata\u001b[0m \t\t do not always have enough \u001b[91mmilitarisation\u001b[0m to reject the null hypothesis\n",
            "\u001b[92mto\u001b[0m \t\t not always have enough data \u001b[91mbldg.\u001b[0m reject the null hypothesis ,\n",
            "\u001b[92mreject\u001b[0m \t\t always have enough data to \u001b[91mforecast-polish\u001b[0m the null hypothesis , but\n",
            "\u001b[92mthe\u001b[0m \t\t have enough data to reject \u001b[91mcanaanite\u001b[0m null hypothesis , but that\n",
            "\u001b[92mnull\u001b[0m \t\t enough data to reject the \u001b[91msuckling\u001b[0m hypothesis , but that is\n",
            "\u001b[92mhypothesis\u001b[0m \t data to reject the null \u001b[91mtoyotomi\u001b[0m , but that is a\n",
            "\u001b[92m,\u001b[0m \t\t to reject the null hypothesis \u001b[91mmetalcore\u001b[0m but that is a distinct\n",
            "\u001b[92mbut\u001b[0m \t\t reject the null hypothesis , \u001b[91mmanfed\u001b[0m that is a distinct issue\n",
            "\u001b[92mthat\u001b[0m \t\t the null hypothesis , but \u001b[91mguts\u001b[0m is a distinct issue :\n",
            "\u001b[92mis\u001b[0m \t\t null hypothesis , but that \u001b[91mapply\u001b[0m a distinct issue : wherever\n",
            "\u001b[92ma\u001b[0m \t\t hypothesis , but that is \u001b[91mo.\u001b[0m distinct issue : wherever there\n",
            "\u001b[92mdistinct\u001b[0m \t , but that is a \u001b[91mhighline\u001b[0m issue : wherever there is\n",
            "\u001b[92missue\u001b[0m \t\t but that is a distinct \u001b[91mmanipulates\u001b[0m : wherever there is enough\n",
            "\u001b[92m:\u001b[0m \t\t that is a distinct issue \u001b[91mnonfat\u001b[0m wherever there is enough data\n",
            "\u001b[92mwherever\u001b[0m \t is a distinct issue : \u001b[91mback-to-school\u001b[0m there is enough data ,\n",
            "\u001b[92mthere\u001b[0m \t\t a distinct issue : wherever \u001b[91msupervises\u001b[0m is enough data , it\n",
            "\u001b[92mis\u001b[0m \t\t distinct issue : wherever there \u001b[91msuckling\u001b[0m enough data , it is\n",
            "\u001b[92menough\u001b[0m \t\t issue : wherever there is \u001b[91m'round\u001b[0m data , it is rejected\n",
            "\u001b[92mdata\u001b[0m \t\t : wherever there is enough \u001b[91mshearson\u001b[0m , it is rejected .\n",
            "\u001b[92mare\u001b[0m \t\t since words in a text \u001b[91mpyrophoric\u001b[0m not random , we know\n",
            "\u001b[92mnot\u001b[0m \t\t words in a text are \u001b[91mlil\u001b[0m random , we know that\n",
            "\u001b[92mrandom\u001b[0m \t\t in a text are not \u001b[91mmarigny\u001b[0m , we know that our\n",
            "\u001b[92m,\u001b[0m \t\t a text are not random \u001b[91mnodule\u001b[0m we know that our corpora\n",
            "\u001b[92mwe\u001b[0m \t\t text are not random , \u001b[91mpython\u001b[0m know that our corpora are\n",
            "\u001b[92mknow\u001b[0m \t\t are not random , we \u001b[91mmidtjylland\u001b[0m that our corpora are not\n",
            "\u001b[92mthat\u001b[0m \t\t not random , we know \u001b[91mcomte\u001b[0m our corpora are not randomly\n",
            "\u001b[92mour\u001b[0m \t\t random , we know that \u001b[91moutta\u001b[0m corpora are not randomly generated\n",
            "\u001b[92mcorpora\u001b[0m \t , we know that our \u001b[91mmidtjylland\u001b[0m are not randomly generated ,\n",
            "\u001b[92mare\u001b[0m \t\t we know that our corpora \u001b[91murgently\u001b[0m not randomly generated , and\n",
            "\u001b[92mnot\u001b[0m \t\t know that our corpora are \u001b[91mholywood\u001b[0m randomly generated , and the\n",
            "\u001b[92mrandomly\u001b[0m \t that our corpora are not \u001b[91mgraphemes\u001b[0m generated , and the hypothesis\n",
            "\u001b[92mgenerated\u001b[0m \t our corpora are not randomly \u001b[91mthinker\u001b[0m , and the hypothesis test\n",
            "\u001b[92mthe\u001b[0m \t\t gives us reason to view \u001b[91mshoesource\u001b[0m relation between , for example\n",
            "\u001b[92mrelation\u001b[0m \t us reason to view the \u001b[91mvosper\u001b[0m between , for example ,\n",
            "\u001b[92mbetween\u001b[0m \t reason to view the relation \u001b[91mtarullo\u001b[0m , for example , a\n",
            "\u001b[92m,\u001b[0m \t\t to view the relation between \u001b[91mkummer\u001b[0m for example , a verb\n",
            "\u001b[92m<unk>\u001b[0m \t\t for example , a verb \u001b[91mkavaje\u001b[0m s syntax and its semantics\n",
            "\u001b[92m,\u001b[0m \t\t s syntax and its semantics \u001b[91msenan\u001b[0m as motivated rather than arbitrary\n",
            "\u001b[92mas\u001b[0m \t\t syntax and its semantics , \u001b[91mplagiarism\u001b[0m motivated rather than arbitrary .\n",
            "\u001b[92merror\u001b[0m \t\t the average value of the \u001b[91mmembranes\u001b[0m term , language is never\n",
            "\u001b[92mterm\u001b[0m \t\t average value of the error \u001b[91mporky\u001b[0m , language is never ,\n",
            "\u001b[92m,\u001b[0m \t\t value of the error term \u001b[91municron\u001b[0m language is never , ever\n",
            "\u001b[92mlanguage\u001b[0m \t of the error term , \u001b[91msailing\u001b[0m is never , ever ,\n",
            "\u001b[92mis\u001b[0m \t\t the error term , language \u001b[91mmudra\u001b[0m never , ever , ever\n",
            "\u001b[92mnever\u001b[0m \t\t error term , language is \u001b[91maccredit\u001b[0m , ever , ever ,\n",
            "\u001b[92m,\u001b[0m \t\t term , language is never \u001b[91mjefferson-pilot\u001b[0m ever , ever , random\n",
            "\u001b[92m,\u001b[0m \t\t the hypothesis can , therefore \u001b[91msuckling\u001b[0m be couched as : are\n",
            "\u001b[92mbe\u001b[0m \t\t hypothesis can , therefore , \u001b[91mconifer\u001b[0m couched as : are the\n",
            "\u001b[92mcouched\u001b[0m \t can , therefore , be \u001b[91mmoldy\u001b[0m as : are the error\n",
            "\u001b[92mas\u001b[0m \t\t , therefore , be couched \u001b[91marista\u001b[0m : are the error terms\n",
            "\u001b[92m:\u001b[0m \t\t therefore , be couched as \u001b[91mfugu\u001b[0m are the error terms systematically\n",
            "\u001b[92mare\u001b[0m \t\t , be couched as : \u001b[91mvnc\u001b[0m the error terms systematically greater\n",
            "\u001b[92mthe\u001b[0m \t\t be couched as : are \u001b[91mhamlyn\u001b[0m error terms systematically greater than\n",
            "\u001b[92mbecomes\u001b[0m \t % of them , devastate \u001b[91mbaustoffindustrie\u001b[0m one of the verbs for\n",
            "\u001b[92mone\u001b[0m \t\t of them , devastate becomes \u001b[91msupervises\u001b[0m of the verbs for which\n",
            "\u001b[92mof\u001b[0m \t\t them , devastate becomes one \u001b[91mlalabalavu\u001b[0m the verbs for which we\n",
            "\u001b[92mthe\u001b[0m \t\t , devastate becomes one of \u001b[91mchigi\u001b[0m verbs for which we have\n",
            "\u001b[92mverbs\u001b[0m \t\t devastate becomes one of the \u001b[91m...\u001b[0m for which we have plenty\n",
            "\u001b[92mfor\u001b[0m \t\t becomes one of the verbs \u001b[91maccordionist\u001b[0m which we have plenty of\n",
            "\u001b[92mwhich\u001b[0m \t\t one of the verbs for \u001b[91mchrono\u001b[0m we have plenty of data\n",
            "\u001b[92mwe\u001b[0m \t\t of the verbs for which \u001b[91mbiopharmaceutical\u001b[0m have plenty of data ,\n",
            "\u001b[92mhave\u001b[0m \t\t the verbs for which we \u001b[91mcaperton\u001b[0m plenty of data , and\n",
            "\u001b[92mplenty\u001b[0m \t\t verbs for which we have \u001b[91mintl0\u001b[0m of data , and crude\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7kuoSZtue29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b9987f3-d711-4421-a8d1-72590957196f"
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf6536RiuvU-",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-6-unfreeze-finetune\"></a>\n",
        "## Unfreeze the Embedddings and Tune it on the CBOW Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBXLt0ayuqo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
        "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\n",
        "        # Note the `freeze=False`, by default if you use `nn.Embedding.from_pretrained(),\n",
        "        # `freeze` is set to True\n",
        "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, freeze=False)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Ppz5lxux31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 2 \n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
        "hidden_size = 300\n",
        "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z-qhGlUu2qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e0c5861-be48-46f1-9e36-3d473cebb140"
      },
      "source": [
        "learning_rate = 0.003\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "\n",
        "model = nn.DataParallel(pretrained_cbow_model)\n",
        "\n",
        "num_epochs = 100\n",
        "for _e in tqdm(range(num_epochs)):\n",
        "    epoch_loss = []\n",
        "    for sent_idx in range(w2v_dataset._len):\n",
        "        for w2v_io in w2v_dataset[sent_idx]:\n",
        "            # Retrieve the inputs and outputs.\n",
        "            x = tensor(w2v_io['x']).to(device)\n",
        "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long)).to(device)\n",
        "            \n",
        "            if -1 in x or int(y) == -1:\n",
        "                continue\n",
        "            # Zero gradient.\n",
        "            model.zero_grad()\n",
        "            # Calculate the log probability of the context embeddings.\n",
        "            logprobs = pretrained_cbow_model(x)\n",
        "            # This unsqueeze thing is really a feature/bug... -_-\n",
        "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss.append(float(loss))\n",
        "    # Save model after every epoch.\n",
        "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
        "    losses.append(sum(epoch_loss)/len(epoch_loss))"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 1/100 [00:26<43:02, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 2/100 [00:52<42:36, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 3/100 [01:18<42:09, 26.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 4/100 [01:44<41:42, 26.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 5/100 [02:10<41:15, 26.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 6/100 [02:36<40:48, 26.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 7/100 [03:02<40:21, 26.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 8/100 [03:28<39:55, 26.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 9/100 [03:54<39:29, 26.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 10/100 [04:20<39:02, 26.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 11/100 [04:46<38:39, 26.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 12/100 [05:12<38:14, 26.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 13/100 [05:38<37:48, 26.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 14/100 [06:04<37:22, 26.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 15/100 [06:30<36:57, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 16/100 [06:57<36:31, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 17/100 [07:23<36:05, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 18/100 [07:49<35:39, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 19/100 [08:15<35:15, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 20/100 [08:41<34:47, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 21/100 [09:07<34:20, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 22/100 [09:33<33:54, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 23/100 [09:59<33:28, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 24/100 [10:25<33:02, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 25/100 [10:51<32:37, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 26/100 [11:18<32:11, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 27/100 [11:44<31:45, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 28/100 [12:10<31:23, 26.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 29/100 [12:36<30:55, 26.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 30/100 [13:02<30:28, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 31/100 [13:28<30:01, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 32/100 [13:54<29:35, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 33/100 [14:20<29:08, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 34/100 [14:46<28:42, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 35/100 [15:13<28:16, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 36/100 [15:39<27:50, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 37/100 [16:05<27:24, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 38/100 [16:31<26:57, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 39/100 [16:57<26:31, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 40/100 [17:23<26:04, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 41/100 [17:49<25:38, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 42/100 [18:15<25:12, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 43/100 [18:41<24:46, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 44/100 [19:07<24:21, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 45/100 [19:34<23:58, 26.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 46/100 [20:00<23:32, 26.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 47/100 [20:26<23:05, 26.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 48/100 [20:52<22:41, 26.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 49/100 [21:18<22:14, 26.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 50/100 [21:44<21:47, 26.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 51/100 [22:11<21:20, 26.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 52/100 [22:37<20:54, 26.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 53/100 [23:03<20:27, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 54/100 [23:29<20:02, 26.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▌    | 55/100 [23:55<19:35, 26.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 56/100 [24:21<19:09, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 57/100 [24:47<18:42, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 58/100 [25:13<18:17, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 59/100 [25:39<17:50, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 60/100 [26:06<17:24, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████    | 61/100 [26:32<16:58, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 62/100 [26:58<16:31, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 63/100 [27:24<16:05, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 64/100 [27:50<15:39, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▌   | 65/100 [28:16<15:13, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 66/100 [28:42<14:47, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 67/100 [29:08<14:22, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 68/100 [29:34<13:55, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 69/100 [30:01<13:30, 26.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 70/100 [30:27<13:03, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████   | 71/100 [30:53<12:37, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 72/100 [31:19<12:11, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 73/100 [31:45<11:45, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 74%|███████▍  | 74/100 [32:11<11:18, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 75%|███████▌  | 75/100 [32:37<10:52, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▌  | 76/100 [33:03<10:26, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 77%|███████▋  | 77/100 [33:29<10:00, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 78%|███████▊  | 78/100 [33:56<09:33, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 79%|███████▉  | 79/100 [34:22<09:07, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 80/100 [34:48<08:41, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 81%|████████  | 81/100 [35:14<08:15, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 82%|████████▏ | 82/100 [35:40<07:49, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 83%|████████▎ | 83/100 [36:06<07:23, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 84%|████████▍ | 84/100 [36:32<06:57, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 85%|████████▌ | 85/100 [36:58<06:31, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 86%|████████▌ | 86/100 [37:24<06:05, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 87%|████████▋ | 87/100 [37:50<05:39, 26.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 88%|████████▊ | 88/100 [38:17<05:13, 26.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 89%|████████▉ | 89/100 [38:43<04:47, 26.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 90/100 [39:09<04:21, 26.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 91%|█████████ | 91/100 [39:35<03:54, 26.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 92%|█████████▏| 92/100 [40:01<03:28, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 93%|█████████▎| 93/100 [40:27<03:02, 26.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 94%|█████████▍| 94/100 [40:53<02:36, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 95%|█████████▌| 95/100 [41:19<02:10, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 96%|█████████▌| 96/100 [41:45<01:44, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 97%|█████████▋| 97/100 [42:11<01:18, 26.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 98%|█████████▊| 98/100 [42:37<00:52, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 99%|█████████▉| 99/100 [43:04<00:26, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 100/100 [43:30<00:00, 26.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mmB-Tl4u9AX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e01aa7f-d2b7-4479-d30d-606db0f7562d"
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        \n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "        with torch.no_grad():\n",
        "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mrandom\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91msee\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91mfor\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91mplan\u001b[0m ( or\n",
            "\u001b[92m<unk>\u001b[0m \t\t ( or \u001b[91mother\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mrandom\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mbnc\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[91mand\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[91mto\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mtheoretical\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mthat\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mfilter\u001b[0m for many\n",
            "\u001b[92mfor\u001b[0m \t\t re- source \u001b[91mof\u001b[0m many languages\n",
            "\u001b[92mmany\u001b[0m \t\t source for \u001b[91mtwo\u001b[0m languages .\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mx\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91mwas\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[92m,\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mon\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m)\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[92m,\u001b[0m a ,\n",
            "\u001b[92ma\u001b[0m \t\t r , \u001b[91mc\u001b[0m , m\n",
            "\u001b[92m,\u001b[0m \t\t , a \u001b[91mand\u001b[0m m ,\n",
            "\u001b[92mm\u001b[0m \t\t a , \u001b[91me\u001b[0m , p\n",
            "\u001b[92m,\u001b[0m \t\t , m \u001b[91m(\u001b[0m p )\n",
            "\u001b[92mp\u001b[0m \t\t m , \u001b[91my\u001b[0m ) .\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mwhether\u001b[0m methods are\n",
            "\u001b[92mmethods\u001b[0m \t , their \u001b[91mrandom\u001b[0m are inevitably\n",
            "\u001b[92mare\u001b[0m \t\t their methods \u001b[91mis\u001b[0m inevitably noisy\n",
            "\u001b[92minevitably\u001b[0m \t methods are \u001b[91mvery\u001b[0m noisy ,\n",
            "\u001b[92mnoisy\u001b[0m \t\t are inevitably \u001b[91mby\u001b[0m , suffering\n",
            "\u001b[92m,\u001b[0m \t\t inevitably noisy \u001b[92m,\u001b[0m suffering ,\n",
            "\u001b[92msuffering\u001b[0m \t noisy , \u001b[91me\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t , suffering \u001b[91mvalues\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t suffering , \u001b[91mor\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mword\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mand\u001b[0m just those\n",
            "\u001b[92mjust\u001b[0m \t\t , from \u001b[91mwhat\u001b[0m those parser\n",
            "\u001b[92mthose\u001b[0m \t\t from just \u001b[91mthat\u001b[0m parser errors\n",
            "\u001b[92mparser\u001b[0m \t\t just those \u001b[91mparsing\u001b[0m errors that\n",
            "\u001b[92merrors\u001b[0m \t\t those parser \u001b[91m,\u001b[0m that the\n",
            "\u001b[92mthat\u001b[0m \t\t parser errors \u001b[91min\u001b[0m the whole\n",
            "\u001b[92mthe\u001b[0m \t\t errors that \u001b[92mthe\u001b[0m whole process\n",
            "\u001b[92mwhole\u001b[0m \t\t that the \u001b[91mfiltering\u001b[0m process is\n",
            "\u001b[92mprocess\u001b[0m \t the whole \u001b[91mprobability\u001b[0m is designed\n",
            "\u001b[92mis\u001b[0m \t\t whole process \u001b[91mon\u001b[0m designed to\n",
            "\u001b[92mdesigned\u001b[0m \t process is \u001b[91mgenerally\u001b[0m to address\n",
            "\u001b[92mto\u001b[0m \t\t is designed \u001b[91mby\u001b[0m address ,\n",
            "\u001b[92maddress\u001b[0m \t designed to \u001b[91mrandom\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t to address \u001b[91mas\u001b[0m and they\n",
            "\u001b[92mand\u001b[0m \t\t address , \u001b[91mindeed\u001b[0m they do\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mdoes\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[92mnot\u001b[0m wish to\n",
            "\u001b[92mwish\u001b[0m \t\t do not \u001b[91mbe\u001b[0m to accept\n",
            "\u001b[92mto\u001b[0m \t\t not wish \u001b[92mto\u001b[0m accept any\n",
            "\u001b[92maccept\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
            "\u001b[92many\u001b[0m \t\t to accept \u001b[91mthe\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t accept any \u001b[91mlists\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m,\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mfiltering\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mh0\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[91mfor\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91man\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91m,\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mover\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[92ma\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91msimple\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91msubset\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[91meach\u001b[0m verb .\n",
            "\u001b[92mmight\u001b[0m \t\t while it \u001b[91marbitrary\u001b[0m seem plausible\n",
            "\u001b[92mseem\u001b[0m \t\t it might \u001b[91mbe\u001b[0m plausible that\n",
            "\u001b[92mplausible\u001b[0m \t might seem \u001b[91mrejected\u001b[0m that oddities\n",
            "\u001b[92mthat\u001b[0m \t\t seem plausible \u001b[91mor\u001b[0m oddities would\n",
            "\u001b[92moddities\u001b[0m \t plausible that \u001b[91mis\u001b[0m would in\n",
            "\u001b[92mwould\u001b[0m \t\t that oddities \u001b[91mand\u001b[0m in some\n",
            "\u001b[92min\u001b[0m \t\t oddities would \u001b[91mhave\u001b[0m some way\n",
            "\u001b[92msome\u001b[0m \t\t would in \u001b[91ma\u001b[0m way balance\n",
            "\u001b[92mway\u001b[0m \t\t in some \u001b[91mto\u001b[0m balance out\n",
            "\u001b[92mbalance\u001b[0m \t some way \u001b[91mor\u001b[0m out to\n",
            "\u001b[92mout\u001b[0m \t\t way balance \u001b[91m,\u001b[0m to give\n",
            "\u001b[92mto\u001b[0m \t\t balance out \u001b[92mto\u001b[0m give a\n",
            "\u001b[92m<unk>\u001b[0m \t\t give a \u001b[91mand\u001b[0m tion that\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mis\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mconsidered\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91min\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91msee\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mfrom\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mor\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mtechnical\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91msize\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[91mis\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91min\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91mreach\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mwere\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mto\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[92mbeen\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mused\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91m,\u001b[0m , this\n",
            "\u001b[92m,\u001b[0m \t\t randomly selected \u001b[91mto\u001b[0m this turns\n",
            "\u001b[92mthis\u001b[0m \t\t selected , \u001b[91mwhen\u001b[0m turns out\n",
            "\u001b[92mturns\u001b[0m \t\t , this \u001b[91mliterature\u001b[0m out not\n",
            "\u001b[92mout\u001b[0m \t\t this turns \u001b[91mis\u001b[0m not to\n",
            "\u001b[92mnot\u001b[0m \t\t turns out \u001b[91m)\u001b[0m to be\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mincrease\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mused\u001b[0m case .\n",
            "\u001b[92mted\u001b[0m \t\t briscoe , \u001b[91mgeoffrey\u001b[0m and john\n",
            "\u001b[92mand\u001b[0m \t\t , ted \u001b[91m,\u001b[0m john carroll\n",
            "\u001b[92m<unk>\u001b[0m \t\t john carroll \u001b[91m(\u001b[0m automatic extraction\n",
            "\u001b[92m<unk>\u001b[0m \t\t extraction of \u001b[91mverbs\u001b[0m from corpora\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91m,\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mwhen\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mtwo\u001b[0m : is\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91ma\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91mnull\u001b[0m value ?\n",
            "\u001b[92mchristopher\u001b[0m \t manning , \u001b[91mgeoffrey\u001b[0m and hinrich\n",
            "\u001b[92mstatistical\u001b[0m \t foundations of \u001b[91mthe\u001b[0m natural language\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[92mnatural\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[92mlanguage\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mtask\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mfalse\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[92m,\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[92mwe\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mposits\u001b[0m h0 .\n",
            "\u001b[92mfrank\u001b[0m \t\t owen , \u001b[91mgeoffrey\u001b[0m and ronald\n",
            "\u001b[92mand\u001b[0m \t\t , frank \u001b[92mand\u001b[0m ronald jones\n",
            "\u001b[92m<unk>\u001b[0m \t\t ronald jones \u001b[91mwith\u001b[0m statistics .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[91ma\u001b[0m sample size\n",
            "\u001b[92msample\u001b[0m \t\t where the \u001b[91msame\u001b[0m size varies\n",
            "\u001b[92msize\u001b[0m \t\t the sample \u001b[91m(\u001b[0m varies by\n",
            "\u001b[92mvaries\u001b[0m \t\t sample size \u001b[91m,\u001b[0m by an\n",
            "\u001b[92mby\u001b[0m \t\t size varies \u001b[91mto\u001b[0m an order\n",
            "\u001b[92man\u001b[0m \t\t varies by \u001b[91mthe\u001b[0m order of\n",
            "\u001b[92morder\u001b[0m \t\t by an \u001b[91mmeasure\u001b[0m of magnitude\n",
            "\u001b[92mof\u001b[0m \t\t an order \u001b[92mof\u001b[0m magnitude ,\n",
            "\u001b[92mmagnitude\u001b[0m \t order of \u001b[91manother\u001b[0m , or\n",
            "\u001b[92m,\u001b[0m \t\t of magnitude \u001b[92m,\u001b[0m or where\n",
            "\u001b[92mor\u001b[0m \t\t magnitude , \u001b[91mpairs\u001b[0m where it\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[92mwhere\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mfalse\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[92m,\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mable\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mfor\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91mcells\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t the conference \u001b[91mis\u001b[0m the south-central\n",
            "\u001b[92mthe\u001b[0m \t\t conference of \u001b[91mcorpus\u001b[0m south-central sas\n",
            "\u001b[92msouth-central\u001b[0m \t of the \u001b[91mscf\u001b[0m sas users\n",
            "\u001b[92msas\u001b[0m \t\t the south-central \u001b[91mor\u001b[0m users group\n",
            "\u001b[92musers\u001b[0m \t\t south-central sas \u001b[91m,\u001b[0m group ,\n",
            "\u001b[92massumptions\u001b[0m \t making false \u001b[91mhypothesis\u001b[0m is often\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91m,\u001b[0m often an\n",
            "\u001b[92moften\u001b[0m \t\t assumptions is \u001b[91mto\u001b[0m an ingenious\n",
            "\u001b[92man\u001b[0m \t\t is often \u001b[91mthe\u001b[0m ingenious way\n",
            "\u001b[92mingenious\u001b[0m \t often an \u001b[91msimplest\u001b[0m way to\n",
            "\u001b[92mway\u001b[0m \t\t an ingenious \u001b[91mor\u001b[0m to proceed\n",
            "\u001b[92mto\u001b[0m \t\t ingenious way \u001b[91m(\u001b[0m proceed ;\n",
            "\u001b[92mproceed\u001b[0m \t way to \u001b[91mestimate\u001b[0m ; the\n",
            "\u001b[92m;\u001b[0m \t\t to proceed \u001b[91min\u001b[0m the problem\n",
            "\u001b[92mthe\u001b[0m \t\t proceed ; \u001b[92mthe\u001b[0m problem arises\n",
            "\u001b[92mproblem\u001b[0m \t ; the \u001b[91mrelation\u001b[0m arises where\n",
            "\u001b[92marises\u001b[0m \t\t the problem \u001b[91m,\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t problem arises \u001b[91mas\u001b[0m the literal\n",
            "\u001b[92mthe\u001b[0m \t\t arises where \u001b[91ma\u001b[0m literal falsity\n",
            "\u001b[92mliteral\u001b[0m \t where the \u001b[91mtechnical\u001b[0m falsity of\n",
            "\u001b[92mfalsity\u001b[0m \t the literal \u001b[91mcomponent\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t literal falsity \u001b[91min\u001b[0m the assumption\n",
            "\u001b[92mthe\u001b[0m \t\t falsity of \u001b[92mthe\u001b[0m assumption is\n",
            "\u001b[92massumption\u001b[0m \t of the \u001b[91mprobability\u001b[0m is overlooked\n",
            "\u001b[92mis\u001b[0m \t\t the assumption \u001b[92mis\u001b[0m overlooked ,\n",
            "\u001b[92moverlooked\u001b[0m \t assumption is \u001b[91mrandom\u001b[0m , and\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mto\u001b[0m ate inferences\n",
            "\u001b[92mare\u001b[0m \t\t ate inferences \u001b[91m)\u001b[0m drawn .\n",
            "\u001b[92m<unk>\u001b[0m \t\t language is \u001b[91mnot\u001b[0m and hence\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[92m,\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t at linguistic \u001b[91m,\u001b[0m ena in\n",
            "\u001b[92mcorpora\u001b[0m \t ena in \u001b[91mevents\u001b[0m , the\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mand\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mdo\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[92m,\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[92mbut\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
            "\u001b[92mis\u001b[0m \t\t but that \u001b[92mis\u001b[0m a distinct\n",
            "\u001b[92ma\u001b[0m \t\t that is \u001b[92ma\u001b[0m distinct issue\n",
            "\u001b[92mdistinct\u001b[0m \t is a \u001b[91mstrong\u001b[0m issue :\n",
            "\u001b[92missue\u001b[0m \t\t a distinct \u001b[91mreason\u001b[0m : wherever\n",
            "\u001b[92m:\u001b[0m \t\t distinct issue \u001b[91mwords\u001b[0m wherever there\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[92m,\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91m,\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mthe\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mdata\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mcomplete\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[92mwe\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mshall\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mat\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mtwo\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mevidence\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mor\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mthen\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mcapable\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[92m,\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mfor\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[91ma\u001b[0m hypothesis test\n",
            "\u001b[92m<unk>\u001b[0m \t\t hypothesis test \u001b[91m:\u001b[0m firms the\n",
            "\u001b[92mcases\u001b[0m \t\t some such \u001b[91mwords\u001b[0m are reviewed\n",
            "\u001b[92mare\u001b[0m \t\t such cases \u001b[91mis\u001b[0m reviewed in\n",
            "\u001b[92mreviewed\u001b[0m \t cases are \u001b[91mset\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t proceedings of \u001b[91mrandomness\u001b[0m ( recherche\n",
            "\u001b[92m<unk>\u001b[0m \t\t e par \u001b[91my\u001b[0m ) ,\n",
            "\u001b[92mof\u001b[0m \t\t the bulk \u001b[92mof\u001b[0m linguistic questions\n",
            "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91mthese\u001b[0m questions concern\n",
            "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91mtesting\u001b[0m concern the\n",
            "\u001b[92mand\u001b[0m \t\t between a \u001b[92mand\u001b[0m m. a\n",
            "\u001b[92mm.\u001b[0m \t\t a and \u001b[91min\u001b[0m a linguistic\n",
            "\u001b[92ma\u001b[0m \t\t and m. \u001b[91mand\u001b[0m linguistic account\n",
            "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91maverage\u001b[0m account of\n",
            "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91mprobability\u001b[0m of a\n",
            "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91mis\u001b[0m a phenomenon\n",
            "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m)\u001b[0m to view\n",
            "\u001b[92mto\u001b[0m \t\t us reason \u001b[91min\u001b[0m view the\n",
            "\u001b[92mview\u001b[0m \t\t reason to \u001b[91mestablish\u001b[0m the relation\n",
            "\u001b[92mthe\u001b[0m \t\t to view \u001b[92mthe\u001b[0m relation between\n",
            "\u001b[92mrelation\u001b[0m \t view the \u001b[91mfrequency\u001b[0m between ,\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mmodel\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mfrequency\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[92mfor\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91mrepresented\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[92m,\u001b[0m a verb\n",
            "\u001b[92m<unk>\u001b[0m \t\t a verb \u001b[91mand\u001b[0m s syntax\n",
            "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91mfor\u001b[0m its semantics\n",
            "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91mscf\u001b[0m semantics ,\n",
            "\u001b[92msemantics\u001b[0m \t and its \u001b[91mdata\u001b[0m , as\n",
            "\u001b[92m,\u001b[0m \t\t its semantics \u001b[92m,\u001b[0m as motivated\n",
            "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91mwe\u001b[0m motivated rather\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mor\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mprobabilities\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mbe\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91merror\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[92m,\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[92m,\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91min\u001b[0m the hypothesis\n",
            "\u001b[92mcan\u001b[0m \t\t the hypothesis \u001b[91mtest\u001b[0m , therefore\n",
            "\u001b[92m,\u001b[0m \t\t hypothesis can \u001b[92m,\u001b[0m therefore ,\n",
            "\u001b[92mtherefore\u001b[0m \t can , \u001b[91mprovably\u001b[0m , be\n",
            "\u001b[92m,\u001b[0m \t\t , therefore \u001b[91mcan\u001b[0m be couched\n",
            "\u001b[92mbe\u001b[0m \t\t therefore , \u001b[91mall\u001b[0m couched as\n",
            "\u001b[92mcouched\u001b[0m \t , be \u001b[91m,\u001b[0m as :\n",
            "\u001b[92mas\u001b[0m \t\t be couched \u001b[91mwhether\u001b[0m : are\n",
            "\u001b[92m:\u001b[0m \t\t couched as \u001b[91mvery\u001b[0m are the\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91mdoes\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[91msame\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mis\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mof\u001b[0m greater than\n",
            "\u001b[92m<unk>\u001b[0m \t\t with just \u001b[91mlarge\u001b[0m % of\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mx\u001b[0m , devastate\n",
            "\u001b[92m,\u001b[0m \t\t of them \u001b[91mdata\u001b[0m devastate becomes\n",
            "\u001b[92mdevastate\u001b[0m \t them , \u001b[91mas\u001b[0m becomes one\n",
            "\u001b[92mbecomes\u001b[0m \t , devastate \u001b[91mby\u001b[0m one of\n",
            "\u001b[92mone\u001b[0m \t\t devastate becomes \u001b[91midentical\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t becomes one \u001b[92mof\u001b[0m the verbs\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mthese\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mfilter\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m,\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91meach\u001b[0m we have\n",
            "\u001b[92mwe\u001b[0m \t\t for which \u001b[91mthey\u001b[0m have plenty\n",
            "\u001b[92mhave\u001b[0m \t\t which we \u001b[92mhave\u001b[0m plenty of\n",
            "\u001b[92mplenty\u001b[0m \t\t we have \u001b[91ma\u001b[0m of data\n",
            "\u001b[92mof\u001b[0m \t\t have plenty \u001b[92mof\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t plenty of \u001b[91mrandom\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t of data \u001b[91mfrequencies\u001b[0m and crude\n",
            "\u001b[92m<unk>\u001b[0m \t\t and crude \u001b[91mor\u001b[0m methods will\n",
            "\u001b[92m<unk>\u001b[0m \t\t distinguish associated \u001b[91mdata\u001b[0m from noise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSl2J6WDvIvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c481a25e-75d6-4854-d96f-bf768459f83c"
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2537764350453172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Cxrwh9vKoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}