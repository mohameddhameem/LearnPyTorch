{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Basics with PyTorch - NLTK - Word2Vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOycvrk2jRL7nNoXJy2MO5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohameddhameem/LearnPyTorch/blob/master/NLP_Basics_with_PyTorch_NLTK_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70OkV4V5paqN",
        "colab_type": "text"
      },
      "source": [
        "# Basics of NLP with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVd7SxQtplrw",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "\n",
        "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
        "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
        "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
        "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
        "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
        "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
        "<br><br>\n",
        "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
        "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
        "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
        "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
        "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
        "    - <a href=\"#section-3-1-4-fill-cbow\">The CBOW model</a>\n",
        "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
        "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
        "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
        "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
        "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
        "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
        "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
        "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
        "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
        "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
        "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
        "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
        "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
        "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZGAC0BHp0eZ",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0\"></a>\n",
        "# 3.0. Data Preparation\n",
        "\n",
        "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
        "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
        "\n",
        "There are already several other libraries that help with loading text datasets, e.g. \n",
        "\n",
        " - FastAI https://docs.fast.ai/text.data.html\n",
        " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
        " - Torch Text https://github.com/pytorch/text#data\n",
        " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
        " - SpaCy https://github.com/explosion/thinc\n",
        " \n",
        "\n",
        "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cltZ9XdoqGP1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-1\"></a>\n",
        "## 3.0.1  Vocabulary\n",
        "\n",
        "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmM3NtDFwFEY",
        "colab_type": "code",
        "outputId": "0360381f-4dc6-4b04-aceb-19664f2409c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install sklearn torch tqdm nltk lazyme ansi requests gensim tsundoku\n",
        "!python -m nltk.downloader movie_reviews punkt\n",
        "from IPython.display import display, Markdown, Latex\n",
        "from tsundoku.word2vec_hints import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting lazyme\n",
            "  Downloading lazyme-0.0.23.tar.gz (5.4 kB)\n",
            "Collecting ansi\n",
            "  Downloading ansi-0.1.3.tar.gz (4.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.8.1)\n",
            "Collecting tsundoku\n",
            "  Downloading tsundoku-0.0.6.tar.gz (3.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.6/dist-packages (from tsundoku) (5.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.11.15)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.4.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->tsundoku) (45.1.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.14.15)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->tsundoku) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->tsundoku) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->tsundoku) (0.1.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Building wheels for collected packages: lazyme, ansi, tsundoku\n",
            "  Building wheel for lazyme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lazyme: filename=lazyme-0.0.23-py3-none-any.whl size=7929 sha256=7395af905717442e6189cc3adc04d20a2232c7af94a4fa927242d16b841bf0c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/14/dc/ad5e41a497bf8aa3434c727cd8b0a0e67876eae94fb3130724\n",
            "  Building wheel for ansi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ansi: filename=ansi-0.1.3-py3-none-any.whl size=6825 sha256=078fd70d05949f05ebbd32a495acb0f98a65059d504b0ddeca1f198d711138b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/fd/65/a952ef68e49306861529325cbe318acac0642a673487d36a64\n",
            "  Building wheel for tsundoku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tsundoku: filename=tsundoku-0.0.6-py3-none-any.whl size=4872 sha256=13ef14e1aadcb7b837e2be8c952d06492a771662a0faea3fe09da01e926c04f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/d1/e8/20a3112b49141d5fc3981578e0bffb6b17c57b44786fbbacc2\n",
            "Successfully built lazyme ansi tsundoku\n",
            "Installing collected packages: lazyme, ansi, tsundoku\n",
            "Successfully installed ansi-0.1.3 lazyme-0.0.23 tsundoku-0.0.6\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOfn_lHQpfJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from itertools import chain\n",
        "\n",
        "from tqdm import tqdm\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNksrIzDqLWF",
        "colab_type": "code",
        "outputId": "963705d7-46bc-4879-e907-9e94c60df920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try: # Use the default NLTK tokenizer.\n",
        "    from nltk import word_tokenize, sent_tokenize \n",
        "    # Testing whether it works. \n",
        "    # Sometimes it doesn't work on some machines because of setup issues.\n",
        "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
        "    print('It Works')\n",
        "except: # Use a naive sentence tokenizer and toktok.\n",
        "    import re\n",
        "    from nltk.tokenize import ToktokTokenizer\n",
        "    # See https://stackoverflow.com/a/25736515/610569\n",
        "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
        "    # Use the toktok tokenizer that requires no dependencies.\n",
        "    toktok = ToktokTokenizer()\n",
        "    word_tokenize = word_tokenize = toktok.tokenize\n",
        "    print('It Still Works')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It Works\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeJwCwxbqpQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "uniq_tokens = set(chain(*tokenized_text))\n",
        "\n",
        "vocab = {}   # Assign indices to every word.\n",
        "idx2tok = {} # Also keep an dict of index to words.\n",
        "for i, token in enumerate(uniq_tokens):\n",
        "    vocab[token] = i\n",
        "    idx2tok[i] = token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uKEXqgq21y",
        "colab_type": "code",
        "outputId": "c8fc233b-bb38-4fbd-fe80-0f323d69aa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 33,\n",
              " ')': 79,\n",
              " ',': 52,\n",
              " '.': 49,\n",
              " 'a': 2,\n",
              " 'able': 5,\n",
              " 'almost': 39,\n",
              " 'always': 16,\n",
              " 'and': 48,\n",
              " 'arbitrary': 64,\n",
              " 'are': 71,\n",
              " 'associations': 31,\n",
              " 'at': 70,\n",
              " 'be': 34,\n",
              " 'been': 17,\n",
              " 'between': 13,\n",
              " 'choose': 41,\n",
              " 'corpora': 44,\n",
              " 'corpus': 69,\n",
              " 'data': 4,\n",
              " 'demonstrably': 1,\n",
              " 'do': 12,\n",
              " 'does': 54,\n",
              " 'enough': 55,\n",
              " 'essentially': 56,\n",
              " 'establish': 80,\n",
              " 'evidence': 30,\n",
              " 'experimental': 72,\n",
              " 'fact': 28,\n",
              " 'frequencies': 47,\n",
              " 'frequently': 81,\n",
              " 'has': 15,\n",
              " 'have': 45,\n",
              " 'hence': 61,\n",
              " 'how': 25,\n",
              " 'hypothesis': 51,\n",
              " 'in': 75,\n",
              " 'inference': 11,\n",
              " 'is': 68,\n",
              " 'it': 46,\n",
              " 'language': 43,\n",
              " 'led': 29,\n",
              " 'linguistic': 73,\n",
              " 'literature': 26,\n",
              " 'look': 58,\n",
              " 'misleading': 10,\n",
              " 'moreover': 50,\n",
              " 'never': 60,\n",
              " 'non-random': 14,\n",
              " 'not': 27,\n",
              " 'null': 59,\n",
              " 'of': 84,\n",
              " 'often': 7,\n",
              " 'or': 8,\n",
              " 'phenomena': 85,\n",
              " 'posits': 76,\n",
              " 'present': 38,\n",
              " 'randomly': 63,\n",
              " 'randomness': 0,\n",
              " 'relation': 86,\n",
              " 'results': 67,\n",
              " 'review': 57,\n",
              " 'shall': 62,\n",
              " 'show': 77,\n",
              " 'so': 6,\n",
              " 'statistical': 32,\n",
              " 'studies': 22,\n",
              " 'support': 82,\n",
              " 'systematically': 35,\n",
              " 'testing': 23,\n",
              " 'that': 42,\n",
              " 'the': 20,\n",
              " 'there': 53,\n",
              " 'to': 3,\n",
              " 'true': 74,\n",
              " 'two': 9,\n",
              " 'unhelpful': 36,\n",
              " 'used': 78,\n",
              " 'users': 21,\n",
              " 'uses': 37,\n",
              " 'we': 66,\n",
              " 'when': 65,\n",
              " 'where': 18,\n",
              " 'which': 40,\n",
              " 'will': 24,\n",
              " 'word': 19,\n",
              " 'words': 83}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbBzwCehrATy",
        "colab_type": "code",
        "outputId": "977fc955-06bf-4e51-eb57-5d5cd547cc61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Retrieve the index of the word 'corpora'\n",
        "vocab['corpora']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDAfYLSLrEKA",
        "colab_type": "code",
        "outputId": "9c1235a8-17f9-4dab-f9d5-7dd697b65d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# The indexed representation of the first sentence.\n",
        "\n",
        "sent0 = tokenized_text[0]\n",
        "print('sent0', sent0)\n",
        "[vocab[token] for token in sent0] "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent0 ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[43, 21, 60, 41, 83, 63, 52, 48, 43, 68, 56, 14, 49]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARvZ0wFarXi6",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-1-a\"></a>\n",
        "\n",
        "### Pet Peeve (Gensim)\n",
        "\n",
        "`gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
        "\n",
        "Using `gensim`, I would have written the above as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BMX_O3HrJH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "vocab = Dictionary(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_T-LSAXrl7n",
        "colab_type": "code",
        "outputId": "112a2b4e-c888-4411-b9a0-7ff29be08a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Note the key-value order is different of gensim from the native Python's\n",
        "dict(vocab.items())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ',',\n",
              " 1: '.',\n",
              " 2: 'and',\n",
              " 3: 'choose',\n",
              " 4: 'essentially',\n",
              " 5: 'is',\n",
              " 6: 'language',\n",
              " 7: 'never',\n",
              " 8: 'non-random',\n",
              " 9: 'randomly',\n",
              " 10: 'users',\n",
              " 11: 'words',\n",
              " 12: 'a',\n",
              " 13: 'hypothesis',\n",
              " 14: 'null',\n",
              " 15: 'posits',\n",
              " 16: 'randomness',\n",
              " 17: 'statistical',\n",
              " 18: 'testing',\n",
              " 19: 'uses',\n",
              " 20: 'which',\n",
              " 21: 'at',\n",
              " 22: 'be',\n",
              " 23: 'corpora',\n",
              " 24: 'hence',\n",
              " 25: 'in',\n",
              " 26: 'linguistic',\n",
              " 27: 'look',\n",
              " 28: 'phenomena',\n",
              " 29: 'the',\n",
              " 30: 'true',\n",
              " 31: 'we',\n",
              " 32: 'when',\n",
              " 33: 'will',\n",
              " 34: '(',\n",
              " 35: ')',\n",
              " 36: 'able',\n",
              " 37: 'almost',\n",
              " 38: 'always',\n",
              " 39: 'data',\n",
              " 40: 'enough',\n",
              " 41: 'establish',\n",
              " 42: 'it',\n",
              " 43: 'moreover',\n",
              " 44: 'not',\n",
              " 45: 'shall',\n",
              " 46: 'that',\n",
              " 47: 'there',\n",
              " 48: 'to',\n",
              " 49: 'where',\n",
              " 50: 'arbitrary',\n",
              " 51: 'between',\n",
              " 52: 'corpus',\n",
              " 53: 'demonstrably',\n",
              " 54: 'do',\n",
              " 55: 'does',\n",
              " 56: 'fact',\n",
              " 57: 'frequently',\n",
              " 58: 'have',\n",
              " 59: 'inference',\n",
              " 60: 'relation',\n",
              " 61: 'so',\n",
              " 62: 'studies',\n",
              " 63: 'support',\n",
              " 64: 'two',\n",
              " 65: 'are',\n",
              " 66: 'associations',\n",
              " 67: 'evidence',\n",
              " 68: 'experimental',\n",
              " 69: 'frequencies',\n",
              " 70: 'how',\n",
              " 71: 'of',\n",
              " 72: 'present',\n",
              " 73: 'systematically',\n",
              " 74: 'word',\n",
              " 75: 'been',\n",
              " 76: 'has',\n",
              " 77: 'led',\n",
              " 78: 'literature',\n",
              " 79: 'misleading',\n",
              " 80: 'often',\n",
              " 81: 'or',\n",
              " 82: 'results',\n",
              " 83: 'review',\n",
              " 84: 'show',\n",
              " 85: 'unhelpful',\n",
              " 86: 'used'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2KbCph1rqJs",
        "colab_type": "code",
        "outputId": "0ab7a5ce-3392-4286-cbc5-2c2ea17405e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab.token2id['corpora']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IWY7H80ruKu",
        "colab_type": "code",
        "outputId": "4fabd9ea-aa12-44a8-f3ea-514c5659a677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab.doc2idx(sent0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8LR3hTmr5FG",
        "colab_type": "text"
      },
      "source": [
        "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzuaIDNr_D2",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2\"></a>\n",
        "\n",
        "# 3.0.2 Dataset\n",
        "\n",
        "Lets try creating a `torch.utils.data.Dataset` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aJ1f0z9rzRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Text(Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self.vocab = Dictionary(tokenized_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        # Hint: You want to return a vectorized sentence here.\n",
        "        return {'x': self.vectorize(self.sents[index])}\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkicubMs1R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BELOE IS FROM MY LECTURER. FEEL FREE TO TRY\n",
        "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
        "##hint_dataset_vectorize()\n",
        "##code_text_dataset_vectorize()\n",
        "\n",
        "# Option 2: \"I give up just, run the code for me\" \n",
        "# Uncomment the next two lines, if you really gave up... \n",
        "#full_code_text_dataset_vectorize()\n",
        "#from tsundoku.word2vec import Text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V98xzXBZvLt1",
        "colab_type": "code",
        "outputId": "1d1a05b2-97f2-4869-e902-c89483553955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "tokenized_text[5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " 'present',\n",
              " 'experimental',\n",
              " 'evidence',\n",
              " 'of',\n",
              " 'how',\n",
              " 'arbitrary',\n",
              " 'associations',\n",
              " 'between',\n",
              " 'word',\n",
              " 'frequencies',\n",
              " 'and',\n",
              " 'corpora',\n",
              " 'are',\n",
              " 'systematically',\n",
              " 'non-random',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nTtQnJivdpe",
        "colab_type": "code",
        "outputId": "c78d9138-6a0b-4096-8348-f52a24668fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_dataset = Text(tokenized_text)\n",
        "text_dataset[5] # First sentence. Representation of above text"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': [31, 72, 68, 67, 71, 70, 50, 66, 51, 74, 69, 2, 23, 65, 73, 8, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mj8VJ_mwpLM",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2-return-dict\"></a>\n",
        "\n",
        "### Return `dict` in `__getitem__()`\n",
        "\n",
        "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
        "\n",
        "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYAQWlCweKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LabeledText(Dataset):\n",
        "    def __init__(self, tokenized_texts, labels):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self.labels = labels # Sentence level labels.\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        return {'x': self.vectorize(self.sents[index]), 'y': self.labels[index]}\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuHeSkMiw1PB",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-0-2-labeleddata\"></a>\n",
        "\n",
        "### Lets try the `LabeledDataset` on a movie review corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKyJmgC1wvwM",
        "colab_type": "code",
        "outputId": "811acf1e-de63-46a8-f447-c768eace8da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "documents = []\n",
        "labels = []\n",
        "\n",
        "for fileid in tqdm(movie_reviews.fileids()):\n",
        "    label = fileid.split('/')[0]\n",
        "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
        "    documents.append(doc)\n",
        "    labels.append(label)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:09<00:00, 220.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwEZ5VmNw5xo",
        "colab_type": "code",
        "outputId": "dd3e7693-d199-4783-cc1a-3afcd1a73867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(documents[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'s\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '``', 'sorta', '``', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'did', \"n't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'s\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '``', 'normal', '``', 'but', 'then', 'downshifts', 'into', 'this', '``', 'fantasy', '``', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'s\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'do', \"n't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'s\", 'biggest', 'problem', '.', 'it', \"'s\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'did', \"n't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '``', 'into', 'it', '``', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'do', \"n't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'ve\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '``', 'the', 'suits', '``', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'s\", 'unraveling', '.', 'overall', ',', 'the', 'film', 'does', \"n't\", 'stick', 'because', 'it', 'does', \"n't\", 'entertain', ',', 'it', \"'s\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'s\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'s\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwziDJdFw-of",
        "colab_type": "code",
        "outputId": "470a4818-1425-443f-a7cc-89ea4a3875d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "labeled_dataset = LabeledText(documents, labels)\n",
        "print(labeled_dataset[0])  # First review in the data."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'x': [243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5], 'y': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnNPd0OxEyR",
        "colab_type": "code",
        "outputId": "0d8864f4-7f2b-444f-c394-69f3bd635846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(labeled_dataset[0]['x'])  # First review in vectorized index format."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqLJVcrCxLdh",
        "colab_type": "code",
        "outputId": "f52d948f-4846-42ee-bc16-6945e262ebd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(labeled_dataset[0]['y'])  # Label of the first review in the data. "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985OjlPTxUlk",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1\"></a>\n",
        "\n",
        "# 3.1 Word2Vec Training\n",
        "\n",
        "Word2Vec has two training variants:\n",
        "\n",
        " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
        " - **Skip-grams**: Predict context words given center word.\n",
        "  \n",
        "Visually, they look like this:\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm9cdGM2xhND",
        "colab_type": "text"
      },
      "source": [
        "Fig. 1. The skip-gram model. Both the input vector xx and the output yy are one-hot encoded word representations. <br>The hidden layer is the word embedding of size NN.\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_b0MdnzyXRm",
        "colab_type": "text"
      },
      "source": [
        "Fig. 2. The CBOW model. Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer. Other symbols have the same meanings as in Fig 1.\n",
        "\n",
        "(Pretty network images above are from [https://lilianweng.github.io](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-continuous-bag-of-words-cbow))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdcgrrdky6P6",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-1\"></a>\n",
        "\n",
        "## 3.1.1. CBOW\n",
        "\n",
        "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE-og_THxOWi",
        "colab_type": "code",
        "outputId": "e31d110c-0927-49d1-fb37-c8fc590e2247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from lazyme import per_window, per_chunk\n",
        "\n",
        "xx =[1,2,3,4]\n",
        "list(per_window(xx, n=2))\n",
        "list(per_chunk(xx, n=3))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 3), (4, None, None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suLX0NBvzFn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def per_window(sequence, n=1):\n",
        "    \"\"\"\n",
        "    From http://stackoverflow.com/q/42220614/610569\n",
        "        >>> list(per_window([1,2,3,4], n=2))\n",
        "        [(1, 2), (2, 3), (3, 4)]\n",
        "        >>> list(per_window([1,2,3,4], n=3))\n",
        "        [(1, 2, 3), (2, 3, 4)]\n",
        "    \"\"\"\n",
        "    start, stop = 0, n\n",
        "    seq = list(sequence)\n",
        "    while stop <= len(seq):\n",
        "        yield seq[start:stop]\n",
        "        start += 1\n",
        "        stop += 1\n",
        "\n",
        "def cbow_iterator(tokens, window_size):\n",
        "    n = window_size * 2 + 1\n",
        "    for window in per_window(tokens, n):\n",
        "        target = window.pop(window_size)\n",
        "        yield window, target   # X = window ; Y = target. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyf9slOh06S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
        "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNZ4NW2r1IFE",
        "colab_type": "code",
        "outputId": "825e8014-a44c-444d-b30b-8bbbff401e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(cbow_iterator(sent0, 2)) \n",
        "#the first part is X and target is Y\n",
        "#X => 'language', 'users', 'choose', 'words'\n",
        "#y => 'never'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['language', 'users', 'choose', 'words'], 'never'),\n",
              " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
              " (['never', 'choose', 'randomly', ','], 'words'),\n",
              " (['choose', 'words', ',', 'and'], 'randomly'),\n",
              " (['words', 'randomly', 'and', 'language'], ','),\n",
              " (['randomly', ',', 'language', 'is'], 'and'),\n",
              " ([',', 'and', 'is', 'essentially'], 'language'),\n",
              " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
              " (['language', 'is', 'non-random', '.'], 'essentially')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4upcyLol1KF9",
        "colab_type": "code",
        "outputId": "8a16cbee-2769-4da8-ebef-22003e1c424a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(cbow_iterator(sent0, 3)) "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
              " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
              " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
              " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
              " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
              " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
              " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiIkqfO1ndl",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-2\"></a>\n",
        "\n",
        "## 3.1.2. Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGmQ1741i-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def skipgram_iterator(tokens, window_size):\n",
        "    n = window_size * 2 + 1 \n",
        "    for i, window in enumerate(per_window(tokens, n)):\n",
        "        target = window.pop(window_size)\n",
        "        # Generate positive samples.\n",
        "        for context_word in window:\n",
        "            yield target, context_word, 1\n",
        "        # Generate negative samples.\n",
        "        for _ in range(n-1):\n",
        "            leftovers = tokens[:i] + tokens[i+n:]\n",
        "            yield target, random.choice(leftovers), 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tw3CKO81ruM",
        "colab_type": "code",
        "outputId": "b5a2e831-5083-41ec-ac1a-629b13388bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(skipgram_iterator(sent0, 2))\n",
        "#1 is positive sample and 0 negative"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('never', 'language', 1),\n",
              " ('never', 'users', 1),\n",
              " ('never', 'choose', 1),\n",
              " ('never', 'words', 1),\n",
              " ('never', '.', 0),\n",
              " ('never', '.', 0),\n",
              " ('never', 'essentially', 0),\n",
              " ('never', 'and', 0),\n",
              " ('choose', 'users', 1),\n",
              " ('choose', 'never', 1),\n",
              " ('choose', 'words', 1),\n",
              " ('choose', 'randomly', 1),\n",
              " ('choose', '.', 0),\n",
              " ('choose', 'and', 0),\n",
              " ('choose', 'language', 0),\n",
              " ('choose', 'non-random', 0),\n",
              " ('words', 'never', 1),\n",
              " ('words', 'choose', 1),\n",
              " ('words', 'randomly', 1),\n",
              " ('words', ',', 1),\n",
              " ('words', 'users', 0),\n",
              " ('words', 'non-random', 0),\n",
              " ('words', 'users', 0),\n",
              " ('words', 'non-random', 0),\n",
              " ('randomly', 'choose', 1),\n",
              " ('randomly', 'words', 1),\n",
              " ('randomly', ',', 1),\n",
              " ('randomly', 'and', 1),\n",
              " ('randomly', 'language', 0),\n",
              " ('randomly', 'language', 0),\n",
              " ('randomly', 'never', 0),\n",
              " ('randomly', 'users', 0),\n",
              " (',', 'words', 1),\n",
              " (',', 'randomly', 1),\n",
              " (',', 'and', 1),\n",
              " (',', 'language', 1),\n",
              " (',', 'users', 0),\n",
              " (',', 'choose', 0),\n",
              " (',', 'choose', 0),\n",
              " (',', 'never', 0),\n",
              " ('and', 'randomly', 1),\n",
              " ('and', ',', 1),\n",
              " ('and', 'language', 1),\n",
              " ('and', 'is', 1),\n",
              " ('and', 'essentially', 0),\n",
              " ('and', 'choose', 0),\n",
              " ('and', 'essentially', 0),\n",
              " ('and', '.', 0),\n",
              " ('language', ',', 1),\n",
              " ('language', 'and', 1),\n",
              " ('language', 'is', 1),\n",
              " ('language', 'essentially', 1),\n",
              " ('language', 'choose', 0),\n",
              " ('language', 'language', 0),\n",
              " ('language', 'language', 0),\n",
              " ('language', 'users', 0),\n",
              " ('is', 'and', 1),\n",
              " ('is', 'language', 1),\n",
              " ('is', 'essentially', 1),\n",
              " ('is', 'non-random', 1),\n",
              " ('is', 'choose', 0),\n",
              " ('is', 'language', 0),\n",
              " ('is', 'words', 0),\n",
              " ('is', 'words', 0),\n",
              " ('essentially', 'language', 1),\n",
              " ('essentially', 'is', 1),\n",
              " ('essentially', 'non-random', 1),\n",
              " ('essentially', '.', 1),\n",
              " ('essentially', 'words', 0),\n",
              " ('essentially', ',', 0),\n",
              " ('essentially', 'never', 0),\n",
              " ('essentially', 'users', 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCDPaz12EyN",
        "colab_type": "text"
      },
      "source": [
        "## Cut-away: What is `partial`?\n",
        "\n",
        "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYt9mXzG1tk4",
        "colab_type": "code",
        "outputId": "7df6c144-256a-43ef-bda3-18592d3b7487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "# Generates bigrams\n",
        "list(ngrams('this is a sentence'.split(), n=2))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcbr82d2Ipu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
        "bigrams = partial(ngrams, n=2)\n",
        "trigrams = partial(ngrams, n=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iKOZYNH2NYf",
        "colab_type": "code",
        "outputId": "fbd7d001-4d5d-4d20-9ecb-90b76ec8ee96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(trigrams('this is a sentence'.split()))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3dOFOLO2PeD",
        "colab_type": "code",
        "outputId": "5c92e725-0406-4bab-9104-3f36b50afe84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "list(bigrams('this is a sentence'.split()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLyudWp2Uy1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-3\"></a>\n",
        "\n",
        "## 3.1.3 Word2Vec Dataset\n",
        "\n",
        "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
        "\n",
        "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es7gpYNM2Qx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecText(Dataset):\n",
        "    def __init__(self, tokenized_texts, window_size, variant):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self._len = len(self.sents)\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "        self.window_size = window_size\n",
        "        self.variant = variant\n",
        "        if variant.lower() == 'cbow':\n",
        "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
        "        elif variant.lower() == 'skipgram':\n",
        "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "\n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        vectorized_sent = self.vectorize(self.sents[index])\n",
        "        return list(self._iterator(vectorized_sent))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized.\n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
        "        return self.vocab.doc2idx(tokens)\n",
        "\n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]\n",
        "\n",
        "    def cbow_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1\n",
        "        for window in per_window(tokens, n):\n",
        "            target = window.pop(window_size)\n",
        "            yield {'x':window, 'y':target}   # X = window ; Y = target. \n",
        "\n",
        "    def skipgram_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1 \n",
        "        for i, window in enumerate(per_window(tokens, n)):\n",
        "            target = window.pop(window_size)\n",
        "            # Generate positive samples.\n",
        "            for context_word in window:\n",
        "                yield {'x':(target, context_word), 'y':1}\n",
        "            # Generate negative samples.\n",
        "            for _ in range(n-1):\n",
        "                leftovers = tokens[:i] + tokens[i+n:]\n",
        "                yield {'x': (target, random.choice(leftovers)), 'y':0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHreIlYP3e2z",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-3-hint\"></a>\n",
        "## Hints for the cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DWMTG8l3fUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
        "##hint_word2vec_dataset()\n",
        "\n",
        "# Option 2: \"I give up just, run the code for me\" \n",
        "# Uncomment the next two lines, if you really gave up... \n",
        "##full_code_word2vec_dataset()\n",
        "##from tsundoku.word2vec import Word2VecText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPxvwu4w3qYP",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-hint\"></a>\n",
        "\n",
        "## 3.1.4. Train a CBOW model\n",
        "\n",
        "### Lets Get Some Data\n",
        "\n",
        "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxICbkpt3iaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os,requests, io #codecs\n",
        "\n",
        "\n",
        "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
        "if os.path.isfile('language-never-random.txt'):\n",
        "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
        "        text = fin.read()\n",
        "else:\n",
        "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "    text = requests.get(url).content.decode('utf8')\n",
        "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "        fout.write(text)\n",
        "\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
        "window_size = 2\n",
        "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxe-EDyT36OX",
        "colab_type": "code",
        "outputId": "a71c2d0e-f5f3-468e-b2c1-f98ece78af88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish that it is not true. In\n",
            "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
            "tion between two phenomena is demonstrably non-random, does not sup-\n",
            "port the inference that it is not arbitrary. We present experimental evidence\n",
            "of how arbitrary associations between word frequencies and corpora are\n",
            "systematically non-random. We review literature in which hypothesis test-\n",
            "ing has been used, and show how it has often led to unhelpful or mislead-\n",
            "ing results.\n",
            "Keywords: 쎲쎲쎲\n",
            "\n",
            "1. Int\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqHLaRwm4IOH",
        "colab_type": "code",
        "outputId": "1b9bdcc7-167a-4649-f64b-7990b03d541c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Sanity check, lets take a look at the data.\n",
        "print(tokenized_text[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxUNZmu-4O11",
        "colab_type": "code",
        "outputId": "cf40a78e-a0b3-4e57-a44b-0cf496cdb866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzY04ie74TKK",
        "colab_type": "code",
        "outputId": "9e6569c4-f25f-4072-91d3-ed98709fdd38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from lazyme import color_str\n",
        "\n",
        "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
        "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
        "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
        "    target = vocab.get(int(y), '<unk>')\n",
        "\n",
        "    if not prediction:\n",
        "        predicted_word = '______'\n",
        "    else:\n",
        "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
        "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
        "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
        "    \n",
        "\n",
        "sent_idx = 10\n",
        "window_size = 2\n",
        "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
        "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
        "for w2v_io in w2v_dataset[sent_idx]:\n",
        "    context, target = w2v_io['x'], w2v_io['y']\n",
        "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
        "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
            "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
            "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
            "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
            "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
            "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
            "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
            "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
            "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
            "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
            "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
            "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
            "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
            "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
            "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
            "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
            "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
            "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
            "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
            "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
            "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
            "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
            "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
            "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
            "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
            "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
            "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
            "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
            "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
            "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
            "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
            "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09Ap2Nq4oZ1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-cbow-model\"></a>\n",
        "\n",
        "## The CBOW Model\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"600\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VLFQA3T44RR",
        "colab_type": "text"
      },
      "source": [
        "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN142ipB4g9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974oU-pE5JYa",
        "colab_type": "text"
      },
      "source": [
        "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
        "\n",
        "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aohU8BTc5F0w",
        "colab_type": "code",
        "outputId": "e2f2b1db-9c0a-452e-b900-7bd76e544c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "w2v_dataset[0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'x': [10, 8, 0, 7], 'y': 11},\n",
              " {'x': [8, 11, 7, 0], 'y': 0},\n",
              " {'x': [11, 0, 0, 7], 'y': 7},\n",
              " {'x': [0, 7, 7, 0], 'y': 0},\n",
              " {'x': [7, 0, 0, 13], 'y': 7},\n",
              " {'x': [0, 7, 13, 3], 'y': 0},\n",
              " {'x': [7, 0, 3, 9], 'y': 13},\n",
              " {'x': [0, 13, 9, 2], 'y': 3},\n",
              " {'x': [13, 3, 2, 10], 'y': 9},\n",
              " {'x': [3, 9, 10, 15], 'y': 2},\n",
              " {'x': [9, 2, 15, 11], 'y': 10},\n",
              " {'x': [2, 10, 11, 5], 'y': 15},\n",
              " {'x': [10, 15, 5, 16], 'y': 11},\n",
              " {'x': [15, 11, 16, 14], 'y': 5},\n",
              " {'x': [11, 5, 14, 0], 'y': 16},\n",
              " {'x': [5, 16, 0, 4], 'y': 14},\n",
              " {'x': [16, 14, 4, 10], 'y': 0},\n",
              " {'x': [14, 0, 10, 8], 'y': 4},\n",
              " {'x': [0, 4, 8, 6], 'y': 10},\n",
              " {'x': [4, 10, 6, 12], 'y': 8},\n",
              " {'x': [10, 8, 12, 1], 'y': 6}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPbUZnBd5Lpj",
        "colab_type": "code",
        "outputId": "66171177-727b-40a2-a6bc-82940e901787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Lets take a look at the first output.\n",
        "x, y = w2v_dataset[0][0]['x'],  w2v_dataset[0][0]['y']\n",
        "\n",
        "x = tensor(x)\n",
        "y = autograd.Variable(tensor(y, dtype=torch.long))\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10,  8,  0,  7])\n",
            "tensor(11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPyMs7z5RJF",
        "colab_type": "code",
        "outputId": "452f4b01-3df5-40f6-c025-2d4678c1e91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "embd_size = 5\n",
        "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
        "emb.state_dict()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight', tensor([[-0.4362, -0.6812,  1.2125,  0.6570,  0.5783],\n",
              "                      [-0.4853,  1.4662, -0.4784,  0.2618, -0.0357],\n",
              "                      [-0.8017, -1.2327, -0.3651, -0.2540, -0.7131],\n",
              "                      ...,\n",
              "                      [-2.0410, -0.2616, -0.3868,  1.2900,  0.9262],\n",
              "                      [ 1.9550,  0.5650,  0.3065,  2.3414, -0.8358],\n",
              "                      [ 0.6498, -0.6371,  1.4429, -0.0929, -1.3977]]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4PxmYTw5cg7",
        "colab_type": "code",
        "outputId": "6b48961a-7ee4-406e-a9e3-4318d6e24c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(emb.state_dict()['weight'].shape)\n",
        "emb.state_dict()['weight']"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1388, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4362, -0.6812,  1.2125,  0.6570,  0.5783],\n",
              "        [-0.4853,  1.4662, -0.4784,  0.2618, -0.0357],\n",
              "        [-0.8017, -1.2327, -0.3651, -0.2540, -0.7131],\n",
              "        ...,\n",
              "        [-2.0410, -0.2616, -0.3868,  1.2900,  0.9262],\n",
              "        [ 1.9550,  0.5650,  0.3065,  2.3414, -0.8358],\n",
              "        [ 0.6498, -0.6371,  1.4429, -0.0929, -1.3977]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WEktdC5fWW",
        "colab_type": "code",
        "outputId": "e5d9403e-76cf-49eb-b364-aba506bdab68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(emb(x).shape)\n",
        "print(emb(x))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5])\n",
            "tensor([[ 0.9918, -1.4597, -1.5499,  0.5558,  0.7540],\n",
            "        [ 0.2289,  1.4218,  1.1224, -1.2136,  0.3193],\n",
            "        [-0.4362, -0.6812,  1.2125,  0.6570,  0.5783],\n",
            "        [ 1.8373,  0.2288,  0.6186, -2.7566, -0.1693]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDUv-3Vq5jRl",
        "colab_type": "code",
        "outputId": "53d0ecf6-5949-4344-8ead-8a834d583f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(emb(x).view(1, -1).shape)\n",
        "emb(x).view(1, -1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9918, -1.4597, -1.5499,  0.5558,  0.7540,  0.2289,  1.4218,  1.1224,\n",
              "         -1.2136,  0.3193, -0.4362, -0.6812,  1.2125,  0.6570,  0.5783,  1.8373,\n",
              "          0.2288,  0.6186, -2.7566, -0.1693]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrjn4g0-5v8a",
        "colab_type": "code",
        "outputId": "57dafccc-4e42-4614-bfd8-494c29644f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "hidden_size = 100\n",
        "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
        "print(lin1.state_dict())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('weight', tensor([[ 0.0074,  0.1428,  0.0050,  ..., -0.0942,  0.1171,  0.0991],\n",
            "        [ 0.1931,  0.0506,  0.2151,  ..., -0.0898, -0.1973,  0.0324],\n",
            "        [ 0.0938,  0.1459,  0.1730,  ...,  0.0226, -0.1896,  0.0414],\n",
            "        ...,\n",
            "        [-0.0699, -0.0863, -0.1129,  ..., -0.0482, -0.0014, -0.2217],\n",
            "        [ 0.0278, -0.1371, -0.0976,  ...,  0.0028,  0.0906,  0.1877],\n",
            "        [ 0.1431, -0.1961,  0.0766,  ...,  0.1160,  0.1541, -0.1425]])), ('bias', tensor([ 0.1982,  0.0936, -0.1876, -0.0629,  0.2037, -0.1139,  0.0983,  0.1430,\n",
            "        -0.2008, -0.0380,  0.0356,  0.1374, -0.0760,  0.1998,  0.1085,  0.2071,\n",
            "        -0.0727, -0.1893,  0.1829, -0.1136,  0.1484,  0.0588,  0.0792,  0.1369,\n",
            "         0.1276,  0.0873, -0.0008, -0.0312, -0.2169,  0.0255,  0.1599, -0.0803,\n",
            "        -0.0868,  0.0281, -0.1582, -0.2169, -0.1304,  0.2157,  0.2010,  0.1734,\n",
            "        -0.1278,  0.0292, -0.2116, -0.0700, -0.1426,  0.1594, -0.1668, -0.1989,\n",
            "        -0.2103, -0.0538,  0.0456,  0.2003,  0.1179,  0.1171, -0.0843,  0.1443,\n",
            "         0.0569, -0.0731, -0.1625,  0.0444,  0.0775, -0.1153,  0.2087,  0.1919,\n",
            "         0.1921, -0.1134,  0.1621,  0.0033,  0.0521,  0.0732, -0.0178, -0.1872,\n",
            "        -0.0434, -0.0212, -0.0519, -0.1772, -0.1574, -0.1105, -0.1058, -0.1619,\n",
            "         0.1369, -0.0325,  0.0678, -0.1253,  0.0098,  0.2130, -0.1852,  0.2131,\n",
            "         0.1767,  0.2037,  0.1082, -0.1238,  0.0765, -0.0338,  0.1553,  0.0387,\n",
            "        -0.1694, -0.1258, -0.0307, -0.0230]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q53JRdWz6H3p",
        "colab_type": "code",
        "outputId": "8db2383f-839a-443c-b92d-599594d0e532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(lin1.state_dict()['weight'].shape)\n",
        "print(lin1.state_dict()['weight'])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 20])\n",
            "tensor([[ 0.0074,  0.1428,  0.0050,  ..., -0.0942,  0.1171,  0.0991],\n",
            "        [ 0.1931,  0.0506,  0.2151,  ..., -0.0898, -0.1973,  0.0324],\n",
            "        [ 0.0938,  0.1459,  0.1730,  ...,  0.0226, -0.1896,  0.0414],\n",
            "        ...,\n",
            "        [-0.0699, -0.0863, -0.1129,  ..., -0.0482, -0.0014, -0.2217],\n",
            "        [ 0.0278, -0.1371, -0.0976,  ...,  0.0028,  0.0906,  0.1877],\n",
            "        [ 0.1431, -0.1961,  0.0766,  ...,  0.1160,  0.1541, -0.1425]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD17_G0Q6N21",
        "colab_type": "code",
        "outputId": "6b76ac17-dca3-408b-a28e-871fd23481e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(lin1(emb(x).view(1, -1)).shape)\n",
        "lin1(emb(x).view(1, -1))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7641,  0.5809, -0.5649,  0.1592,  0.9268, -0.7464, -1.6303,  1.2528,\n",
              "         -0.0726,  0.8460,  0.5353,  0.7202, -0.4779,  0.3409, -0.8413, -0.2567,\n",
              "         -0.3728, -0.9532, -0.3040,  0.5332,  0.0916, -0.5901, -0.4576,  0.4552,\n",
              "         -0.6805,  0.7243, -0.3763,  0.3995, -0.1878,  1.2692, -0.3799, -0.2008,\n",
              "          1.2072,  0.1770, -0.7755,  0.4268,  0.4666,  1.6226, -1.2220,  0.9727,\n",
              "          1.2672,  0.2104,  0.2719, -1.1168,  0.5812,  0.8811, -0.4158,  0.1724,\n",
              "         -0.7037, -1.1789,  0.8150,  0.6209,  0.4016,  1.2351, -0.2459, -0.4558,\n",
              "         -0.7442,  1.6945,  0.0639,  0.1497,  0.5780, -0.4240, -0.6631, -0.3053,\n",
              "          0.7113, -0.0839, -0.3849, -0.9692,  0.0733, -0.6707, -0.4821, -1.2236,\n",
              "         -0.2444,  0.3739, -0.2464, -0.6618,  0.8411, -0.3917, -0.2027, -1.4340,\n",
              "         -0.5156,  0.0431,  0.4206, -1.3687,  0.2786,  0.8755, -0.7381,  0.3823,\n",
              "          0.4621,  0.2471,  1.1775,  1.9066, -0.7870, -0.5882, -0.6986,  0.3964,\n",
              "         -1.2402,  0.4137,  0.5156, -0.0020]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DhwHGqf6TrK",
        "colab_type": "code",
        "outputId": "f603e9f4-6d67-44ab-bef7-5afee01fd1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "relu = nn.ReLU()\n",
        "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
        "relu(lin1(emb(x).view(1, -1)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.5809, 0.0000, 0.1592, 0.9268, 0.0000, 0.0000, 1.2528, 0.0000,\n",
              "         0.8460, 0.5353, 0.7202, 0.0000, 0.3409, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.5332, 0.0916, 0.0000, 0.0000, 0.4552, 0.0000, 0.7243, 0.0000,\n",
              "         0.3995, 0.0000, 1.2692, 0.0000, 0.0000, 1.2072, 0.1770, 0.0000, 0.4268,\n",
              "         0.4666, 1.6226, 0.0000, 0.9727, 1.2672, 0.2104, 0.2719, 0.0000, 0.5812,\n",
              "         0.8811, 0.0000, 0.1724, 0.0000, 0.0000, 0.8150, 0.6209, 0.4016, 1.2351,\n",
              "         0.0000, 0.0000, 0.0000, 1.6945, 0.0639, 0.1497, 0.5780, 0.0000, 0.0000,\n",
              "         0.0000, 0.7113, 0.0000, 0.0000, 0.0000, 0.0733, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.3739, 0.0000, 0.0000, 0.8411, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0431, 0.4206, 0.0000, 0.2786, 0.8755, 0.0000, 0.3823, 0.4621, 0.2471,\n",
              "         1.1775, 1.9066, 0.0000, 0.0000, 0.0000, 0.3964, 0.0000, 0.4137, 0.5156,\n",
              "         0.0000]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcMI7Gcf6YV0",
        "colab_type": "code",
        "outputId": "5cc00ce2-e691-47c2-cf72-ffdc974a6fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
        "print(lin2.state_dict()['weight'].shape)\n",
        "lin2.state_dict()['weight']"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1388, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0585, -0.0861,  0.0670,  ..., -0.0053, -0.0603, -0.0526],\n",
              "        [-0.0318, -0.0026,  0.0374,  ..., -0.0844, -0.0262,  0.0038],\n",
              "        [-0.0712,  0.0469,  0.0186,  ..., -0.0570, -0.0348,  0.0559],\n",
              "        ...,\n",
              "        [-0.0016,  0.0802,  0.0419,  ..., -0.0291,  0.0036,  0.0146],\n",
              "        [ 0.0232,  0.0371,  0.0704,  ..., -0.0279, -0.0635, -0.0696],\n",
              "        [ 0.0015,  0.0452, -0.0857,  ..., -0.0621,  0.0760,  0.0736]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFasnpS36ciE",
        "colab_type": "code",
        "outputId": "f207e349-a476-42b6-90c9-ce62bc1482e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "h_x = relu(lin1(emb(x).view(1, -1)))\n",
        "print(lin2(h_x).shape)\n",
        "lin2(h_x)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1388])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1733, -0.1482,  0.3290,  ..., -0.5421, -0.1029, -0.5061]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_a33B3p6lrZ",
        "colab_type": "code",
        "outputId": "5e1f753a-88d8-473f-9e02-4645eca6455a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "softmax = nn.LogSoftmax(dim=1)\n",
        "softmax(lin2(h_x)).detach().numpy().tolist()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-7.463493347167969,\n",
              "  -7.438375473022461,\n",
              "  -6.961195945739746,\n",
              "  -7.765158653259277,\n",
              "  -6.978302001953125,\n",
              "  -7.149682521820068,\n",
              "  -7.314380645751953,\n",
              "  -7.30507755279541,\n",
              "  -7.2558393478393555,\n",
              "  -7.356801509857178,\n",
              "  -7.257361888885498,\n",
              "  -6.807392120361328,\n",
              "  -8.05484390258789,\n",
              "  -6.7146100997924805,\n",
              "  -7.206259250640869,\n",
              "  -7.0334153175354,\n",
              "  -7.875589370727539,\n",
              "  -6.845860958099365,\n",
              "  -7.789090156555176,\n",
              "  -7.551806449890137,\n",
              "  -7.287324905395508,\n",
              "  -6.970990180969238,\n",
              "  -7.546713829040527,\n",
              "  -7.21389102935791,\n",
              "  -7.416082859039307,\n",
              "  -7.180885314941406,\n",
              "  -7.121163368225098,\n",
              "  -6.841947078704834,\n",
              "  -6.91126823425293,\n",
              "  -7.817423343658447,\n",
              "  -7.695698261260986,\n",
              "  -6.850391387939453,\n",
              "  -7.231947422027588,\n",
              "  -7.22010612487793,\n",
              "  -6.939421653747559,\n",
              "  -7.1842474937438965,\n",
              "  -7.359320640563965,\n",
              "  -7.511264324188232,\n",
              "  -7.240150451660156,\n",
              "  -7.792934417724609,\n",
              "  -7.200679779052734,\n",
              "  -7.102390289306641,\n",
              "  -6.85539436340332,\n",
              "  -7.385459899902344,\n",
              "  -7.115449905395508,\n",
              "  -6.9741315841674805,\n",
              "  -7.236946105957031,\n",
              "  -7.480724334716797,\n",
              "  -7.3725738525390625,\n",
              "  -6.756231307983398,\n",
              "  -7.849459648132324,\n",
              "  -7.2465715408325195,\n",
              "  -7.4336018562316895,\n",
              "  -7.008065223693848,\n",
              "  -7.300927639007568,\n",
              "  -7.120304107666016,\n",
              "  -7.147802352905273,\n",
              "  -7.821254730224609,\n",
              "  -7.008895397186279,\n",
              "  -7.496881484985352,\n",
              "  -7.558655738830566,\n",
              "  -7.820375919342041,\n",
              "  -7.104543209075928,\n",
              "  -7.138918876647949,\n",
              "  -6.737453460693359,\n",
              "  -7.61199426651001,\n",
              "  -7.281118869781494,\n",
              "  -7.058661460876465,\n",
              "  -7.244933128356934,\n",
              "  -7.17562198638916,\n",
              "  -7.202343940734863,\n",
              "  -7.178559303283691,\n",
              "  -7.019768714904785,\n",
              "  -7.618287563323975,\n",
              "  -7.787583827972412,\n",
              "  -7.0306596755981445,\n",
              "  -7.000456809997559,\n",
              "  -7.8653388023376465,\n",
              "  -7.106421947479248,\n",
              "  -7.050431728363037,\n",
              "  -7.118996620178223,\n",
              "  -7.222893714904785,\n",
              "  -7.0141825675964355,\n",
              "  -7.777153968811035,\n",
              "  -7.27530574798584,\n",
              "  -7.2490949630737305,\n",
              "  -7.38680362701416,\n",
              "  -7.693150043487549,\n",
              "  -6.673583507537842,\n",
              "  -7.653299331665039,\n",
              "  -7.262910842895508,\n",
              "  -7.828359603881836,\n",
              "  -7.553128242492676,\n",
              "  -7.5879082679748535,\n",
              "  -7.0548014640808105,\n",
              "  -6.782792568206787,\n",
              "  -7.326785087585449,\n",
              "  -6.964470863342285,\n",
              "  -6.959845542907715,\n",
              "  -7.357907295227051,\n",
              "  -7.374415874481201,\n",
              "  -7.173901081085205,\n",
              "  -6.8672943115234375,\n",
              "  -6.9293622970581055,\n",
              "  -6.747613906860352,\n",
              "  -7.344166278839111,\n",
              "  -7.379401683807373,\n",
              "  -7.137741565704346,\n",
              "  -7.044472694396973,\n",
              "  -7.392157077789307,\n",
              "  -7.472403526306152,\n",
              "  -7.171188831329346,\n",
              "  -7.138036251068115,\n",
              "  -7.155738353729248,\n",
              "  -7.179591655731201,\n",
              "  -7.062777519226074,\n",
              "  -7.7301812171936035,\n",
              "  -7.476185321807861,\n",
              "  -7.773683547973633,\n",
              "  -7.3311591148376465,\n",
              "  -7.868940353393555,\n",
              "  -7.708112716674805,\n",
              "  -7.233226299285889,\n",
              "  -7.692890644073486,\n",
              "  -6.870965480804443,\n",
              "  -7.134681224822998,\n",
              "  -7.202875137329102,\n",
              "  -6.7505035400390625,\n",
              "  -7.18617057800293,\n",
              "  -7.0396833419799805,\n",
              "  -7.171091556549072,\n",
              "  -7.253530502319336,\n",
              "  -7.2856669425964355,\n",
              "  -7.7986931800842285,\n",
              "  -7.377006530761719,\n",
              "  -6.893314361572266,\n",
              "  -7.597390174865723,\n",
              "  -7.25639009475708,\n",
              "  -7.531867504119873,\n",
              "  -7.270462512969971,\n",
              "  -7.312973499298096,\n",
              "  -6.901711940765381,\n",
              "  -7.200968265533447,\n",
              "  -7.2946248054504395,\n",
              "  -7.398442268371582,\n",
              "  -7.403077602386475,\n",
              "  -7.840123653411865,\n",
              "  -7.107517719268799,\n",
              "  -6.973400592803955,\n",
              "  -7.012897968292236,\n",
              "  -7.533697128295898,\n",
              "  -7.355706691741943,\n",
              "  -8.207721710205078,\n",
              "  -7.22355842590332,\n",
              "  -7.461366176605225,\n",
              "  -6.952611446380615,\n",
              "  -7.365489959716797,\n",
              "  -7.0469231605529785,\n",
              "  -7.406105995178223,\n",
              "  -7.143924713134766,\n",
              "  -7.237369537353516,\n",
              "  -7.164935111999512,\n",
              "  -7.314244747161865,\n",
              "  -7.288265228271484,\n",
              "  -7.159487724304199,\n",
              "  -7.142762184143066,\n",
              "  -7.752956390380859,\n",
              "  -7.103923797607422,\n",
              "  -7.616300582885742,\n",
              "  -7.510287761688232,\n",
              "  -6.899817943572998,\n",
              "  -6.953014373779297,\n",
              "  -7.192216873168945,\n",
              "  -7.395026206970215,\n",
              "  -7.691954612731934,\n",
              "  -7.6453962326049805,\n",
              "  -6.792098522186279,\n",
              "  -7.2143940925598145,\n",
              "  -6.99188232421875,\n",
              "  -7.297379970550537,\n",
              "  -7.654853343963623,\n",
              "  -7.43466329574585,\n",
              "  -6.917036533355713,\n",
              "  -6.998756408691406,\n",
              "  -6.842922687530518,\n",
              "  -6.603637218475342,\n",
              "  -7.0693206787109375,\n",
              "  -7.225489139556885,\n",
              "  -7.12615966796875,\n",
              "  -7.251601219177246,\n",
              "  -7.350136756896973,\n",
              "  -7.497532367706299,\n",
              "  -6.4745564460754395,\n",
              "  -7.555307388305664,\n",
              "  -6.9624409675598145,\n",
              "  -7.4354166984558105,\n",
              "  -7.349552154541016,\n",
              "  -7.208213806152344,\n",
              "  -7.405025482177734,\n",
              "  -7.3217315673828125,\n",
              "  -7.1609673500061035,\n",
              "  -7.461496353149414,\n",
              "  -6.989336013793945,\n",
              "  -7.068872451782227,\n",
              "  -6.941628932952881,\n",
              "  -6.902873992919922,\n",
              "  -6.753816604614258,\n",
              "  -7.21022891998291,\n",
              "  -6.9758734703063965,\n",
              "  -7.472157001495361,\n",
              "  -7.548191547393799,\n",
              "  -7.205769062042236,\n",
              "  -7.347873210906982,\n",
              "  -7.120328426361084,\n",
              "  -6.963499069213867,\n",
              "  -7.9145612716674805,\n",
              "  -7.196330547332764,\n",
              "  -7.1193671226501465,\n",
              "  -7.624668598175049,\n",
              "  -7.307946681976318,\n",
              "  -7.031637191772461,\n",
              "  -7.628838539123535,\n",
              "  -7.486627101898193,\n",
              "  -6.9360222816467285,\n",
              "  -7.636354446411133,\n",
              "  -7.67799186706543,\n",
              "  -6.9956183433532715,\n",
              "  -6.871026039123535,\n",
              "  -7.916067123413086,\n",
              "  -7.0491623878479,\n",
              "  -7.541826248168945,\n",
              "  -7.144245624542236,\n",
              "  -7.474897861480713,\n",
              "  -7.031883239746094,\n",
              "  -7.478883266448975,\n",
              "  -7.13798189163208,\n",
              "  -7.291932582855225,\n",
              "  -7.350024700164795,\n",
              "  -7.5488762855529785,\n",
              "  -7.035224437713623,\n",
              "  -7.357210159301758,\n",
              "  -6.67771053314209,\n",
              "  -6.878348350524902,\n",
              "  -7.5754313468933105,\n",
              "  -7.194005489349365,\n",
              "  -7.023861885070801,\n",
              "  -7.091107368469238,\n",
              "  -7.667536735534668,\n",
              "  -7.945000648498535,\n",
              "  -7.600454330444336,\n",
              "  -7.375375747680664,\n",
              "  -6.625588417053223,\n",
              "  -7.049141883850098,\n",
              "  -7.510447978973389,\n",
              "  -7.406316757202148,\n",
              "  -7.623081207275391,\n",
              "  -7.068115711212158,\n",
              "  -6.768925666809082,\n",
              "  -7.1046013832092285,\n",
              "  -7.434452533721924,\n",
              "  -7.402369499206543,\n",
              "  -7.212270736694336,\n",
              "  -7.378073692321777,\n",
              "  -7.460097789764404,\n",
              "  -7.38104772567749,\n",
              "  -6.8865251541137695,\n",
              "  -7.729854583740234,\n",
              "  -7.495132923126221,\n",
              "  -7.609447956085205,\n",
              "  -7.175901412963867,\n",
              "  -6.961245536804199,\n",
              "  -7.045045375823975,\n",
              "  -7.216825485229492,\n",
              "  -7.319109916687012,\n",
              "  -7.070388317108154,\n",
              "  -7.546260356903076,\n",
              "  -7.383783340454102,\n",
              "  -7.371052265167236,\n",
              "  -7.542187690734863,\n",
              "  -7.556950569152832,\n",
              "  -7.491156578063965,\n",
              "  -6.8373517990112305,\n",
              "  -7.388033390045166,\n",
              "  -7.406482219696045,\n",
              "  -7.463728904724121,\n",
              "  -7.346934795379639,\n",
              "  -7.137772560119629,\n",
              "  -6.635529518127441,\n",
              "  -7.373965740203857,\n",
              "  -7.055889129638672,\n",
              "  -7.127965927124023,\n",
              "  -7.143039703369141,\n",
              "  -7.375695705413818,\n",
              "  -7.259061813354492,\n",
              "  -6.977392196655273,\n",
              "  -7.333556175231934,\n",
              "  -7.283953666687012,\n",
              "  -7.744078159332275,\n",
              "  -7.578771591186523,\n",
              "  -7.522561073303223,\n",
              "  -7.721421241760254,\n",
              "  -7.128169536590576,\n",
              "  -7.4183244705200195,\n",
              "  -7.365483283996582,\n",
              "  -7.359050750732422,\n",
              "  -7.33414888381958,\n",
              "  -6.875057697296143,\n",
              "  -6.9739508628845215,\n",
              "  -7.391720294952393,\n",
              "  -7.234372138977051,\n",
              "  -7.047097206115723,\n",
              "  -7.922665596008301,\n",
              "  -6.917558670043945,\n",
              "  -7.368692398071289,\n",
              "  -7.2523674964904785,\n",
              "  -7.325550079345703,\n",
              "  -7.22748327255249,\n",
              "  -7.321302890777588,\n",
              "  -7.386417388916016,\n",
              "  -7.183684349060059,\n",
              "  -7.432862281799316,\n",
              "  -6.559239864349365,\n",
              "  -7.561830520629883,\n",
              "  -6.992343425750732,\n",
              "  -7.06934928894043,\n",
              "  -6.767911911010742,\n",
              "  -7.253117561340332,\n",
              "  -7.283669948577881,\n",
              "  -6.894247055053711,\n",
              "  -7.2427287101745605,\n",
              "  -7.465025901794434,\n",
              "  -6.965750694274902,\n",
              "  -7.4315104484558105,\n",
              "  -6.799746513366699,\n",
              "  -6.730620384216309,\n",
              "  -7.166062355041504,\n",
              "  -6.892877578735352,\n",
              "  -6.839114189147949,\n",
              "  -7.403127670288086,\n",
              "  -7.031978607177734,\n",
              "  -7.576189994812012,\n",
              "  -7.476875305175781,\n",
              "  -7.202317237854004,\n",
              "  -7.366002559661865,\n",
              "  -7.560932636260986,\n",
              "  -7.667208194732666,\n",
              "  -7.277419090270996,\n",
              "  -7.2133989334106445,\n",
              "  -7.284456253051758,\n",
              "  -7.38515043258667,\n",
              "  -7.3338212966918945,\n",
              "  -7.310588359832764,\n",
              "  -7.019652843475342,\n",
              "  -7.128084182739258,\n",
              "  -7.5275983810424805,\n",
              "  -8.100265502929688,\n",
              "  -7.3257222175598145,\n",
              "  -7.8876118659973145,\n",
              "  -7.639954566955566,\n",
              "  -7.266539573669434,\n",
              "  -6.867953777313232,\n",
              "  -7.02204704284668,\n",
              "  -7.44001579284668,\n",
              "  -7.691196441650391,\n",
              "  -7.45913028717041,\n",
              "  -6.740984916687012,\n",
              "  -7.425634860992432,\n",
              "  -7.477519989013672,\n",
              "  -7.593584060668945,\n",
              "  -7.343087196350098,\n",
              "  -7.477219104766846,\n",
              "  -7.569967269897461,\n",
              "  -7.872014045715332,\n",
              "  -7.0741963386535645,\n",
              "  -7.014440536499023,\n",
              "  -7.188746452331543,\n",
              "  -6.734218120574951,\n",
              "  -7.432697296142578,\n",
              "  -7.38340950012207,\n",
              "  -6.781013488769531,\n",
              "  -7.260248184204102,\n",
              "  -7.765322208404541,\n",
              "  -7.5684614181518555,\n",
              "  -6.548478126525879,\n",
              "  -7.540448188781738,\n",
              "  -7.267685890197754,\n",
              "  -7.215649127960205,\n",
              "  -7.544877529144287,\n",
              "  -7.368700981140137,\n",
              "  -7.528255939483643,\n",
              "  -7.003602504730225,\n",
              "  -7.197500705718994,\n",
              "  -6.911508560180664,\n",
              "  -7.3784894943237305,\n",
              "  -7.104753017425537,\n",
              "  -7.1283040046691895,\n",
              "  -6.803983211517334,\n",
              "  -7.784985542297363,\n",
              "  -7.197536945343018,\n",
              "  -7.033973693847656,\n",
              "  -6.957037925720215,\n",
              "  -6.810218334197998,\n",
              "  -6.713581562042236,\n",
              "  -7.886354446411133,\n",
              "  -7.380492687225342,\n",
              "  -7.252155303955078,\n",
              "  -6.896421432495117,\n",
              "  -7.023872375488281,\n",
              "  -7.302888870239258,\n",
              "  -7.4726786613464355,\n",
              "  -7.753908157348633,\n",
              "  -7.633281707763672,\n",
              "  -7.375436782836914,\n",
              "  -7.025136470794678,\n",
              "  -7.196145057678223,\n",
              "  -7.089681625366211,\n",
              "  -7.370871067047119,\n",
              "  -7.048311710357666,\n",
              "  -7.465620994567871,\n",
              "  -7.843570709228516,\n",
              "  -7.3282976150512695,\n",
              "  -7.463898658752441,\n",
              "  -6.840935230255127,\n",
              "  -7.2293195724487305,\n",
              "  -7.222826957702637,\n",
              "  -7.65601921081543,\n",
              "  -7.542288780212402,\n",
              "  -7.5413312911987305,\n",
              "  -7.7551774978637695,\n",
              "  -7.914539337158203,\n",
              "  -7.869945049285889,\n",
              "  -7.723451614379883,\n",
              "  -6.686038970947266,\n",
              "  -7.086874008178711,\n",
              "  -7.502294540405273,\n",
              "  -7.022478103637695,\n",
              "  -7.263708114624023,\n",
              "  -7.182791233062744,\n",
              "  -7.481047630310059,\n",
              "  -7.456085205078125,\n",
              "  -7.825048446655273,\n",
              "  -7.826809883117676,\n",
              "  -6.914222240447998,\n",
              "  -7.16887903213501,\n",
              "  -6.964430809020996,\n",
              "  -7.230640411376953,\n",
              "  -7.4885640144348145,\n",
              "  -7.009417533874512,\n",
              "  -7.451727390289307,\n",
              "  -7.400079250335693,\n",
              "  -6.763363838195801,\n",
              "  -7.784334182739258,\n",
              "  -6.963548183441162,\n",
              "  -7.776491165161133,\n",
              "  -7.199227333068848,\n",
              "  -6.680545330047607,\n",
              "  -7.240113735198975,\n",
              "  -7.472684860229492,\n",
              "  -7.652699947357178,\n",
              "  -7.164513111114502,\n",
              "  -7.346218109130859,\n",
              "  -6.85269832611084,\n",
              "  -7.129302024841309,\n",
              "  -7.687620639801025,\n",
              "  -7.523027420043945,\n",
              "  -8.148337364196777,\n",
              "  -7.309760570526123,\n",
              "  -6.697824478149414,\n",
              "  -7.699812412261963,\n",
              "  -7.006423473358154,\n",
              "  -6.752769470214844,\n",
              "  -7.39114236831665,\n",
              "  -6.827472686767578,\n",
              "  -7.169796466827393,\n",
              "  -7.518702983856201,\n",
              "  -7.130304336547852,\n",
              "  -6.759909152984619,\n",
              "  -6.855745792388916,\n",
              "  -7.513348579406738,\n",
              "  -7.508898735046387,\n",
              "  -7.196051597595215,\n",
              "  -7.043327331542969,\n",
              "  -6.949872970581055,\n",
              "  -7.423630714416504,\n",
              "  -7.246546268463135,\n",
              "  -7.132580757141113,\n",
              "  -7.041750907897949,\n",
              "  -7.137368202209473,\n",
              "  -6.886255741119385,\n",
              "  -6.955747604370117,\n",
              "  -7.9599761962890625,\n",
              "  -6.720386981964111,\n",
              "  -7.223137378692627,\n",
              "  -7.7873382568359375,\n",
              "  -7.510684967041016,\n",
              "  -7.150826454162598,\n",
              "  -7.640384197235107,\n",
              "  -7.912744522094727,\n",
              "  -6.862428188323975,\n",
              "  -6.901447772979736,\n",
              "  -7.031596660614014,\n",
              "  -7.253159046173096,\n",
              "  -6.442063331604004,\n",
              "  -7.289004802703857,\n",
              "  -7.772626876831055,\n",
              "  -7.315606117248535,\n",
              "  -7.093564510345459,\n",
              "  -7.308586597442627,\n",
              "  -7.320427894592285,\n",
              "  -7.581097602844238,\n",
              "  -7.702747344970703,\n",
              "  -6.812370777130127,\n",
              "  -7.208285808563232,\n",
              "  -7.530450344085693,\n",
              "  -7.693711757659912,\n",
              "  -7.39442253112793,\n",
              "  -7.333406925201416,\n",
              "  -7.547106742858887,\n",
              "  -7.21304988861084,\n",
              "  -7.066915512084961,\n",
              "  -7.520910263061523,\n",
              "  -6.839107513427734,\n",
              "  -8.040471076965332,\n",
              "  -7.676504135131836,\n",
              "  -7.219448089599609,\n",
              "  -7.299658298492432,\n",
              "  -7.236968517303467,\n",
              "  -7.399374485015869,\n",
              "  -7.248579025268555,\n",
              "  -6.978919506072998,\n",
              "  -6.556662082672119,\n",
              "  -7.50800895690918,\n",
              "  -7.729488849639893,\n",
              "  -6.860868453979492,\n",
              "  -7.103837966918945,\n",
              "  -7.400275230407715,\n",
              "  -7.156091690063477,\n",
              "  -7.083510398864746,\n",
              "  -7.649496078491211,\n",
              "  -7.46047306060791,\n",
              "  -7.386013507843018,\n",
              "  -7.252106666564941,\n",
              "  -7.897632122039795,\n",
              "  -6.581887245178223,\n",
              "  -7.666158676147461,\n",
              "  -6.809419631958008,\n",
              "  -7.3867573738098145,\n",
              "  -8.12331485748291,\n",
              "  -6.82977294921875,\n",
              "  -7.388978004455566,\n",
              "  -7.709277153015137,\n",
              "  -7.3752522468566895,\n",
              "  -7.6773176193237305,\n",
              "  -7.385434150695801,\n",
              "  -7.582111835479736,\n",
              "  -7.444537162780762,\n",
              "  -7.154150485992432,\n",
              "  -6.955228328704834,\n",
              "  -7.351147174835205,\n",
              "  -7.26596736907959,\n",
              "  -7.698168754577637,\n",
              "  -7.2187180519104,\n",
              "  -7.168417453765869,\n",
              "  -7.162919044494629,\n",
              "  -7.466582298278809,\n",
              "  -7.343790531158447,\n",
              "  -7.038904190063477,\n",
              "  -6.616886615753174,\n",
              "  -7.270917892456055,\n",
              "  -7.200280666351318,\n",
              "  -7.234138488769531,\n",
              "  -7.290305137634277,\n",
              "  -7.765542507171631,\n",
              "  -7.307897567749023,\n",
              "  -7.201444625854492,\n",
              "  -7.6393938064575195,\n",
              "  -7.825702667236328,\n",
              "  -7.451388835906982,\n",
              "  -6.801947593688965,\n",
              "  -7.191102027893066,\n",
              "  -7.061628818511963,\n",
              "  -7.907280445098877,\n",
              "  -7.670230388641357,\n",
              "  -6.974636554718018,\n",
              "  -7.246468544006348,\n",
              "  -7.374460697174072,\n",
              "  -7.30464506149292,\n",
              "  -7.463682174682617,\n",
              "  -6.94228982925415,\n",
              "  -7.413424015045166,\n",
              "  -7.769645690917969,\n",
              "  -7.194056987762451,\n",
              "  -7.672894477844238,\n",
              "  -7.150882720947266,\n",
              "  -7.460211277008057,\n",
              "  -7.207243919372559,\n",
              "  -7.469502925872803,\n",
              "  -7.171518325805664,\n",
              "  -6.7225847244262695,\n",
              "  -7.068103790283203,\n",
              "  -7.7388811111450195,\n",
              "  -8.087910652160645,\n",
              "  -7.385497093200684,\n",
              "  -7.021806240081787,\n",
              "  -7.284727096557617,\n",
              "  -7.534544467926025,\n",
              "  -7.271401882171631,\n",
              "  -6.932488441467285,\n",
              "  -7.754761219024658,\n",
              "  -7.5779852867126465,\n",
              "  -7.24934196472168,\n",
              "  -7.218743324279785,\n",
              "  -7.431891441345215,\n",
              "  -7.101352691650391,\n",
              "  -7.195137977600098,\n",
              "  -7.045048236846924,\n",
              "  -7.460363388061523,\n",
              "  -7.826466083526611,\n",
              "  -6.980077743530273,\n",
              "  -6.773653984069824,\n",
              "  -7.535569190979004,\n",
              "  -6.9749226570129395,\n",
              "  -7.450278282165527,\n",
              "  -7.273666858673096,\n",
              "  -7.809105396270752,\n",
              "  -7.522916316986084,\n",
              "  -7.09970760345459,\n",
              "  -7.69372034072876,\n",
              "  -7.109872817993164,\n",
              "  -7.00416374206543,\n",
              "  -7.255476951599121,\n",
              "  -7.4614996910095215,\n",
              "  -7.375340938568115,\n",
              "  -7.782185077667236,\n",
              "  -6.867380142211914,\n",
              "  -7.542794704437256,\n",
              "  -7.532525539398193,\n",
              "  -7.636120796203613,\n",
              "  -6.800053119659424,\n",
              "  -7.156777381896973,\n",
              "  -7.410533905029297,\n",
              "  -6.85624885559082,\n",
              "  -7.172935485839844,\n",
              "  -7.357029438018799,\n",
              "  -7.10047721862793,\n",
              "  -7.208697319030762,\n",
              "  -7.245827674865723,\n",
              "  -7.065162658691406,\n",
              "  -6.924765586853027,\n",
              "  -7.527285575866699,\n",
              "  -7.818431854248047,\n",
              "  -6.7939910888671875,\n",
              "  -7.4035420417785645,\n",
              "  -7.473734378814697,\n",
              "  -7.200863361358643,\n",
              "  -7.3792572021484375,\n",
              "  -7.121086597442627,\n",
              "  -7.189033508300781,\n",
              "  -7.203441619873047,\n",
              "  -7.136812686920166,\n",
              "  -7.316490173339844,\n",
              "  -7.215696334838867,\n",
              "  -7.422266960144043,\n",
              "  -7.671635627746582,\n",
              "  -7.335607051849365,\n",
              "  -7.384064674377441,\n",
              "  -7.465638160705566,\n",
              "  -7.385339736938477,\n",
              "  -7.247809886932373,\n",
              "  -6.990006446838379,\n",
              "  -7.305384635925293,\n",
              "  -7.11320161819458,\n",
              "  -7.820596694946289,\n",
              "  -7.387997150421143,\n",
              "  -7.554930210113525,\n",
              "  -7.6753458976745605,\n",
              "  -7.287437915802002,\n",
              "  -7.075037956237793,\n",
              "  -6.783634662628174,\n",
              "  -6.918795585632324,\n",
              "  -7.016332149505615,\n",
              "  -7.268725395202637,\n",
              "  -7.511166095733643,\n",
              "  -7.520188808441162,\n",
              "  -7.531901836395264,\n",
              "  -7.260549068450928,\n",
              "  -7.523439884185791,\n",
              "  -7.988223552703857,\n",
              "  -7.251490592956543,\n",
              "  -6.5955095291137695,\n",
              "  -7.706151962280273,\n",
              "  -7.94237756729126,\n",
              "  -6.915739059448242,\n",
              "  -6.689903736114502,\n",
              "  -7.291583061218262,\n",
              "  -7.337181091308594,\n",
              "  -7.214626789093018,\n",
              "  -7.803559303283691,\n",
              "  -7.169351577758789,\n",
              "  -7.2497239112854,\n",
              "  -7.485851764678955,\n",
              "  -7.211061954498291,\n",
              "  -6.995943546295166,\n",
              "  -7.30237340927124,\n",
              "  -7.2794575691223145,\n",
              "  -7.765194416046143,\n",
              "  -7.416640758514404,\n",
              "  -7.037232875823975,\n",
              "  -7.505080699920654,\n",
              "  -7.096313953399658,\n",
              "  -7.486171722412109,\n",
              "  -7.497026443481445,\n",
              "  -7.3115949630737305,\n",
              "  -7.394089698791504,\n",
              "  -6.8796467781066895,\n",
              "  -7.665348529815674,\n",
              "  -7.902761936187744,\n",
              "  -7.5634446144104,\n",
              "  -6.977944374084473,\n",
              "  -7.051672458648682,\n",
              "  -7.281638145446777,\n",
              "  -7.121528625488281,\n",
              "  -7.466078758239746,\n",
              "  -7.020357608795166,\n",
              "  -7.639021873474121,\n",
              "  -7.102049827575684,\n",
              "  -6.846623420715332,\n",
              "  -7.571051120758057,\n",
              "  -7.517241954803467,\n",
              "  -7.271603584289551,\n",
              "  -6.862733840942383,\n",
              "  -7.734559535980225,\n",
              "  -7.3590087890625,\n",
              "  -7.192679405212402,\n",
              "  -7.277522087097168,\n",
              "  -7.650908470153809,\n",
              "  -7.421136856079102,\n",
              "  -7.382895469665527,\n",
              "  -7.228873252868652,\n",
              "  -7.247734546661377,\n",
              "  -7.690187454223633,\n",
              "  -7.550308704376221,\n",
              "  -7.190132141113281,\n",
              "  -6.892293453216553,\n",
              "  -7.485289573669434,\n",
              "  -7.279441833496094,\n",
              "  -7.295103073120117,\n",
              "  -7.355175971984863,\n",
              "  -7.397151470184326,\n",
              "  -7.054955005645752,\n",
              "  -7.143435955047607,\n",
              "  -6.866085052490234,\n",
              "  -6.979989051818848,\n",
              "  -7.347173690795898,\n",
              "  -7.032512664794922,\n",
              "  -7.109068393707275,\n",
              "  -7.515644073486328,\n",
              "  -7.390308380126953,\n",
              "  -7.159261703491211,\n",
              "  -7.516518592834473,\n",
              "  -7.475310325622559,\n",
              "  -6.81953239440918,\n",
              "  -7.047779083251953,\n",
              "  -7.561697006225586,\n",
              "  -7.142817497253418,\n",
              "  -7.562234878540039,\n",
              "  -7.462007522583008,\n",
              "  -7.288938999176025,\n",
              "  -7.464788913726807,\n",
              "  -7.18983268737793,\n",
              "  -7.569079875946045,\n",
              "  -6.7405924797058105,\n",
              "  -7.067220211029053,\n",
              "  -7.333359718322754,\n",
              "  -7.045359134674072,\n",
              "  -7.6801934242248535,\n",
              "  -7.328033924102783,\n",
              "  -7.354582786560059,\n",
              "  -6.819229602813721,\n",
              "  -7.08306360244751,\n",
              "  -6.543639183044434,\n",
              "  -7.3458709716796875,\n",
              "  -7.215846538543701,\n",
              "  -7.035260200500488,\n",
              "  -7.71978759765625,\n",
              "  -7.2213311195373535,\n",
              "  -6.958137512207031,\n",
              "  -7.9894866943359375,\n",
              "  -7.345939636230469,\n",
              "  -7.357888698577881,\n",
              "  -7.17226505279541,\n",
              "  -6.74935245513916,\n",
              "  -7.539416313171387,\n",
              "  -7.0120439529418945,\n",
              "  -7.548070430755615,\n",
              "  -7.718864440917969,\n",
              "  -7.449709892272949,\n",
              "  -7.394474029541016,\n",
              "  -7.3812103271484375,\n",
              "  -7.1434149742126465,\n",
              "  -6.946884632110596,\n",
              "  -6.867218971252441,\n",
              "  -6.888874530792236,\n",
              "  -7.685413360595703,\n",
              "  -7.93300724029541,\n",
              "  -7.288027286529541,\n",
              "  -7.445416450500488,\n",
              "  -6.934222221374512,\n",
              "  -7.395867824554443,\n",
              "  -7.248863220214844,\n",
              "  -6.948991298675537,\n",
              "  -7.327219009399414,\n",
              "  -7.248800277709961,\n",
              "  -7.023622512817383,\n",
              "  -7.775052070617676,\n",
              "  -7.205339431762695,\n",
              "  -7.561794281005859,\n",
              "  -7.108441352844238,\n",
              "  -7.3761467933654785,\n",
              "  -6.803365707397461,\n",
              "  -7.84288215637207,\n",
              "  -7.469142913818359,\n",
              "  -6.949735164642334,\n",
              "  -7.159811496734619,\n",
              "  -7.362681865692139,\n",
              "  -7.6310319900512695,\n",
              "  -7.33974027633667,\n",
              "  -7.206213474273682,\n",
              "  -7.288674354553223,\n",
              "  -7.139737129211426,\n",
              "  -7.289268970489502,\n",
              "  -7.901342391967773,\n",
              "  -7.265974998474121,\n",
              "  -7.3569111824035645,\n",
              "  -7.200924396514893,\n",
              "  -7.217719078063965,\n",
              "  -6.801962375640869,\n",
              "  -7.405453205108643,\n",
              "  -6.846510410308838,\n",
              "  -6.973386764526367,\n",
              "  -7.434186935424805,\n",
              "  -6.975584983825684,\n",
              "  -7.343689918518066,\n",
              "  -7.527308940887451,\n",
              "  -7.70799446105957,\n",
              "  -7.315596103668213,\n",
              "  -7.751266002655029,\n",
              "  -7.193542957305908,\n",
              "  -7.267819881439209,\n",
              "  -7.149858474731445,\n",
              "  -7.424907684326172,\n",
              "  -7.234642028808594,\n",
              "  -6.956216335296631,\n",
              "  -7.517362594604492,\n",
              "  -7.144722938537598,\n",
              "  -6.928101062774658,\n",
              "  -6.889483451843262,\n",
              "  -7.641043186187744,\n",
              "  -7.569342613220215,\n",
              "  -6.878028869628906,\n",
              "  -7.052603244781494,\n",
              "  -7.271040439605713,\n",
              "  -7.261992931365967,\n",
              "  -7.534884929656982,\n",
              "  -7.415406703948975,\n",
              "  -6.709035873413086,\n",
              "  -7.407715320587158,\n",
              "  -7.363073825836182,\n",
              "  -7.393095970153809,\n",
              "  -7.382219314575195,\n",
              "  -7.563155174255371,\n",
              "  -6.8849616050720215,\n",
              "  -7.436952114105225,\n",
              "  -7.311860084533691,\n",
              "  -7.2010111808776855,\n",
              "  -7.406111717224121,\n",
              "  -6.880331039428711,\n",
              "  -7.677973747253418,\n",
              "  -6.888676643371582,\n",
              "  -6.695704460144043,\n",
              "  -7.243399620056152,\n",
              "  -6.942949295043945,\n",
              "  -7.439328193664551,\n",
              "  -6.924553394317627,\n",
              "  -7.13469123840332,\n",
              "  -7.011948108673096,\n",
              "  -7.18181848526001,\n",
              "  -6.958261489868164,\n",
              "  -7.110024929046631,\n",
              "  -6.95598840713501,\n",
              "  -7.187394618988037,\n",
              "  -7.261474609375,\n",
              "  -7.697605609893799,\n",
              "  -7.1684136390686035,\n",
              "  -7.098144054412842,\n",
              "  -7.550232410430908,\n",
              "  -7.883556365966797,\n",
              "  -7.500676155090332,\n",
              "  -6.8427839279174805,\n",
              "  -7.360500812530518,\n",
              "  -7.39555025100708,\n",
              "  -7.559286594390869,\n",
              "  -7.426767826080322,\n",
              "  -7.208434104919434,\n",
              "  -7.153626918792725,\n",
              "  -7.5381269454956055,\n",
              "  -7.6462202072143555,\n",
              "  -7.708051681518555,\n",
              "  -7.3315534591674805,\n",
              "  -7.064418792724609,\n",
              "  -7.131505012512207,\n",
              "  -7.180689334869385,\n",
              "  -7.226266860961914,\n",
              "  -7.296131610870361,\n",
              "  -7.44865608215332,\n",
              "  -7.83114767074585,\n",
              "  -7.786620140075684,\n",
              "  -7.286041736602783,\n",
              "  -7.3835368156433105,\n",
              "  -6.68330192565918,\n",
              "  -6.878483772277832,\n",
              "  -7.277186870574951,\n",
              "  -6.982109069824219,\n",
              "  -6.768312931060791,\n",
              "  -7.21458625793457,\n",
              "  -7.417262077331543,\n",
              "  -7.031683921813965,\n",
              "  -7.158539295196533,\n",
              "  -7.134244918823242,\n",
              "  -7.33216667175293,\n",
              "  -7.610998630523682,\n",
              "  -7.0136399269104,\n",
              "  -7.834059715270996,\n",
              "  -7.55146598815918,\n",
              "  -7.175351142883301,\n",
              "  -7.3081278800964355,\n",
              "  -6.983748435974121,\n",
              "  -7.292247295379639,\n",
              "  -8.071723937988281,\n",
              "  -7.323277473449707,\n",
              "  -7.379400730133057,\n",
              "  -7.0037312507629395,\n",
              "  -7.614428997039795,\n",
              "  -7.171125411987305,\n",
              "  -6.830499649047852,\n",
              "  -7.04484748840332,\n",
              "  -6.993005752563477,\n",
              "  -7.363426685333252,\n",
              "  -7.109523773193359,\n",
              "  -7.271751880645752,\n",
              "  -7.1889448165893555,\n",
              "  -7.078743934631348,\n",
              "  -7.798337936401367,\n",
              "  -7.536113739013672,\n",
              "  -6.8159403800964355,\n",
              "  -7.16132116317749,\n",
              "  -7.160096645355225,\n",
              "  -7.110551834106445,\n",
              "  -7.171254634857178,\n",
              "  -8.124130249023438,\n",
              "  -7.9913177490234375,\n",
              "  -7.54184103012085,\n",
              "  -7.197051048278809,\n",
              "  -6.500443458557129,\n",
              "  -7.047937393188477,\n",
              "  -7.670326232910156,\n",
              "  -6.96941614151001,\n",
              "  -7.573372840881348,\n",
              "  -7.501349449157715,\n",
              "  -7.518294811248779,\n",
              "  -6.821398735046387,\n",
              "  -7.542976379394531,\n",
              "  -7.0813422203063965,\n",
              "  -7.303495407104492,\n",
              "  -7.319398403167725,\n",
              "  -7.03607177734375,\n",
              "  -7.185543537139893,\n",
              "  -7.407401084899902,\n",
              "  -7.396921634674072,\n",
              "  -7.250539779663086,\n",
              "  -7.219648361206055,\n",
              "  -7.157538414001465,\n",
              "  -7.0543341636657715,\n",
              "  -7.419567584991455,\n",
              "  -6.620712757110596,\n",
              "  -7.2598443031311035,\n",
              "  -7.282869815826416,\n",
              "  -7.712253570556641,\n",
              "  -7.48032283782959,\n",
              "  -7.5181884765625,\n",
              "  -7.3376359939575195,\n",
              "  -7.662539482116699,\n",
              "  -6.937165260314941,\n",
              "  -7.074670791625977,\n",
              "  -7.080746650695801,\n",
              "  -7.540456771850586,\n",
              "  -6.915660381317139,\n",
              "  -6.843343257904053,\n",
              "  -7.30100679397583,\n",
              "  -7.374708652496338,\n",
              "  ...]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtuFjTH_6p5D",
        "colab_type": "code",
        "outputId": "b6f7237c-5ec8-4eb0-f098-4f9bf5e00416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Select the index with highest softmax probabilities\n",
        "# See https://pytorch.org/docs/stable/torch.html#torch.max\n",
        "torch.max(softmax(lin2(h_x)), 1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([-6.4421], grad_fn=<MaxBackward0>), indices=tensor([502]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcfWwZk67rG",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-train-cbow\"></a>\n",
        "\n",
        "# Now, we train the CBOW model for real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRig9kyI61Xn",
        "colab_type": "code",
        "outputId": "7c0fe972-0acc-4a73-996b-a405baf1c905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First we split the data into training and testing.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
        "len(tokenized_text_train), len(tokenized_text_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfzjsjoq7Cvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor, autograd\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Put the input context indices into the embeddings\n",
        "        # then squeeze it into a single dimension vector with tensor.view((1,-1))\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        # Put the embedding input through linear layer,\n",
        "        # then an activation function to create the hidden layer.\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        # Put the hidden layer through a second linear layer,\n",
        "        out = self.linear2(hid)\n",
        "        # then a last layer activation function to generate\n",
        "        # pobabilities, hint https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNm40ApS77al",
        "colab_type": "code",
        "outputId": "fea9afef-9942-4065-86b0-562e13b6c502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embd_size = 100\n",
        "learning_rate = 0.003\n",
        "hidden_size = 100\n",
        "window_size = 2\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the dataset.\n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
        "vocab_size = len(w2v_dataset.vocab)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "# Hint: the CBOW model object you've created.\n",
        "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "\n",
        "model = nn.DataParallel(model).to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "for _e in tqdm(range(num_epochs)):\n",
        "    epoch_loss = []\n",
        "    for sent_idx in range(w2v_dataset._len):\n",
        "        for w2v_io in w2v_dataset[sent_idx]:\n",
        "            # Zero gradient.\n",
        "            optimizer.zero_grad()\n",
        "            # Retrieve the inputs and outputs.\n",
        "            x, y = w2v_io['x'], w2v_io['y']\n",
        "            x = tensor(x).to(device)\n",
        "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
        "            # Calculate the log probability of the context embeddings.\n",
        "            logprobs = model(x)\n",
        "            # This unsqueeze thing is really a feature/bug... -_-\n",
        "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss.append(float(loss))\n",
        "    # Save model after every epoch.\n",
        "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
        "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
        "  \n",
        "# Save model after last epoch.\n",
        "#torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [07:37<00:00,  4.57s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lGV7nj9Q4H",
        "colab_type": "code",
        "outputId": "a4cc596e-e3ad-4380-8f40-37b9c7cd8560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set(rc={'figure.figsize':(12, 8)})\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAHUCAYAAADY9fvpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFF4AAAA4GoUXAAAAjvb/AW+A74OodkZrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STJnFmVfp_D7",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
        "\n",
        "# Apply and Evaluate the CBOW Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JbTG84xBeiY",
        "colab_type": "code",
        "outputId": "082092fb-8e59-4823-84ba-0855e29ae124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from lazyme import color_str\n",
        "\n",
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            # Remember how to get the best prediction output? \n",
        "            # Hint: https://pytorch.org/docs/stable/torch.html#torch.max\n",
        "            _, prediction =  torch.max(model(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91mparticularly\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mthe\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mfact\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91mmore\u001b[0m ( or\n",
            "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
            "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mthe\u001b[0m bigram ,\n",
            "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91mplan\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mby\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mby\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[91min\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[92ma\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mrole\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mthe\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91malso\u001b[0m for many\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91muse\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mbetween\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnot\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91mit\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mp\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[91mas\u001b[0m a ,\n",
            "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mdistribution\u001b[0m , p\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mall\u001b[0m methods are\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mthe\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mfor\u001b[0m just those\n",
            "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mthe\u001b[0m errors that\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[92mdo\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mis\u001b[0m wish to\n",
            "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91mon\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[91mnot\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91mno\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91mto\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mof\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mtexts\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91maccurate\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mnot\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[91meach\u001b[0m verb .\n",
            "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mas\u001b[0m out to\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mis\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mevidence\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91msophisticated\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mscfs\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mthat\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mor\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mrelative\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[92mwords\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[92mopposed\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[92mto\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mprobability\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mof\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mfrom\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mwill\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mgenerated\u001b[0m , this\n",
            "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mpaper\u001b[0m out not\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[91midentical\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mmodel\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[91maccurately\u001b[0m case .\n",
            "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
            "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
            "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mlarge\u001b[0m corpora .\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91m______\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mevents\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mcorpora\u001b[0m : is\n",
            "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m)\u001b[0m ⫺ 0.5\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mfrom\u001b[0m greater than\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
            "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mnumber\u001b[0m of statistical\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mfor\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mpopulation\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91magainst\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91minappropriate\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mbetween\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
            "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mis\u001b[0m by an\n",
            "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91msize\u001b[0m , or\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[91mdoes\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mh0\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91mfrom\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mpossible\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mto\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtheoretical\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91muse\u001b[0m of the\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mare\u001b[0m often an\n",
            "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mfrom\u001b[0m way to\n",
            "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91msee\u001b[0m ; the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mholds\u001b[0m where the\n",
            "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
            "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
            "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
            "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
            "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[91mx-and-y\u001b[0m ) ,\n",
            "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91m______\u001b[0m non-random and\n",
            "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
            "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
            "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mshoe-polish\u001b[0m , when\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mare\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mbe\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mdo\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mof\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[91mbeen\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthen\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mliterature\u001b[0m is a\n",
            "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mstatis-\u001b[0m issue :\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhether\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91m1999\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91mthat\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mamerican\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91massumptions\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91mdo\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mbetween\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mhold\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mshall\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[92mthat\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mrandom\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91m______\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[92mare\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mwas\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mby\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mcorpus\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mto\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[92mand\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[91ma\u001b[0m hypothesis test\n",
            "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
            "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mto\u001b[0m the fact\n",
            "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mall\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mcells\u001b[0m concern the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mof\u001b[0m between a\n",
            "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mb\u001b[0m a linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mheuristic\u001b[0m of a\n",
            "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mestimate\u001b[0m the relation\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91msize\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mcorpus\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91min\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91mand\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mχ2\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
            "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mrandomness\u001b[0m , as\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mfrom\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91menglish\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mgener-\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91mwhich\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mh0\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mall\u001b[0m is then\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91min\u001b[0m the hypothesis\n",
            "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91marbitrary\u001b[0m , be\n",
            "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mmarked\u001b[0m as :\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91min\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[92merror\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mfrom\u001b[0m greater than\n",
            "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[91mrather\u001b[0m than 0.5\n",
            "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
            "\u001b[92m1\u001b[0m \t\t with just \u001b[91mis\u001b[0m % of\n",
            "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91m)\u001b[0m of them\n",
            "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mbetween\u001b[0m them ,\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mtasks\u001b[0m , devastate\n",
            "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mnot\u001b[0m one of\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mevidence\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91min\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91m______\u001b[0m we have\n",
            "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mtrue\u001b[0m of data\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mthe\u001b[0m thresholding methods\n",
            "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91min\u001b[0m distinguish associated\n",
            "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mand\u001b[0m associated scfs\n",
            "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91m______\u001b[0m scfs from\n",
            "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91menglish\u001b[0m from noise\n",
            "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m:\u001b[0m noise .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaYievSyqZsU",
        "colab_type": "code",
        "outputId": "3d41c9f9-01b6-4e0d-bc6e-dc5dfc8bd00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(torch.max(model(x), 1))\n",
        "model(x)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([-0.1830], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([217], device='cuda:0'))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -9.0954,  -8.8783, -14.1952,  ..., -14.3613, -12.3311, -12.5668]],\n",
              "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsKur-twqfnn",
        "colab_type": "code",
        "outputId": "257dc125-038f-40f1-faa5-e0903c1d2c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.25957446808510637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4rlSLIrs2pr",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-4-load-model\"></a>\n",
        "\n",
        "# Go back to the 5th Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcXHTSnlqn99",
        "colab_type": "code",
        "outputId": "e4b19a55-0428-4b39-fc24-fc5d401d015c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model_5 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
        "model_5 = torch.nn.DataParallel(model_5)\n",
        "model_5.load_state_dict(torch.load('cbow_checkpoint_5.pt'))\n",
        "model_5.eval()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): CBOW(\n",
              "    (embeddings): Embedding(1303, 100)\n",
              "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
              "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlG1DRJHqt3I",
        "colab_type": "code",
        "outputId": "f9eb1529-c4f2-4cc1-b5ca-be6e4c438d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x = tensor(w2v_io['x']).to(device)\n",
        "        y = tensor(w2v_io['y']).to(device)\n",
        "        \n",
        "        if -1 in x: # Skip unknown words.\n",
        "            continue\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            _, prediction =  torch.max(model_5(x), 1)\n",
        "        true_positive += int(prediction) == int(y)\n",
        "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
        "        all_data += 1"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
            "\u001b[92messentially\u001b[0m \t problem is \u001b[91min\u001b[0m this :\n",
            "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mthe\u001b[0m : if\n",
            "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mrandom\u001b[0m if a\n",
            "\u001b[92mif\u001b[0m \t\t this : \u001b[91min\u001b[0m a word\n",
            "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
            "\u001b[92mword\u001b[0m \t\t if a \u001b[91mrandom\u001b[0m ( or\n",
            "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
            "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mthe\u001b[0m bigram ,\n",
            "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mby\u001b[0m , or\n",
            "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mand\u001b[0m etc .\n",
            "\u001b[92mis\u001b[0m \t\t the web \u001b[91min\u001b[0m a vast\n",
            "\u001b[92ma\u001b[0m \t\t web is \u001b[91mand\u001b[0m vast re-\n",
            "\u001b[92mvast\u001b[0m \t\t is a \u001b[91min\u001b[0m re- source\n",
            "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mthe\u001b[0m source for\n",
            "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mχ2\u001b[0m for many\n",
            "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
            "\u001b[92massociation\u001b[0m \t that the \u001b[91mdata\u001b[0m is random\n",
            "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnot\u001b[0m , arbitrary\n",
            "\u001b[92m,\u001b[0m \t\t is random \u001b[91mthe\u001b[0m arbitrary ,\n",
            "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mrandom\u001b[0m , motivated\n",
            "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
            "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mas\u001b[0m or pre-\n",
            "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m)\u001b[0m ( r\n",
            "\u001b[92m,\u001b[0m \t\t ( r \u001b[91mand\u001b[0m a ,\n",
            "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mever\u001b[0m , p\n",
            "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91mrandom\u001b[0m , from\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91mthe\u001b[0m from just\n",
            "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mand\u001b[0m just those\n",
            "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mthe\u001b[0m errors that\n",
            "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
            "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
            "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mis\u001b[0m wish to\n",
            "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
            "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m(\u001b[0m which there\n",
            "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
            "\u001b[92mis\u001b[0m \t\t which there \u001b[91mnot\u001b[0m any evidence\n",
            "\u001b[92many\u001b[0m \t\t there is \u001b[91ma\u001b[0m evidence as\n",
            "\u001b[92mevidence\u001b[0m \t is any \u001b[91mto\u001b[0m as a\n",
            "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mto\u001b[0m a true\n",
            "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91marbitrary\u001b[0m true scf\n",
            "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mand\u001b[0m scf for\n",
            "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mnot\u001b[0m for the\n",
            "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91min\u001b[0m the verb\n",
            "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
            "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mof\u001b[0m out to\n",
            "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mthe\u001b[0m indistinguishable from\n",
            "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mrandom\u001b[0m from one\n",
            "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mand\u001b[0m one where\n",
            "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91ma\u001b[0m where the\n",
            "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
            "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mas\u001b[0m individual words\n",
            "\u001b[92mindividual\u001b[0m \t where the \u001b[91mnull\u001b[0m words (\n",
            "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91m______\u001b[0m ( as\n",
            "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
            "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
            "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
            "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91min\u001b[0m the texts\n",
            "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
            "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
            "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mof\u001b[0m had been\n",
            "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
            "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mrandom\u001b[0m randomly selected\n",
            "\u001b[92mrandomly\u001b[0m \t had been \u001b[91m______\u001b[0m selected ,\n",
            "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mto\u001b[0m , this\n",
            "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mwe\u001b[0m out not\n",
            "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
            "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
            "\u001b[92mthe\u001b[0m \t\t to be \u001b[92mthe\u001b[0m case .\n",
            "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
            "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91mnumber\u001b[0m of subcategorization\n",
            "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mlarge\u001b[0m corpora .\n",
            "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91m______\u001b[0m tested using\n",
            "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91m______\u001b[0m using the\n",
            "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91msame\u001b[0m : is\n",
            "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m______\u001b[0m greater than\n",
            "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m______\u001b[0m critical value\n",
            "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
            "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mnumber\u001b[0m of statistical\n",
            "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
            "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91m______\u001b[0m processing .\n",
            "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mlanguage\u001b[0m is low\n",
            "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mlanguage\u001b[0m low ,\n",
            "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mdata\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
            "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mand\u001b[0m reject h0\n",
            "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
            "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
            "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mis\u001b[0m by an\n",
            "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mcorpora\u001b[0m , or\n",
            "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
            "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
            "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mand\u001b[0m is wrong\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91m______\u001b[0m wrong to\n",
            "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
            "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
            "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mthe\u001b[0m distinction with\n",
            "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtwo\u001b[0m one .\n",
            "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
            "\u001b[92mconference\u001b[0m \t of the \u001b[91mvalue\u001b[0m of the\n",
            "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mof\u001b[0m often an\n",
            "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mto\u001b[0m way to\n",
            "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91msee\u001b[0m ; the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mfor\u001b[0m where the\n",
            "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mdata\u001b[0m is overlooked\n",
            "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91m______\u001b[0m 16 (\n",
            "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m2\u001b[0m ( 1\n",
            "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
            "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
            "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91m______\u001b[0m non-random and\n",
            "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
            "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
            "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mthe\u001b[0m , when\n",
            "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mare\u001b[0m when we\n",
            "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mand\u001b[0m we look\n",
            "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
            "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mthe\u001b[0m at linguistic\n",
            "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mof\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
            "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[91mis\u001b[0m never be\n",
            "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[91mcan\u001b[0m be true\n",
            "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
            "\u001b[92mnot\u001b[0m \t\t we do \u001b[91min\u001b[0m always have\n",
            "\u001b[92malways\u001b[0m \t\t do not \u001b[91mword\u001b[0m have enough\n",
            "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mof\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t always have \u001b[91ma\u001b[0m data to\n",
            "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
            "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
            "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
            "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
            "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
            "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
            "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
            "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthen\u001b[0m that is\n",
            "\u001b[92mthat\u001b[0m \t\t , but \u001b[91m______\u001b[0m is a\n",
            "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mto\u001b[0m issue :\n",
            "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhether\u001b[0m there is\n",
            "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
            "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
            "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
            "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
            "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
            "\u001b[92mit\u001b[0m \t\t data , \u001b[91mthat\u001b[0m is rejected\n",
            "\u001b[92mis\u001b[0m \t\t , it \u001b[91m______\u001b[0m rejected .\n",
            "\u001b[92min\u001b[0m \t\t since words \u001b[91mand\u001b[0m a text\n",
            "\u001b[92ma\u001b[0m \t\t words in \u001b[91mand\u001b[0m text are\n",
            "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mand\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
            "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mare\u001b[0m random ,\n",
            "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mdata\u001b[0m , we\n",
            "\u001b[92m,\u001b[0m \t\t not random \u001b[91mthe\u001b[0m we know\n",
            "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
            "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mshall\u001b[0m that our\n",
            "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mrandom\u001b[0m our corpora\n",
            "\u001b[92mour\u001b[0m \t\t know that \u001b[91mthe\u001b[0m corpora are\n",
            "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mit\u001b[0m are not\n",
            "\u001b[92mare\u001b[0m \t\t our corpora \u001b[92mare\u001b[0m not randomly\n",
            "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mare\u001b[0m randomly generated\n",
            "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mbe\u001b[0m generated ,\n",
            "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
            "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mto\u001b[0m and the\n",
            "\u001b[92mand\u001b[0m \t\t generated , \u001b[92mand\u001b[0m the hypothesis\n",
            "\u001b[92mthe\u001b[0m \t\t , and \u001b[91ma\u001b[0m hypothesis test\n",
            "\u001b[92mhypothesis\u001b[0m \t and the \u001b[91msame\u001b[0m test con-\n",
            "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mto\u001b[0m the fact\n",
            "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91m)\u001b[0m in section\n",
            "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mfor\u001b[0m concern the\n",
            "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mof\u001b[0m between a\n",
            "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mwas\u001b[0m a linguistic\n",
            "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mof\u001b[0m of a\n",
            "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mreject\u001b[0m the relation\n",
            "\u001b[92mbetween\u001b[0m \t the relation \u001b[91msize\u001b[0m , for\n",
            "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mthe\u001b[0m for example\n",
            "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
            "\u001b[92mexample\u001b[0m \t , for \u001b[91mrandom\u001b[0m , a\n",
            "\u001b[92m,\u001b[0m \t\t for example \u001b[91min\u001b[0m a verb\n",
            "\u001b[92ma\u001b[0m \t\t example , \u001b[91mand\u001b[0m verb ’\n",
            "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
            "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mfor\u001b[0m s syntax\n",
            "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
            "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
            "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mrandom\u001b[0m , as\n",
            "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mand\u001b[0m rather than\n",
            "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91menglish\u001b[0m than arbitrary\n",
            "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mthe\u001b[0m arbitrary .\n",
            "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
            "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
            "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
            "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
            "\u001b[92m,\u001b[0m \t\t error term \u001b[91mwhere\u001b[0m language is\n",
            "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mand\u001b[0m is never\n",
            "\u001b[92mis\u001b[0m \t\t , language \u001b[91mand\u001b[0m never ,\n",
            "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
            "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
            "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
            "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mand\u001b[0m is then\n",
            "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mof\u001b[0m the hypothesis\n",
            "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandom\u001b[0m , be\n",
            "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mare\u001b[0m as :\n",
            "\u001b[92mare\u001b[0m \t\t as : \u001b[91min\u001b[0m the error\n",
            "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mvery\u001b[0m error terms\n",
            "\u001b[92merror\u001b[0m \t\t are the \u001b[91msame\u001b[0m terms systematically\n",
            "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
            "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mis\u001b[0m greater than\n",
            "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[91mas\u001b[0m than 0.5\n",
            "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
            "\u001b[92m1\u001b[0m \t\t with just \u001b[91mis\u001b[0m % of\n",
            "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91m)\u001b[0m of them\n",
            "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mthe\u001b[0m them ,\n",
            "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mcorpora\u001b[0m , devastate\n",
            "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mnot\u001b[0m one of\n",
            "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
            "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91msame\u001b[0m for which\n",
            "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91min\u001b[0m which we\n",
            "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91m______\u001b[0m we have\n",
            "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91ma\u001b[0m of data\n",
            "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mthe\u001b[0m thresholding methods\n",
            "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mfor\u001b[0m distinguish associated\n",
            "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mand\u001b[0m associated scfs\n",
            "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91m______\u001b[0m scfs from\n",
            "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mand\u001b[0m from noise\n",
            "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m______\u001b[0m noise .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3dj17lRtGtK",
        "colab_type": "code",
        "outputId": "0f2e2894-f6b4-4ca9-b4ed-458ef6a679f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)\n",
        "model(x).shape[1] == len(w2v_dataset.vocab)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.19574468085106383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Qi9npoVDWq",
        "colab_type": "text"
      },
      "source": [
        "#How to Handle Unknown Words? \n",
        "\n",
        "This is not the best way to handle unknown words, but we can simply assign an index for unknown words.\n",
        "\n",
        "**Hint:** Ensure that you have `gensim` version >= 3.7.0 first. Otherwise this part of the code won't work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QGgLystN_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e2ccd24-b3ed-4419-a659-c7c42053a003"
      },
      "source": [
        "!python -m pip install -U pip\n",
        "!python -m pip install -U gensim>=3.7.0"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlnxD-rwVMI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7089130-4cae-4977-edb8-8536d3a4b9ff"
      },
      "source": [
        "import gensim\n",
        "gensim.__version__"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0RBfFXIVYud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c75a6ea8-bbd2-4d8b-a5b8-2f7c7a0a494f"
      },
      "source": [
        "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
        "dict(vocab.items())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11E9hmCCVcjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6aad4111-d445-4346-fb1f-f8014a042dfb"
      },
      "source": [
        "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
        "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
        "\n",
        "try:\n",
        "    special_tokens = {'<pad>': 0, '<unk>': 1} # we define our own special tokens for missing words\n",
        "    vocab.patch_with_special_tokens(special_tokens)\n",
        "except: # If gensim is not 3.7.0\n",
        "    pass\n",
        "    \n",
        "dict(vocab.items())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<pad>',\n",
              " 1: '<unk>',\n",
              " 2: 'foo',\n",
              " 3: 'is',\n",
              " 4: 'sentence',\n",
              " 5: 'this',\n",
              " 6: 'a',\n",
              " 7: 'bar'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pWznO2bYCWQ",
        "colab_type": "text"
      },
      "source": [
        "#Lets Rewrite the `Word2VecText` Object\n",
        "\n",
        "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmtJmBH0XmTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecText(Dataset):\n",
        "    def __init__(self, tokenized_texts, window_size, variant):\n",
        "        \"\"\"\n",
        "        :param tokenized_texts: Tokenized text.\n",
        "        :type tokenized_texts: list(list(str))\n",
        "        \"\"\"\n",
        "        self.sents = tokenized_texts\n",
        "        self._len = len(self.sents)\n",
        "        \n",
        "        # Add the unknown word patch here.\n",
        "        self.vocab = Dictionary(self.sents)\n",
        "        try:\n",
        "            special_tokens = {'<pad>': 0, '<unk>': 1}\n",
        "            self.vocab.patch_with_special_tokens(special_tokens)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.window_size = window_size\n",
        "        self.variant = variant\n",
        "        if variant.lower() == 'cbow':\n",
        "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
        "        elif variant.lower() == 'skipgram':\n",
        "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        The primary entry point for PyTorch datasets.\n",
        "        This is were you access the specific data row you want.\n",
        "        \n",
        "        :param index: Index to the data point.\n",
        "        :type index: int\n",
        "        \"\"\"\n",
        "        vectorized_sent = self.vectorize(self.sents[index])\n",
        "        \n",
        "        return list(self._iterator(vectorized_sent))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def vectorize(self, tokens):\n",
        "        \"\"\"\n",
        "        :param tokens: Tokens that should be vectorized. \n",
        "        :type tokens: list(str)\n",
        "        \"\"\"\n",
        "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
        "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
        "    \n",
        "    def unvectorize(self, indices):\n",
        "        \"\"\"\n",
        "        :param indices: Converts the indices back to tokens.\n",
        "        :type tokens: list(int)\n",
        "        \"\"\"\n",
        "        return [self.vocab[i] for i in indices]\n",
        "    \n",
        "    def cbow_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1\n",
        "        for window in per_window(tokens, n):\n",
        "            target = window.pop(window_size)\n",
        "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
        "            \n",
        "    def skipgram_iterator(self, tokens, window_size):\n",
        "        n = window_size * 2 + 1 \n",
        "        for i, window in enumerate(per_window(tokens, n)):\n",
        "            focus = window.pop(window_size)\n",
        "            # Generate positive samples.\n",
        "            for context_word in window:\n",
        "                yield {'x': (focus, context_word), 'y':1}\n",
        "            # Generate negative samples.\n",
        "            for _ in range(n-1):\n",
        "                leftovers = tokens[:i] + tokens[i+n:]\n",
        "                if leftovers:\n",
        "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OSsvhCwYWz1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5\"></a>\n",
        "\n",
        "# Lets try the skipgram task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRCr4fPYSKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "    \n",
        "    def forward(self, focus, context):\n",
        "        embed_focus = self.embeddings(focus).view((1, -1))\n",
        "        embed_context = self.embeddings(context).view((1, -1))\n",
        "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
        "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
        "        log_probs = F.logsigmoid(score)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRlP8wMGYiDV",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5-foward\"></a>\n",
        "\n",
        "# Take a closer look at what's in the `forward()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuv-pGGzYeGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx1 = torch.rand(1,20)\n",
        "xx2 = torch.rand(1,20)\n",
        "\n",
        "xx1_numpy = xx1.detach().numpy()\n",
        "xx2_numpy = xx2.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwgV35oQYkS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a46445c5-95fb-4454-a4b2-0ac972fe8a43"
      },
      "source": [
        "print(xx1_numpy.shape)\n",
        "print(xx2_numpy.T.shape)\n",
        "print(np.dot(xx1_numpy, xx2_numpy.T))\n",
        "\n",
        "print(xx1.shape)\n",
        "print(torch.t(xx2).shape) \n",
        "print(torch.mm(xx1, torch.t(xx2)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 20)\n",
            "(20, 1)\n",
            "[[6.148223]]\n",
            "torch.Size([1, 20])\n",
            "torch.Size([20, 1])\n",
            "tensor([[6.1482]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68J7DuXcYz38",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5-train\"></a>\n",
        "\n",
        "# Train a Skipgram model (for real)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0CXh7nccJbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Utility code to monitor the GPU usage\n",
        "import subprocess\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-XLjsGiYqBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "e5051ee2-ef59-4c06-b0bb-ddb715a50ecb"
      },
      "source": [
        "embd_size = 100\n",
        "learning_rate = 0.03\n",
        "hidden_size = 30 #To reduce training time changing 300 to 30\n",
        "window_size = 3\n",
        "\n",
        "# Initialize the dataset.\n",
        "w2v_dataset = Word2VecText(tokenized_text_train, window_size=3, variant='skipgram')\n",
        "vocab_size = len(w2v_dataset.vocab)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Use the Skipgram object\n",
        "model = SkipGram(vocab_size, embd_size,).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "\n",
        "#model = nn.DataParallel(model) #You can easily run your operations on multiple GPUs by making your model run parallelly using DataParallel\n",
        "#refer - https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\n",
        "num_epochs = 100\n",
        "for _e in tqdm(range(num_epochs)):\n",
        "    epcoh_loss = 0\n",
        "    for sent_idx in range(w2v_dataset._len):\n",
        "        for w2v_io in w2v_dataset[sent_idx]:\n",
        "            # Retrieve the inputs and outputs.\n",
        "            x1, x2 = w2v_io['x']\n",
        "            x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
        "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float)).to(device)\n",
        "            # Zero gradient.\n",
        "            model.zero_grad()\n",
        "            # Calculate the log probability of the context embeddings.\n",
        "            logprobs = model(x1, x2)\n",
        "            # This unsqueeze thing is really a feature/bug... -_-\n",
        "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epcoh_loss += float(loss)\n",
        "    #print('GPU Memory ->', torch.cuda.get_device_properties(device).total_memory)\n",
        "    #print('GPU Space ->', get_gpu_memory_map())\n",
        "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
        "    losses.append(epcoh_loss)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 1/100 [00:36<1:00:37, 36.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU Space -> {0: 811}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 2/100 [01:13<59:53, 36.67s/it]  \u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU Space -> {0: 811}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-b524e56764ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepcoh_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSyLhD6cxKD",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"section-3-1-5-evaluate\"></a>\n",
        "\n",
        "# Evaluate the model on the skipgram task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdsmNseFa5HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_positive = 0\n",
        "all_data = 0\n",
        "# Iterate through the test sentences. \n",
        "for sent in tokenized_text_test:\n",
        "    # Extract all the CBOW contexts (X) and targets (Y)\n",
        "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
        "        model.zero_grad()\n",
        "        # Retrieve the inputs and outputs.\n",
        "        x1, x2 = w2v_io['x']\n",
        "        x1, x2 = tensor(x1).to(device), tensor(x2).to(device) #make sure you are running on GPU or CPU.\n",
        "        y = w2v_io['y']\n",
        "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
        "        true_positive += int(prediction) == int(y)\n",
        "        all_data += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnioy9HCZ6dc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "979212b6-5482-4088-eb43-dfece2e810e8"
      },
      "source": [
        "print('Accuracy:', true_positive/all_data)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tck7ypIxdHkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}